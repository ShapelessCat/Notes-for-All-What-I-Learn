#+TITLE: Essential Effects
#+AUTHOR: Adam Rosien
#+VERSION: 2021-07-05 - Early Access
#+STARTUP: overview
#+STARTUP: entitiespretty

* DONE Preface - 7
  CLOSED: [2021-08-13 Fri 03:04]
** DONE Acknowledgements - 7
   CLOSED: [2021-08-13 Fri 03:04]
** DONE About this book - 7
   CLOSED: [2021-08-13 Fri 03:03]
   - /Cats Effect/ is a library that makes it easy to write code that effectively
     * uses multiple cores and
     * doesn't leak resources.

   - This book aims to
     introduce the _core concepts_ in /Cats Effect/,
     giving you the knowledge you need to go further with the library in your
     own applications.

   - This book is *NOT* a detailed dive into every aspect of /Cats Effect/.
     Our aim is to
     1. give you the understanding you need so you can rapidly apply it,
     2. while setting you up to learn any additional details on your own if needed.

   - Essential Effects will teach you to:
     * *Understand* the MEANING and ROLE of /side effects/ and /effects/.
     * *Understand* how to *encapsulate* /side effects/ in _a SAFER form._
     * Use ~parMapN~ and other /combinators/ to _run /effects/ in parallel._
     * *Fork* independent work *into* /concurrent tasks/,
       then *cancel* or *join* them.
     * Learn how to *SEPARATE* /CPU-bound work/ from /blocking, I/O-bound work/.
     * *Integrate* /callback-based code/, like ~scala.concurrent.Future~, into a
       _safer, effectbased interface._
     * *Build* and *combine* /leak-proof resources/ for applications.
     * *Test* code that performs MULTIPLE /effects/ like /concurrency/ and _I/O_.

   - The design of _the Cats Effect library_ uses /typeclasses/ to encode concepts
     like /parallelism/, /concurrency/, and so on.
       However, rather than programming with an _abstract effect type_ that uses
     /typeclass constraints/ -- a perfectly valid programming technique! -- this
     book uses the concrete ~cats.effect.IO~ type as the main vehicle to _discuss
     and demonstrate_ programming with /effects/.

   - *A functional programming curriculum*

** DONE Cats Effect versions - 9
   CLOSED: [2021-08-13 Fri 02:44]
   CE2 - 2.2.0 (=from Jian= The latest is 2.5.3)
   CE3 (=from Jian= The latest is 3.2.2)

** DONE Source code for examples and exercises - 10
   CLOSED: [2021-08-13 Fri 02:49]
   https://github.com/inner-product/essential-effects-code

   - Solutions are in a branch of this repo, along with being presented both in
     the text and in an appendix.

** DONE Prerequisites - 10
   CLOSED: [2021-08-13 Fri 03:03]
*** DONE Functors - 10 - =RE-READ=
    CLOSED: [2021-07-17 Sat 03:08]
    - =from Jian= I don't know this before here:
      #+begin_src scala
        import cats.implicits._

        val fa: F[A] = ???  // `F` here has a functor instance.

        val replaced: F[String] = fa.map(_ => "replacement")
        // Re-write
        val replaced: F[String] = fa.as("replacement")


        val voided: F[Unit] = fa.map(_ => ())
        // Re-write
        val voided: F[Unit] = fa.void
      #+end_src

*** DONE Applicatives - 11 - =RE-READ=
    CLOSED: [2021-08-13 Fri 03:02]
    - ~map~ and ~mapN~
      #+begin_src scala
        def map[B](A => B): F[B]
        def mapN[C]((A, B) => C): F[C]
        def mapN[D]((A, B, C) => D): F[D]
        //  ... ...
        def mapN[Z]((A, ...) => Z): F[Z]
      #+end_src

    - In /Essential Effects/
      we will use /applicative methods/ to *compose*
      _MULTIPLE_, _INDEPENDENT_ /effects/, such as during /parallel computation/.

    - =from Jian= I don't know this before here:
      #+begin_src scala
        import cats.implicits._

        val first: F[A] = ???
        val second: F[B] = ???

        val third: F[B] = (first, second).mapN((_, b) => b)
        // Re-write
        val third: F[B] = first *> second
      #+end_src

*** DONE Monads - 12
    CLOSED: [2021-08-13 Fri 03:03]

* DONE 1. Effects: evaluation and execution - 14
  CLOSED: [2021-08-15 Sun 00:56]
  - To explore what /effects/ are, and how we can leverage them,
    * We'll distinguish _TWO aspects of code_:
      + *computing values*
      + *interacting with the environment*.

    * We'll also talk about how /transparent, or not/, our code can be in
      describing these aspects, and what we as programmers can do about it.

** DONE 1.1. The substitution model of evaluation - 14
   CLOSED: [2021-08-14 Sat 16:24]
   Use two examples to show the difference between *computing values* and
   *interacting with the environment*. The latter doesn't is not /referential
   transparent/, and _simple /substitution/ doesn't work_.

   - Two properties in *computing values*:
     * local reasoning ::
       No need to look anywhere except the (literal) definition of ~plusOne~.
       There are no references to anything outside of it.
       + =from Jian=
         /Local reasoning/ is good,
         BUT it is not always simple if there is no /referential transparency/
         inside it.

     * referential transparency ::
       Under /substitution/, programs mean the same result
       if they evaluate to the same value.
       + =from Jian=
         It doesn't matter when and where the evaluation happen.

   - Here are a few you might have thought of:
     1. When printing to the console.
     2. When reading values from the outside world.
     3. When expressions refer to mutable variables.

** DONE 1.2. Dealing with side effects - 16
   CLOSED: [2021-08-14 Sat 16:38]
   We really need /side effects/ to actually do something in our programs,
   BUT /side effects/ are unsafe.
   - Q :: How to maintain the ability to reason about code that includes impure
          code?

   - A :: One way is to *localize* the “impure” code that BREAKS /substitution/.
     * To the outside world,
       + The code will look -- and evaluate -- _AS IF_ /substitution/ is taking
          place.
       + But inside the boundary, there are dragons.

     * Example of localizing the _impure code_:
       #+begin_src scala
         def sum(ints: List[Int]): Int = {
           var sum = 0
           ints.foreach(i => sum += i)
           sum
         }

         sum(List(1, 2, 3))
       #+end_src
       + From the callers' point of view, /subsititution/ _is maintained_.

       + Within the _impure code_,
         1. we can't leverage the reasoning that /substitution/ gives us,
         2. so
            to prove to ourselves the code behaved
            we'd have to use other techniques that are outside the scope of this
            book.

   - CONCLUSION:
     /Localization/ is a nice trick,
     BUT _WON'T WORK for everything that breaks /substitution/._
     * Q :: Is there a better (more universal) way?

** DONE 1.3. The Effect Pattern - 17 - =NOTE=
   CLOSED: [2021-08-14 Sat 18:48]
   - Effects :: some conditions that ifi they are imposed, the /side effects/ can
                be tamed into something safer. There are _TWO parts:_
     1. The /type/ of the program _SHOULD tell us_
        *what kind of /effects/ the program will perform,
        in addition to the /type/ of the value it will produce.*
        * Given the impure code a /type/, and make it visible!
        * At the same time, continue to track the /type/ of the result of the
          computation.

     2. If the behavior we want relies upon some _EXTERNALLY-VISIBLE_ /side effect/,
        we
        *separate* describing the /effects/ we want to happen
        *from* actually making them happen.
          THEN we can FREELY *substitute* the _description of effects_ *until*
        the point we run them.
        * This idea is exactly the _SAME_ as the /localization/ idea,
          *EXCEPT* that
          + _instead of_
            performing the side effect at the innermost layer of code
            and
            hiding it from the outer layers,
          + we _delay_ the /side effect/ so it executes *OUTSIDE of ANY evaluation,*
            ensuring /substitution/ still holds within.

   - We'll call these described above conditions the *Effect Pattern*, and apply it
     * to studying and describing the /effects/ we use every day, and
     * to new kinds of /effects/.

   - =IMPORTANT=
     *Effect Pattern Checklist*
     1. Does the /type/ of the program tell us
        a. _what kind of_ /effects/ the program will perform; and
        b. _what type of_ /value/ it will produce?

     2. When /externally-visible side effects/ are required,
        is the _effect description_ *separate from* the _execution_?
        =FIXME= separate ==> separated

   - =from Jian=
     You should always use this checklist to identify and confirm /effects/.

*** TODO 1.3.1. Example: Is ~Option~ an effect? - 18
    - *Effect Pattern Checklist:* ~Option[A]~
      =TODO=
      =TODO= =NOTE= =QUESITON= =???= =RE-READ= the second item of the checklist!!!
      =TODO=

*** TODO 1.3.2. Example: Is ~Future~ an effect? - 20
    - *Effect Pattern Checklist:* ~Future[A]~
      =TODO= =NOTE=

** DONE 1.4. Capturing arbitrary side effects as an effect - 23
   CLOSED: [2021-08-14 Sat 18:54]
   - The ~IO~ /effect/ type in ~cats.effect~ is a /data type/ that allows us to
     *capture* ANY /side effect/, but *in a SAFE way*, following our /Effect Pattern/.

   - Let's implement our own (toy) version (mental model) of ~IO~ to understand
     how it works.
     * Create our first effect to capture *arbitrary* /side effects/.
       #+begin_src scala
         package com.innerproduct.ee.effects

         final case class MyIO[A](unsafeRun:() => A)

         object MyIO {
           def putStr(s: => String): MyIO[Unit] =
             MyIO(() => println(s))
         }

         object Printing extends App {
           val hello = MyIO.putStr("hello!")

           hello.unsafeRun()  // Explicitly run the effect
         }
       #+end_src
       The ~unsafeRun~ name is selected to let everyone know this function
       *does NOT maintain /substitution/.*

     * Try to do evaluation by /substitution/, and you can see the code above
       equals to the ~println("hello!")~.

** DONE 1.5. Composing effects - 25
   CLOSED: [2021-08-15 Sun 00:08]
   Add /combinators/ like ~map~ (modify the output of an /effect/) and ~flatMap~
   (use the output of an /effect/ to create a new /effect/) to ~MyIO~.
     *But be careful!* Composing /effects/ *MUST NOT* _execute_ them.
   We require composition to *maintain substitution*, so we may _build /effects/
   out of other /effects/._
   #+begin_src scala
     package com.innerproduct.ee.effects

     final case class MyIO[A](unsafeRun: () => A) {
       def map[B](f: A => B): MyIO[B] =
         MyIO(() => f(unsafeRun()))

       def flatMap[B](f: A => MyIO[B]): MyIO[B] =
         MyIO(() => f(unsafeRun()).unsafeRun())
     }

     object MyIO {
       def putStr(s: => String): MyIO[Unit] =
         MyIO(() => println(s))
     }

     object Printing extends App {
       val hello = MyIO.putStr("hello!")
       val world = MyIO.putStr("world!")

       val helloWorld: MyIO[Unit] =
         for {
           _ <- hello
           _ <- world
         } yield ()

       helloWorld.unsafeRun()
     }

     // OUTPUT:
     //// hello!
     //// world!
   #+end_src

   - *Exercise 1: Timing*
     =DONE=

*** DONE 1.5.1. ~MyIO~ as an effect - 27
    CLOSED: [2021-08-14 Sat 23:54]
    - *Effect Pattern Checklist:* ~MyIO[A]~

    - *What's a "thunk"?*
      * thunk :: a delayed computation
        + The name is a pun on the past tense of "think",
          so the value of the /thunk/ is available after the "thinking" of
          the computation is complete.

        + A /thunk/ may _optionally_ /memoize/ its result,
          avoiding recomputation when subsequently evaluated.

      * /Call-by-name parameters/ can't themselves be values,
        + =from Jian=
          My understanding is this means /call-by-name parameter syntax/ specifies
          the way to evaluate a parameter, not a value itself.
          _A /value/ is certain evaluation result._

        + =from Jian=
          In another point of view,
          The /call-by-name parameter syntax/ doesn't give us a full control:
          we can't decide to evaluate it or not, it only promises that a
          call-by-name parameter won't evaluate if we don't access it.
          *This is _NOT_ enough!*

        + We need to find a way to annotate /thunk values/. See below.

        so a /thunk/ can alternatively have the /type signature/ ~() => A~:
        _a zero-argument function_ that produces a /value/ of type ~A~ when evaluated.
        =FIXME=
        the ~() => A~ above in the book uses the single character arrow.

    - People do *NOT* always use the terminology *strictly*.
      * Sometimes,
        people call the ~MyIO~ a /thunk/,
        SINCE you can use it to
        + *produce* a _delayed computation_,
        + rather than the more literal interpretation of it *having* a /thunk/.

** DONE 1.6. Summary - 29
   CLOSED: [2021-08-15 Sun 00:56]
   1. The /substitution model of evaluation/ gives us
      - /local reasoning/
      - /fearless refactoring/.

   2. *Interacting* with the /environment/ can *break* /substitution/.
      _ONE SOLUTION_ is to *localize* these /side effects/ so they don’t affect
      evaluation.

   3. _ANOTHER SOLUTION_ is the /Effect Pattern/:
      a set of conditions that makes the presence of /effects/ more _VISIBLE_
      while ensuring /substitution/ is maintained.

      - An /effect type/ tells us what kind of /effects/ the program will perform,
        in addition to the /type/ of the value it will produce.

      - /Effects/
        *separate*
        describing what we want to happen
        *from*
        actually making it happen.
        * We can freely
          *substitute* _the description of /effects/_ *up*
          UNTIL the point we run them.

      - =from Jian=
        I think the /effect pattern/ is superior than the first solution.
        Both of these solutions use the same fundamental idea, but the /effect pattern/
        can expand the /local/ to a larger scale (until ~unsafeRun~ is called), and
        /effect pattern/ introduce /effect types/ and /combinators/ systematically,
        which can make the /effect pattern/ solution scalable and more elegant.

   4. We demonstrated a way to *safely capture* /side effects/ via the ~MyIO[A]~ type,
      which *delayed* the /side effect/ until the ~unsafeRun~ method is called.
      - We produced new ~MyIO~ values with the ~map~ and ~flatMap~ /combinators/.

* DONE 2. Cats Effect ~IO~ - 31
  CLOSED: [2021-08-15 Sun 05:33]
  We already built our ~MyIO~ /effect/. We'll learn ~cats.effect.IO~, which as
  the SAME properties.
    We'll also show how to *build applications* using /effects/ with
  ~cats.effect.IOApp~.

** DONE 2.1. Constructing ~IO~ values - 31
   CLOSED: [2021-08-15 Sun 04:19]
   Use ~IO.delay~ to construct an ~IO~ effect.
   ~IO.apply~ is an alias of ~IO.delay~.
   #+begin_src scala
     def delay[A](a: => A): IO[A]
   #+end_src

   - Q :: When the /effect/ is executed,
          what happens if the /side effect/ _throws an exception_?
          For example, ~IO.delay(throw new RuntimeException("oh noes!"))~

   - A :: This can only happen when you running a /effect/.
     * =from Jian=
       If a /effect/ is constructed properly.

   - Construct ~IO~ /effect/ from a pure value _LESS common_,
     but it can be done: ~IO.pure(12)~.

   - Do *NOT* perform ANY /side effects/ when calling ~IO.pure~,
     * because
       1. they will _be eagerly evaluated_, and
       2. that will _break substitution_.

     * If you are not sure, use ~IO.delay~.
       =from Jian=
       Or if you don't want this _eagerly evaluation_.

   - Lift an exception into ~IO~:
     ~IO.raiseError(new RuntimeException("oh noes!"))~
     * No ~throw~ here

   - Since it is a common alternative /effect type/,
     =FIXME=

     =DOES this mean ~Future~ is a common alternative /effect type/=

     =I think there is a CRITICAL missing "of" after "alternative", since we
     already discussed ~Future~ doesn't fulfill the /effect pattern/.=

     there is a general way to *transform* ~scala.concurrent.Future~ values into
     ~IO~ values:
     #+begin_src scala
       def futurish: Future[String] = ???

       val fut: IO[String] = IO.fromFuture(IO(futurish))
     #+end_src
     In this way, we successfully convert ~Future[T]~ to ~IO[T]~.

** DONE 2.2. Transforming ~IO~ values - 32
   CLOSED: [2021-08-15 Sun 05:05]
   - ~IO~ is
     * a /functor/: ~IO(12).map(_ + 1)~
     * an /applicative/: ~(IO(12), IO("hi")).mapN((i, s) => s"$s: $i")~
     * a /monad/: Use ~flatMap~ and ~for~-comprehension:
       #+begin_src scala
         for {
           i <- IO(12)
           j <- IO(i + 1)
         } yield j.toString
       #+end_src

   - There are many other /combinators/ available.
     =TODO= =TODO= =TODO=
     _Check the Appendix A, Cheatsheets_

*** DONE 2.2.1. Error handling - 33 - =RE-READ=
    CLOSED: [2021-08-15 Sun 05:05]
    footnote 9:
    The error handling methods mentioned here are defined in ~ApplicativeError~
    and ~MonadError~ /type classes/ of the Cats library.

    - As we've seen, an ~IO~ computation can *fail*,
      * either by _throwing an exception_ during execution,
      * or by _capturing an existing exception_ via ~IO.raiseError~.

    - We can, however, detect these _failures_ and do something about it.
      A common combinator for _error handling_ is ~handleErrorWith~,
      which *has a SIMILAR /signature/ to ~flatMap~ EXCEPT it accepts _error values_:*
      #+begin_src scala
        def handleErrorWith[AA >: A](f: Throwable => IO[AA]): IO[AA]
      #+end_src
      * Examples:
        #+begin_src scala
          val ohNoes =
            IO.raiseError[Int](new RuntimeException("oh noes!"))

          val handled: IO[Int] =
            ohNoes.handleErrorWith(_ => IO(12))
        #+end_src
        + If you are sure that the value provided by you is a _successful value_,
          you can simplify the ~ohNoes.handleErrorWith(_ => IO(12))~ to
          ~ohNoes.handleError(_ => 12)~.

        + Caution:
          The effect generated by ~f~ can fail.
          #+begin_src scala
            ohNoes.handleErrorWith(t => IO.raiseError(new OtherException(t)))
          #+end_src

        + If the _transformation between errors_ are what you expect,
          you can simplify the above code as
          ~ohNoes.adaptError(t => new OtherException(t))~
          _This is equivalent to the above ~ohNoes.handleErrorWith(...)~ code!_

        + You can also transform an error to a ~Left~ value of ~Either~:
          ~def attempt: IO[Either[Throwable, A]]~
          #+begin_src scala
            ohNoes.attempt
          #+end_src

    - _Instead of_ hiding the error-handling
      we're now exposing the error,
      BUT *delay* it handling by lifting the error into a successful ~IO~ effect.
      - This is equivalent to
        #+begin_src scala
          val attempted: IO[Either[Throwable, Int]] =
            ohNoes
              .map(i => Right(i): Either[Throwable, Int])
              .handleErrorWith(t => Left(t))

          // is equivalent to

          val attempted: IO[Either[Throwable, Int]] =
            ohNoes.attempt
        #+end_src

    - *Error-handling Decision Tree*
      =IMPORTANT=
      =IMPORTANT=
      =IMPORTANT=
      If an error occurs in your ~IO[A]~ do you want to...
      1. *perform* an /effect/?
         Use ~onError(pf: PartialFunction[Throwable, IO[Unit]]): IO[A]~.

      2. *transform* _ANY ERROR_ *into* _another error_?
         Use ~adaptError(pf: PartialFunction[Throwable, Throwable]): IO[A]~.

      3. *transform* _ANY ERROR_ *into* a _successful value_?
         Use ~handleError(f: Throwable ⇒ A): IO[A]~.

      4. *transform* _SOME kinds of ERRORS_ *into* a _successful value_?
         Use ~recover(pf: PartialFunction[Throwable, A]): IO[A]~.

      5. *transform* _SOME kinds of ERRORS_ *into* _another /effect/?_
         Use ~recoverWith(pf: PartialFunction[Throwable, IO[A]]): IO[A]~.

      6. *make errors visible* BUT *delay error-handling*?
         Use ~attempt: IO[Either[Throwable, A]]~.

      Otherwise, use ~handleErrorWith(f: Throwable => IO[A]): IO[A]~.
      =FIXME= The arrow

** DONE 2.3. Executing ~IO~ values - 35
   CLOSED: [2021-08-15 Sun 05:11]
   We've *delayed* ANY /side effects/ by *encapsulating* them *into* an ~IO~ value.
   When we're done *composing* our program we'll finally *execute* the /effects/.
     There are a number of /methods/ to *execute* them, and they *ALL* are
   *prefixed with* ~unsafe~ to denote that /side effects/ will get executed
   and that our /substitution/ process *no longer applies*.

   - The most common ~unsafe~ method you'll encounter is ~unsafeRunSync~.
     * Inspect this name:
       + ~Run~ means execute, and
       + ~sync~ means synchronous execution;
       together they _run the effects synchronously_ and return the result.

   - Invoking ~unsafeRunSync~ on an ~IO[A]~ will produce a value of type ~A~ if
     the effect succeeds:
     #+begin_src scala
       def unsafeRunSync: A
     #+end_src

   - Use ~unsafeToFuture~ to integrate your effectful code with _legacy interfaces_
     that many consume ~scala.concurrent.Future~.
     #+begin_src scala
       def unsafeToFuture: Future[A]
     #+end_src
     * =from Jian=
       If there is a chance to use cats-effect, you should never use ~Future~.
       + Maybe there are _exceptions_, if performance is taken into account. I'm not sure!!!

   - *CAUTION*:
     * As a general rule,
       + you *should not* be invoking _any ~unsafe~ method_ in your code.
       + When *experimenting in the REPL* or some other throw-away code, *sure*.

     * In an app, *ALWAYS* _delegate_ this responsibility to types like ~IOApp~.

** DONE 2.4. ~IO~ as an effect - 36
   CLOSED: [2021-08-15 Sun 05:13]
   - *Error Pattern Checklist:* ~IO[A]~

** DONE 2.5. Executing effects in applications with ~IOApp~ - 37
   CLOSED: [2021-08-15 Sun 05:26]
   To execute /effects/, the _Cats Effects_ provides the ~IOApp~ type for _applications_.
   ~IOApp~ is _an /executable Scala type/ -- something that has a ~main~ method_ --
   that requires you to declare your /effects/ as a *single* ~IO~ value,
   and
   it performs the task of executing those /effects/.

   - Examples 3. "Hello World" as an ~IOApp~ program. Code available at
     =resources/HelloWorld.scala=
     #+begin_src scala
       package com.innerproduct.ee.resources

       import cats.effect._

       object HelloWorld extends IOApp {
         def run(args: List[String]): IO[ExitCode] =
           helloWorld.as(ExitCode.Success)

         val helloWorld: IO[Unit] =
           IO(println("Hello world!"))
       }
     #+end_src
     * The application /entry point/ is the ~run~ /method/,
       which MUST return ~IO[ExitCode]~.

     * ~ExitCode~ is a enumeration.

   - *Exercise 2: Ticking Clock*
     #+begin_src scala
       package com.innerproduct.ee.io

       import cats.effect.*
       import cats.implicits.given
       import scala.concurrent.duration.given

       object TickingClock extends IOApp {

         def run(args: List[String]): IO[ExitCode] =
           tickingClock.as(ExitCode.Success)

         val tickingClock: IO[Unit] = {
           val currentTime = IO(System.currentTimeMillis())
           for {
             _ <- IO(println(System.currentTimeMillis()))
             _ <- IO.sleep(1.second)
             _ <- tickingClock
           } yield ()
         }

       }
     #+end_src

** DONE 2.6. Summary - 39
   CLOSED: [2021-08-15 Sun 05:33]
   1. ~cats.effect.IO~ is an /effect/ that can *encapsulate* ANY /side effect/.
      - Constructors produce an ~IO~ from
        * pure values,
        * delayed /side effects/,
        * errors, and
        * other types like ~Future~.

      - /Combinators/ let you
        * *build* new /effects/,
        * *transform* their outputs, and
        * *handle* errors.

        It is _ESSENTIAL_ that
        EVERY /combinator/ *AVOIDS the execution* of ANY /effect/,
        otherwise /substitution/ will be broken.

      - We can execute ~IO[A]~ values, who produce
        * EITHER a /value/ of type ~A~
        * OR raise an /exception/.

        You should only run them at the very “edges” of your programs via its
        /unsafe-prefixed methods/.

   2. ~cats.effect.IOApp~ lets you _describe your application_
      as a SINGLE ~IO~ /effect/ that it executes.

* DONE 3. Parallel execution - 41
  CLOSED: [2021-08-16 Mon 19:56]
  1. First we'll discuss if ~IO~ itself _supports /parallelism/, or NOT._

  2. We'll then talk about
     - HOW ~IO~ can support /parallelism/, and
     - HOW that /parallelism/ is implemented.

  3. We'll then see some examples of _DIFFERENT WAYS_ to
     *compose* ~IO~ values _in parallel_.

** DONE 3.1. Does ~IO~ support parallelism? - 41 - =RE-READ= =RE-NOTE=
   CLOSED: [2021-07-19 Mon 00:26]
   To answer the question of whether or not ~IO~ supports /parallelism/, let's
   first compare it to a similar data type, ~scala.concurrent.Future~, which
   we've seen supports /parallelism/ by *scheduling* work *on* MULTIPLE /threads/
   via a ~scala.concurrent.ExecutionContext~.

   =After reading this section I get:=
   The methods ~flatMap~ and ~mapN~ of ~Future~ doesn't support parallelism
   if ~Future~ doesn't support *eagerly scheduling*.

   - In the code below, is the /effect/ of ~hw1~ the _SAME_ as the /effect/ of ~hw2~?
     Do ~hello~ and ~world~ run _in /parallel/, or NOT?_
     What output will we see printed to the console?

   - Because ~Future~ *eagerly schedules* the action, and *caches* the result.
     The code breaks rule #2 of our Effect Pattern:
     #+begin_src scala
       package com.innerproduct.ee.parallel

       import cats.implicits._
       import scala.concurrent._
       import scala.concurrent.duration._

       object Future1 extends App {
         implicit val ec = ExecutionContext.global

         val hello = Future(println(s"[${Thread.currentThread.getName}] Hello"))
         val world = Future(println(s"[${Thread.currentThread.getName}] World"))

         val hw1: Future[Unit] =
           for {
             _ <- hello
             _ <- world
           } yield()

         Await.ready(hw1, 5.seconds)

         val hw2: Future[Unit] =
           (hello, world).mapN((_, _) => ())

         Await.ready(hw2, 5.seconds)
       }

       // [scala-execution-context-global-10] Hello
       // [scala-execution-context-global-11] World
     #+end_src
     * We can't see two output of =Hello= and =World=

     * We can see /parallelism/ -- differents threads: 10 and 11

   - Delay the /side effects/ by defining ~hello~ and ~world~ with ~def~ instead
     of ~val~. Then output is like:
     #+begin_src text
       [scala-execution-context-global-10] Hello
       [scala-execution-context-global-10] World
       [scala-execution-context-global-11] World
       [scala-execution-context-global-10] Hello
     #+end_src
     * =TODO=
       But be careful! Even though we see output happening on two different
       /threads/, that doesn't imply that those computations happened in parallel.
       How might you be able to show they ran in parallel, or not? (It's not too
       important to answer this question.)

     * ~hw2~ computation is actually *non-deterministic*.

     * This demonstrates that for ~Future~, ~flatMap~ and ~mapN~ have *different*
       EFFECTS with respect to /parallelism/.

     * *But note*:
       it is *NOT* the case ~mapN~ for ~Future~ is implemented with /parallelism/
       but ~flatMap~ is implemented as something _sequential_.
         The /parallelism/ comes as a *side effect* -- pun intended -- of
       ~Future~ _eagerly scheduling_ the computation, which happens *before* ~mapN~
       itself is evaluated.

   - What about ~IO~?
     Does using ~mapN~ vs. ~flatMap~ have a different effect, like ~Future~ does?
     #+begin_src scala
       package com.innerproduct.ee.parallel

       import cats.effect._
       import cats.implicits._

       object IOComposition extends App {
         val hello = IO(println(s"[${Thread.currentThread.getName}] Hello"))
         val world = IO(println(s"[${Thread.currentThread.getName}] World"))

         val hw1: IO[Unit] =
           for {
             _ <- hello
             _ <- world
           } yiled ()

         val hw2: IO[Unit] =
           (hello, world).mapN((_, _) => ())

         hw1.unsafeRunSync()
         hw2.unsafeRunSync()
       }

       // [main] Hello
       // [main] World
       // [main] Hello
       // [main] World
     #+end_src
     * The /threads/ are the *SAME* -- ~IO~ does *NOT* provide any support for
       /the effect of parallelism/! And this is by design, because we want
       *DIFFERENT /effects/ to have DIFFERENT /types/*, as per our _Effect Pattern_.

** DONE 3.2. The ~Parallel~ typeclass - 46
   CLOSED: [2021-07-19 Mon 01:32]
   Follow the #1 of our /Effect Pattern/, there is a ~cats.effect~ type for
   /parallelism/. Its name is ~IO.Par~.
   #+begin_src scala
     sealed abstract class IO[+A] { /* ... */ }
     object IO {
       class Par[+A] { /* ... */ }

       object Par {
         def apply[A](ioa: IO[A]): Par[A] = ???
         def unwrap[A](ioa: Par[A]): IO[A] = ???
       }
     }
   #+end_src

   - ~IO.Par~ will *NOT* have a ~Monad~ /instance/, because we do *not* want to
     be able to *serialize* the execution of multiple actions.
       Instead it will have an ~Applicative~ instance, to compose independent
     ~IO.Par~ values:
     #+begin_src scala
       implicit def ap(implicit cs: ContextShift[IO]): Applicative[IO.Par] =
         new Applicative[IO.Par] {
           def pure[A](a: A): IO.Par[A] = IO.Par(IO.pure(a))
           def map[A, B](pa: IO.Par[A])(f: A => B): IO.Par[B] = ???
           def product[A, B](pa: IO.Par[A], pb: IO.Par[B]): IO.Par[(A, B)] = ???
         }
     #+end_src
     * About ~implicit cs: ContextShift[IO]~,
       =TODO= _Chapter 5, Shifting contexts._ =TODO=

     * The implementation of ~product~ will ensure that ~pa~ and ~pb~ execute on
       DIFFERENT /threads/, using ~cs~.
       =TODO=

   - It's a bit *VERBOSE* to have to *switch* types when we translate between
     /sequential/ and /parallel/ execution.
     #+begin_src scala
       val ia: IO[A] = IO(???)
       val ib: IO[B] = IO(???)

       def f(a: A, b: B): C = ???

       val ipa: IO.Par[A] = IO.Par(ia)
       val ipb: IO.Par[B] = IO.Par(ib)

       val ipc: IO.Par[C] = (ipa, ipb).mapN(f)

       val ic: IO[C] = IO.Par.unwrap(ipc)
     #+end_src

   - The ~Parallel~ /type class/ from _cats_ (NOT _cats-effect_):
     #+begin_src scala
       trait Parallel[S[_]] {
         type P[_]

         def monad: Monad[S]

         def applicative: Applicative[P]

         def sequantial: P ~> S

         def parallel: S ~> P
       }
     #+end_src
     1. /Typeclass instances/ are about the type ~S~ (for *sequential*).
        For example, there will be a /typeclass instance/ ~Parallel[IO]~, where
        ~IO~ is the /sequential type/ to be transformed.

     2. The /typeclass instance/ defines the ~P~ type (for *parallel*).
        For the ~Parallel[IO]~ /typeclass instance/, ~P~ would be ~IO.Par~.

     3. ~S~ must have a ~Monad~. That is, operations using ~S~ must be *sequenced*.

     4. ~P~ must have an ~Applicative~. That is, operations using ~P~
        *must not have* _any data ordering dependencies_.

     5. A ~Parallel~ /instance/ must be able to *transform from* _sequential values_
        *to* _parallel values_, and back.
          The ~~>~ symbol is a /type alias/ for ~cats.arrow.FunctionK~, which is a
        transformation from some type ~F[A]~ to another type ~G[A]~, for any type ~A~.
        So the type ~P ~> S~ is equivalent to code like ~def apply[A](pa: P[A]): S[A]~.

   - Figure 3. The ~Parallel~ /typeclass/ encodes transformations between a
     /sequential type/ ~S~ and a /parallel type/ ~P~.

   - Let's use ~Parallel[IO]~ to re-write the above code:
     #+begin_src scala
       val ia: IO[A] = IO(???)
       val ib: IO[B] = IO(???)

       def f(a: A, b: B): C = ???

       val ipa: IO.Par[A] = Parallel[IO].parallel(ia)
       val ipb: IO.Par[B] = Parallel[IO].parallel(ib)

       val ipc: IO.Par[C] = (ipa, ipb).mapN(f)

       val ic: IO[C] = Parallel[IO].sequential(ipc)
     #+end_src
     * We can _do better_, though.
       Once a ~Parallel~ /typeclass instance/ is defined,
       *par-prefixed versions of functions* become available on the /sequential
       type/ that do this translation automatically, so you never see the
       underlying change of type:
       #+begin_src scala
         val ia: IO[A] = IO(???)
         val ib: IO[B] = IO(???)

         def f(a: A, b: B): C = ???

         val ic: IO[C] = (ia, ib).parMapN(f)
       #+end_src
       See Figure 4

** DONE 3.3. Inspecting parallelism - 50
   CLOSED: [2021-08-16 Mon 00:26]
   - Q :: How to get a feel for what is executing?
   - A :: Use a helper method, ~debug~, to add to our code through
          ~import com.innerproduct.ee.debug._~
          =from Jian= CAUTION: this is not a part of _cats-effect_!
     * Example 10. =parallel/DebugExample.scala=
       #+begin_src scala
         package com.innerproduct.ee.parallel

         import cats.effect._
         import cats.implicits._

         import com.innerproduct.ee.debug._

         object DebugExample extends IOApp {
           def run(args: List[String]): IO[ExitCode] =
             seq.as(ExitCode.Success)

           val hello = IO("hello").debug
           val world = IO("world").debug

           val seq =
             (hello, world)
               .mapN((h, w) => s"$h $w")
               .debug
         }
       #+end_src
       + At /runtime/, the ~debug~ method will print
         - the _name_ of the _CURRENT /thread/,_
         - along with the _value_ produced by the /effect/ (as a string produced by invoking ~toString~):
         #+begin_src text
           [ioapp-compute-0] hello
           [ioapp-compute-0] world
           [ioapp-compute-0] hello world
         #+end_src

       + The source for ~debug~:
         #+begin_src scala
           package com.innerproduct.ee

           import cats.effect._

           /** `import com.innerproduct.ee.debug._` to access
            ,*  the `debug` extension methods.
            ,*/
           object debug {
             /** Extension methods for an effect of type `IO[A]`. */
             implicit class DebugHelper[A](io: IO[A]) {

               /** Print to the console the value of the effect
                ,*  along with the thread it was computed on.
                ,*/
               def debug: IO[A] =
                 for {
                   a <- ioa
                   tn = Thread.currentThread.getName
                   _ = println(s"[${Colorize.reversed(tn)}] $a")
                 } yield a
             }
           }
         #+end_src

** DONE 3.4. ~parMapN~ - 52 - =TODO= mutiple ~raiseError~'s handling of ~parMapN~ and ~parTupled~
   CLOSED: [2021-08-16 Mon 16:08]
   ~parMapN~ is the _parallel version of the /applicative/ ~mapN~ method_.
   It lets us *combine* multiple /effects/ *into* one, _in parallel_, by
   specifying how to *combine* the outputs of the /effects/.

   - =from Jian=
     The above metions _parallel version of the /applicative/ ~mapN~ method_.
     Just a reminder, the /applicative ~mapN~ method/ can be parallel or not, which
     depends on if a type have an /monad instance/.
       ~parMapN~ can help its user and guarantee this operation is in parallel.

   - Use the ~debug~ introduced in the previous section, we can inspect the ~parMapN~:
     #+NAME: parallel/ParMapN.scala
     #+begin_src scala
       package com.innerproduct.ee.parallel

       import cats.effect._
       import cats.implicits._
       import com.innerproduct.ee.debug._

       object ParMapN extends IOApp {
         def run(args: List[String]): IO[ExitCode] =
           par.as(ExitCode.Success)

         val hello = IO("hello").debug
         val world = IO("world").debug
         val par =
           (hello, world)
             .parMapN((h, w) => s"$h $w")
             .debug
       }

       // [ioapp-compute-1] world
       // [ioapp-compute-0] hello
       // [ioapp-compute-0] hello world
     #+end_src
     * The /execution order/ of /parallel tasks/ is *non-deterministic*,
       so you may see =hello= and =world= be printed in a _different order_ when
       you run the program.

*** DONE 3.4.1. ~parMapN~ behavior in the presence of errors - 54 - =TODO= =NOTE=
    CLOSED: [2021-07-19 Mon 01:46]
    =MORE NOTES=
    The *FIRST failure* to happen is used AS _the failure of the composed /effect/._

    =FIXME=
    Example 13 last `parMapN` should be `mapN`
    Already sent a email to the author.

*** DONE 3.4.2. ~parTupled~ - 56
    CLOSED: [2021-07-19 Mon 01:55]
    The ~parMapN((_, _) => ())~ code looks a bit *UGLY*.

    The original example
    #+begin_src scala
      (ok, ko1).parMapN((_, _) => ())
    #+end_src

    can be re-written without changing its meaning as

    #+begin_src scala
      val e1 = (ok, ko1).parMapN(???).map(_ => ())
    #+end_src

    can be re-written as

    #+begin_src scala
      val e1 = (ok, ko1).parMapN(???).void
    #+end_src

    can be re-written with the /method/ ~parTupled~ as

    #+begin_src scala
      val e1 = (ok, ko1).parTupled.void
    #+end_src
    - This ~parTupled~ will collect the values in ~IO~ into a tuple wrapped by an
      ~IO~.
      #+begin_src scala
        (ia, ib).parTupled
        (ia, ib, ic).parTupled
        (ia, ib, ic, id).parTupled
      #+end_src

** DONE 3.5. ~parTraverse~
   CLOSED: [2021-08-16 Mon 19:49]
   ~parTraverse~ is the *parallel version* of ~traverse~; both have the type
   signature: ~F[A] => (A => G[B]) => G[F[B]]~

   - For example, if ~F~ is ~List~ and ~G~ is ~IO~,
     then _(par)traverse_ would be a function from a ~List[A]~ to an ~IO[List[B]]~
     when given a function ~A => IO[B]~.
     ~List[A] => (A => IO[B]) => IO[List[B]]~

     - We can inspect the run with ~debug~'s.

   - =TODO= =READ SOURCE CODE=
     That being said, ~parTraverse~ is actually written in terms of ~traverse~,
     where it transforms every ~IO~ into ~IO.Par~.
       Since ~traverse~ only requires the /effect/ to have _an ~Applicative~
     instance_, the ~Applicative[IO.Par]~ is where the /parallelism/ “happens”.
     =IMPORTANT=

*** DONE 3.5.1. Another view of ~parTraverse~ - 59
    CLOSED: [2021-08-16 Mon 19:49]
    _(par)traverse_ is similar to _(par)mapN_, where results are collected,
    *BUT* EVERY INPUT /effect/ has the *SAME* /output type/:
    #+begin_src scala
      def f(i: Int): IO[Int] = IO(i)

      (f(1), f(2)).parMapN((a, b) => List(a, b))                          // IO[List[Int]]
      (f(1), f(2), f(3)).parMapN((a, b, c) => List(a, b, c))              // IO[List[Int]]
      (f(1), f(2), f(3), f(4)).parMapN((a, b, c, d) => List(a, b, c, d))  // IO[List[Int]]

      List(1, 2, 3, 4).parTraverse(f)                                     // IO[List[Int]]
    #+end_src

** DONE 3.6. ~parSequence~ - 59
   CLOSED: [2021-08-16 Mon 19:51]
   _(par)sequence_ turns a nested structure "inside-out"
   =from Jian= like the ~sequence~ method from _cats_.
   ~F[G[A]] => G[F[A]]~

   - For example, if you have a ~List~ of ~IO~ effects, the value types
     transformation will be ~List[IO[A]] => IO[List[A]]~

   - *Note*:
     The ~sequence~ and ~traverse~ are mutually definable:
     ~x.sequence~ is ~x.traverse(identity)~, and
     ~x.traverse(f)~ is ~x.map(f).sequence~.

** DONE 3.7. Summary - 61 - =TODO=
   CLOSED: [2021-08-16 Mon 19:56]
   1. ~IO~ does *not support* _parallel operations_ itself, because it is a ~Monad~.

   2. The ~Parallel~ /type class/ specifies the *TRANSLATION between a pair of
      /effect types/:*
      one that is a ~Monad~ and the other that is “only” an ~Applicative~.

   3. ~Parallel[IO]~ connects the ~IO~ /effect/ to its _PARALLEL counterpart, ~IO.Par~._

   4. _Parallel ~IO~ composition_ requires the *ABILITY* to
      _shift_ computations _to_ other /threads/ within the CURRENT ~ExecutionContext~.
      This is how parallelism is “implemented”.

   5. ~parMapN~, ~parTraverse~, ~parSequence~ are the _PARALLEL_ versions of (the
      sequential) ~mapN~, ~traverse~, and ~sequence~.

      =???= Errors are managed in a fail-fast manner.
      =from Jian=
      The example of ~parMapN~ is wrong, and it can crash in more than one
      ~raiseError~ value.

* DONE 4. Concurrent control - 62
  CLOSED: [2021-07-23 Fri 04:58]
  - So far _we've been working with rather *opaque* /effects/:_
    * Example 16. *Without* /concurrent control/,
                  we can ONLY *describe* and *(eventually) run* /effects/.
      #+begin_src scala
        val i1: IO[A] = ???
        val i2: IO[B] = ???
        val i3: IO[C] = doSomething(i1, i2)

        val c: C = i3.unsafeRunSync()
      #+end_src
      1. we can *describe* them and *eventually run* them to produce
         a value (or an error).
      2. _BUT_ we do *NOT YET* have any way to *CONTROL* a running computation.

  =START HERE=
  - In this chapter we WILL DISCUSS
    * how to *fork* and *join* a /concurrent effect/, *cancel* a _concurrently
      running effect_, and
    * how to *race* MULTIPLE /effects/ concurrently.

  - *Concurrency vs. parallelism*
    - concurrent :: Computations when their /execution lifetimes/ *overlap*.

    - parallel :: Computations when their *executions occur at the SAME instant*
                  in time.

    - That is to say,
      * /concurrency/ is about the looking at
        + the *structure* of the computations and
        + how their /lifetimes/ *align*,
      * whereas
        /parallelism/ is more about the _operational utilization of resources_
        during the execution.

    - For example,
      * with _two_ /threads/ you could run _two_ computations in /parallel/ (and
        /concurrently/ =from Jian= _if they have overlap /lifetimes/!_).

      * with _one_ /thread/ _CAN_ still run two computations *concurrently*:
        if you can *"pause"* one and *switch*, using the same /thread/, to the
        other, and vice-versa.
        + BUT _NO way_ to run in *parallel*.

    - /Concurrency/ EMPHASIZES the *non-deterministic aspects* of computation:
      * we _CAN'T_ tell when anything happens,
      * _ONLY_ that their *lifetimes overlap*.
    - WHEREAS /parallelism/ requires *determinism*:
      no matter how many resources you have, you must produce the same answer.
      =TODO= =???= =TODO= =LEARN MORE=

** DONE 4.1. Decomposing the behavior of ~parMapN~ - 64
   CLOSED: [2021-08-17 Tue 10:54]
   To demonstrate *forking*, *joining*, and *cancelation* of /concurrent effects/,
   we'll write our own version of ~parMapN~, which involves each of them.
   We only _write two-argument variation of ~parMapN~._
   #+begin_src scala
     def myParMapN[A, B, C](ia: IO[A], ib: IO[B])(f: (A, B) => C): IO[C] =
       ???
   #+end_src
   - =IMPORTANT=
     The criterion of a quanlified ~myParMapN~ --
     just like ~parMapN~, needs to:
     1. *start* both the ~ia~ and ~ib~ computations
        so they _run concurrently (*"fork"* them);_
     2. *wait* for each result;
     3. *cancel* the "other" /effect/ if ~ia~ or ~ib~ _fails_; and
     4. finally *combine* the results with the ~f~ function.

   - It's important to note that _in order to_ *"wait"* and *"cancel"*,
     we'll need *SOMETHING to "wait" and "cancel" ON,*
     a kind of handle to the _"started" computation_.
     * In _Cats Effect_ that concept is a /fiber/.

   - Fiber :: a _STARTED computation_ that can be *"wait" and "cancel" ON*.
     =from Jian= =re-phrase the previous sentence=

** DONE 4.2. Gaining control with ~Fiber~ - 64
   CLOSED: [2021-08-17 Tue 13:37]
   When we write an expression like
   #+begin_src scala
     for {
       result <- effect
       ...
   #+end_src
   the value result only exists once it is produced by the /effect/. We're
   essentially *waiting until* the ~result~ is available to *continue* the
   computation.

   - Instead of waiting for the ~result~, we could
     1. instead *fork* an /effect/:
        the /effect/ will be *started*, but we _aren't interested in waiting_ for
        its completion.

     2. The _result of forking_ is a _value_ that lets us manage the /forked effect/:
        a /fiber/.

   - In Cats Effect, use the ~start~ /method/ to *fork* an /effect/:
     * _Example 17. Forking an effect with start. Code available at =control/Start.scala=._
       #+begin_src scala
         package com.innerproduct.ee.control

         import cats.effect._
         import com.innerproduct.ee.debug._

         object Start extends IOApp {

           def run(args: List[String]): IO[ExitCode] =
             for {
               _ <- task.start
               _ <- IO("task was started").debug
             } yield ExitCode.Success

           val task: IO[String] =
             IO("task").debug
         }

         // [ioapp-compute-1] task
         // [ioapp-compute-0] task was started
       #+end_src
       When you ~start~ an /effect/ its execution is *“forked”*:
       it is *shifted to* a DIFFERENT /thread/.

     * The (simplified) signature of ~start~:
       #+begin_src scala
         def start: IO[Fiber[IO, A]]
       #+end_src
       + It returns a ~Fiber~, a data type which lets us *act on* the /start-ed
         effect/.
       + Q :: But why does ~start~ RETURN _the ~Fiber~ *inside* an ~IO~?_
       + A :: BECAUSE _if it instead produced, directly, a ~Fiber~, that would
               mean our original ~IO~ is running right now,_
               BUT *in reality it isn't*.
         - The source ~IO~ ONLY executes when we explicitly run it,
           so we need to
           *delay ACCESS* to this /fiber/ -- by wrapping it in an effect --
           *until* the source ~IO~ is executed.

     * Now that we’ve demonstrated _forking_ a ~Fiber~,
       we feel the need to *offer a warning*:
       a ~Fiber~ is a *VERY “low-level” mechanism* for /concurrent control/.
       + WHILE it's absolutely necessary for implementing the /concurrency/ and
          /parallelism/ of _Cats Effect_,
       + as a developer you can often _BETTER_ achieve your goals by *using
         higher-level abstractions and operations.*
         =IMPORTANT=

**** DONE 4.2.1. Continuing ~myParMapN~: forking effects - 66
     CLOSED: [2021-08-17 Tue 11:12]
     - We can use ~start~ to *fork* a /concurrent effect/,
       so let's use it for our ~myParMapN~.
       #+begin_src scala
         def myParMapN[A, B, C](ia: IO[A], ib: IO[B])(f: (A, B) => C): IO[C] =
           for {
             fiberA <- ia.start
             fiberB <- ib.start
           } yield ???
       #+end_src
       * We *start* each /effect/ to run them /concurrently/.

       * We *DON'T YET* KNOW
         how to *gather* their results or possibly *cancel* them.
         =from Jian= This is what we should discuss in the following sections.

     - Here's our progress for the requirements:
       ☑ *start* both the ia and ib computations so they run concurrently (“fork” them);
       ☐ *wait* for each result;
       ☐ *cancel* _the “OTHER” /effect/_ if ~ia~ or ~ib~ fails; and
       ☐ finally *combine* the results with the ~f~ function.

**** DONE 4.2.2. Joining a running ~Fiber~ - 66
     CLOSED: [2021-08-17 Tue 13:37]
     When we call ~start~ on an ~IO[A]~ value we receive a ~Fiber[IO, A]~ value.
     It lets us talk about _the execution of an ~IO[A]~ computation._

     - Q :: What can we do with a ~Fiber~?
     - A :: The first thing we can do is to ~join~ it,
       #+begin_src scala
         val joined: IO[String] =
           for {
             fiber <- IO("task").start
             s     <- fiber.join
           } yield s
       #+end_src
       ~join~ will _return the result_ of the /forked ~IO~ effect/.
       1. Because of this *join*, we're *giving up* the control the /fiber/ gave us, and
       2. subsequently we can ONLY talk about the eventual result of the previously
          _forked value_.

     - Q :: What happens if we *join* the ~Fiber~ that we just ~start~-ed?
     - Q :: What executes on which /thread/?
     - A ::
       #+begin_src scala
         package com.innerproduct.ee.control

         import cats.effect._
         import com.innerproduct.ee.debug._
         import scala.concurrent.duration._

         object JoinAfterStart extends IOApp {

           def run(args: List[String]): IO[ExitCode] =
             for {
               fiber <- task.start
               _     <- IO("pre-join").debug
               _     <- fiber.join.debug
               _     <- IO("post-join").debug
             } yield ExitCode.Success

           val task: IO[String] =
             IO.sleep(2.seconds) *> IO("task").debug
         }
         // [ioapp-compute-0] pre-join
         // [ioapp-compute-1] task
         // [ioapp-compute-1] task
         // [ioapp-compute-1] post-join
       #+end_src
       - =from Jian=
         The order of the first two log messages are undeterministic.
         We see this order is because the ~IO.sleep(2.seconds) *>~ in the
         definition of ~task~.

       - Notice that ~task~ is on a *different* /thread/ than the ="pre-join"=
         output.
           We also see =task= printed twice, once for the ~IO("task").debug~ and
         once for the ~fiber.join.debug~.

       - =IMPORTANT= =IMPORTANT= =IMPORTANT=
         When we ~join~ a ~Fiber~, execution *continues on the /thread/ the Fiber
         was running on* (in this case, ioapp-compute-1).

**** DONE 4.2.3. Continuing ~myParMapN~: joining forked effects - 68
     CLOSED: [2021-08-17 Tue 13:37]
     - Since we an await the results of a /concurrent effect/ with ~join~,
       we can update our ~myParMapN~.
       #+begin_src scala
         def myParMapN[A, B, C](ia: IO[A], ib: IO[B])(f: (A, B) => C): IO[C] =
           for {
             fiberA <- ia.start
             fiberB <- ib.start
             a <- fiberA.join
             b <- fiberB.join
           } yield f(a, b)
       #+end_src
       Since the ~f~ need both results, it doesn't matter which order we ~join~ in.

     - Here's our progress for the requirements:
       ☑ *start* both the ~ia~ and ~ib~ computations so they run concurrently (“fork” them);
       ☑ *wait* for each result;
       ☐ *cancel* the “other” effect if ~ia~ or ~ib~ fails; and
       ☑ finally *combine* the results with the ~f~ function.

** DONE 4.3. Canceling a running ~Fiber~ - 68
   CLOSED: [2021-08-17 Tue 21:19]
   The second thing we can do with a ~Fiber~ is to *cancel* it.
   #+begin_src scala
     def cancel: cats.effect.CancelToken[IO]

     type CancelToken[F[_]] = F[Unit]
   #+end_src

   - *Canceling a ~Fiber~ is itself an effect.*
     It produces a ~Unit~ value once the /effect/ is *canceled*.

   - Q :: Why might we want to stop a running task?
   - A :: Usually it is because we've learned some information that tells us the
          computation isn't needed any longer.
     * For example,
       we might start a fetch from a (relatively slow) datastore, but if the
       user decides to _cancel_ the overall operation, we should _cancel_ the
       fetch to the underlying datastore.

   - Example 19. =control/Cancel.scala=
     #+begin_src scala
       package com.innerproduct.ee.control

       import cats.effect._
       import cats.effect.implicits._
       import com.innerproduct.ee.debug._

       object Cancel extends IOApp {

         def run(args: List[String]): IO[ExitCode] =
           for {
             fiber <- task.onCancel(IO("i was cancelled").debug.void).start
             _     <- IO("pre-cancel").debug
             _     <- fiber.cancel
             _     <- IO("canceled").debug
           } yield ExitCode.Success

         val task: IO[String] =
           IO("task").debug *>
             IO.never

       }

       // [ioapp-compute-1] task
       // [ioapp-compute-0] pre-cancel
       // [ioapp-compute-0] i was cancelled
       // [ioapp-compute-0] canceled
     #+end_src
     * =TODO= =from Jian= =email=
       The order is undeterministic! The first two messages can be any order.
         I get the order mentioned above, which is different from the order in
       this book. Both are right, but the order in book is rare. In my laptop, I
       can easily get that order by preprend ~IO.sleep(15.millis) *>~ to the
       current ~task~ definition. Or use the code below:
       #+begin_src scala
         def t: String = {
           Thread.sleep(3.seconds)
           "task"
         }

         val task: IO[String] =
           IO(t).debug *>
             IO.never
       #+end_src

     * ~cancel~ is *idempotent*:
       Invoking it more than once has the same effect as invoking it once --
       a canceled task will continue to be canceled.

     * =CAUTION=
       =IMPORTANT=
       =IMPORTANT=
       =IMPORTANT=
       However, _if you ~join~ after you ~cancel~,_ the ~join~ will *NEVER FINISH*,
       + *REASON*: no result will ever be produced.

*** DONE 4.3.1. How does cancelation work? - 70
    CLOSED: [2021-08-17 Tue 21:19]
    Let's set up a situation where there's _a /long-lived effect/ running CONCURRENTLY_
    with an /effect/ that produces an error. For the former we’ll use the previously
    written "ticking clock":
    #+begin_src scala
      val tickingClock: IO[Unit] =
        for {
          _ <- IO(System.currentTimeMillis()).debug
          _ <- IO.sleep(1.seconds)
          _ <- tickingClock
        } yiled ()
    #+end_src

    - Run it _concurrently_ with a /failing effect/ using ~parTupled~:
      #+begin_src scala
        // We raise an error after two seconds, to give the ticking clock a chance to
        // print a few times to the console.
        val ohNoes =
          IO.sleep(2.seconds) *>
            IO.raiseError(new RuntimeException("oh noes!"))

        val together = (tickingClock, ohNoes).parTupled

        // [ioapp-compute-0] 1603147303459
        // [ioapp-compute-1] 1603147304469
        // java.lang.RuntimeException: oh noes!
        //     at com.innerproduct.ee.concurrent.CancelledClock$.<clinit>(CancelledClock.scala:16)
        //     at com.innerproduct.ee.concurrent.CancelledClock.main(CancelledClock.scala)
      #+end_src
      * Once the exception is raised, the ~tickingClock~ will be *cancelled* by
        some kind of “error handler” belonging to _the ~parMapN~-composed effect_.

      * Our _ENDLESSLY recursing ~tickingClock~ effect_ stops,
        and we *didn't explicitly do anything*. So
        + HOW does cancelation work?
        + can our effects
          - *"know"* if they've *been canceled*?
          - *react* to that information?

    - To define the behavior of /cancelation/,
      _Cats Effect_ uses the concept of a /cancelation boundary/.
      * Cancelation boundary ::
        As an effect executes,
        if a /cancelation boundary/ -- whatever that is -- is encountered,
        then the _cancelation status_ for the CURRENT /effect/ is checked, and *if
        that /effect/ has been canceled then execution will stop.*

    - From one perspective, *cancelation is "AUTOMATIC"*
      BECAUSE _Cats Effect_ itself *periodically inserts* a /cancelation boundary/
      during _effect execution_.
      * Alternatively, one can *"manually" insert* a /cancelation boundary/ with
        ~IO.cancelBoundary~.
        + =footnote 15=
          ~IO.cancelBoundary~ only exists in Cats Effect 2.
          Read the footnotes 14 and 15 to get more details and the rationale.
          =IMPORTANT=

*** DONE 4.3.2. Continuing ~myParMapN~: cancelation-on-error behavior - 71
    CLOSED: [2021-08-17 Tue 21:19]
    If an _error_ *occurs DURING* one of our /effects/,
    we need to *cancel* "the other" /fiber/.

    1. Let's use the ~onError~ combinator to handle each /effect/:
       =CAUTION= this is our first version, not a workable version!!!
       #+begin_src scala
         def myParMapN[A, B, C](ia: IO[A], ib: IO[B])(f: (A, B) => C): IO[C] =
           for {
             fiberA <- ia.start
             fiberB <- ib.start
             a <- fiberA.join.onError(_ => fiberB.cancel)
             b <- fiberB.join.onError(_ => fiberA.cancel)
           } yield f(a, b)
       #+end_src
       There is a critical bug.

    2. The issue is that _registering an ~onError~ handler_ is itself an
       /effect/, so in the code above the handler would only be registered if we
       couple it to the result of ~fiberA.join~.
         But if we do that, then we won't be _registering the ~onError~ handler_
       with the result of ~fiberB~ until after ~fiberA~ has actually finished.
       =from Jian= Blocking?

    3. We need to instead _ENSURE_ that _both ~onError~ handlers are registered._
       If only we could write
       #+begin_src scala
         for {
           fa <- ia.start
           fb <- ib.start
           faj = fa.join.onError(_ => fb.cancel)
           fbj = fb.join.onError(_ => fa.cancel)
           c <- myParMapN(
             fa.join.onError(_ => fb.cancel),
             fb.join.onError(_ => fa.cancel)
           )(f)
         } yield c
       #+end_src
       * =from Jian=
         Use ~=~ insteand ~<-~ to separate the value building of coupling
         /fiber join/ and /error handler registration/ from /effect/ running.

       * BUT that would be using the method we are trying to write! (And it
         would incorrectly handle cancelation).
         =from Jian= Circular Reference!!!

    4. If we tried something "clever" like:
       #+begin_src scala
         for {
           fa <- ia.start
           fb <- ib.start
           faj = fa.join.onError(_ => fb.cancel)
           fbj = fb.join.onError(_ => fa.cancel)
           registerA <- faj.start
           registerB <- fbj.start
           a <- registerA.join
           b <- registerB.join
         } yield f(a, b)
       #+end_src
       this too will not properly handle /cancelation/:
       IF one of the /effects/ is *cancelled*,
       THEN a _SUBSEQUENT join_ will *never complete*.

    5. *We're stuck*!!!:
       we need to *avoid* doing a ~join~ *on* a _potentially cancelled effect_,
       BUT here either /effect/ could be cancelled first -- we don't know which.
       1) The ~Fiber~ API *isn't expressive enough* to give us the information we need.

       2) _To solve the problem,_ we need a DIFFERENT *"primitive" operation*:
          we'll instead /race/ two /effects/, which will
          1. let us know which /effect/ finishes first
          2. so that we can subsequently _join the OTHER /effect/._

** DONE 4.4. Racing multiple effects - 72
   CLOSED: [2021-08-17 Tue 21:40]
   When we *compose* multiple /effects/ *concurrently* with ~parMapN~,
   we provide a function to ~parMapN~ to _transform the gathered output of every
   concurrently executing effect._

   - Q :: What if instead we were
     ONLY INTERESTED IN _the /effect/ that *completed first*,_
     relating them temporally.
     * We call this a /race/, and can have one using the ~IO.race~ /combinator/:
       #+begin_src scala
         def race[A, B](lh: IO[A], rh: IO[B])
                       (implicit cs: ContextShift[IO]): IO[Either[A, B]]
       #+end_src

   - ~race~ is like ~parTupled~, but only return the first completed one.
     #+begin_src scala
       val ia: IO[A] = ???
       val ib: IO[B] = ???

       (ia, ib).parTupled  // IO[(A, B)]
       IO.race(ia, ib)     // IO[Either[A, B]]
     #+end_src
     * One particularly useful kind of ~race~ is _a /timeout/ for an /effect/:_
       we *race* the /effect/ against a corresponding _“sleep” effect_. If the
       *sleep finishes BEFORE* the /main effect/, a /timeout/ has occurred.
       + Example 20. =control/Timeout.scala=
         #+begin_src scala
           package com.innerproduct.ee.control

           import cats.effect._
           import cats.effect.implicits._
           import com.innerproduct.ee.debug._
           import scala.concurrent.duration._

           object Timeout extends IOApp {
             def run(args: List[String]): IO[ExitCode] =
               for {
                 done <- IO.race(task, timeout)
                 _    <- done match {
                   case Left(_)  => IO("    task: won").debug
                   case Right(_) => IO("timeouot: won").debug
                 }
               } yield ExitCode.Success

             val task: IO[Unit]    = annotatedSleep("   task", 100.millis)
             val timeout: IO[Unit] = annotatedSleep("timeout", 500.millis)

             def annotatedSleep(name: String, duration: FiniteDuration): IO[Unit] =
               {
                 IO(s"$name: starting").debug *>
                   IO.sleep(duration) *>
                   IO(s"$name: done").debug
               }.onCancel(IO(s"$name: cancelled").debug.void).void
           }
         #+end_src
         - =FIXME=
           Entry 4, "task was cancelled" now is in _monospace font_. This is not right.
           Only "task" should be _monospace font_.

         - ~IO.race~ *races* two /effects/, and returns the value of the first to finish.
           *The loser of the race is cancelled.*
           =IMPORTANT=

         - =IMPORTANT=
           This pattern is so common there's a /built-in combinator/: ~IO.timeout~.
           #+begin_src scala
             done <- IO.race(task, timeout)
             _    <- done match {
               case Left(_)  => IO("    task: won").debug
               case Right(_) => IO("timeouot: won").debug
             }
           #+end_src

           can be replaced with
           #+begin_src scala
             _ <- task.timeout(500.millis)
           #+end_src

         - A ~java.util.concurrent.TimeoutException~ can be raised
           if the /effect/ takes longer than the _timeout duration_.
           =from Jian=
           #+begin_src scala
             // [ioapp-compute-1]    task: starting
             // [ioapp-compute-2]    task: cancelled
             // java.util.concurrent.TimeoutException: 50 milliseconds
             //     at timeout @ com.innerproduct.ee.control.Timeout$.run(Timeout.scala:11)
             //     at map @ com.innerproduct.ee.control.Timeout$.run(Timeout.scala:11)
             //     at main$ @ com.innerproduct.ee.control.Timeout$.main(Timeout.scala:8)
           #+end_src

         - If you do want to
           *act when a timeout occurs*
           INSTEAD OF only having the /effect/ _canceled (=from Jian= and throw an exception),_
           you could use the ~IO.timeoutTo~ method which lets you provide an
           _alternative ~IO~ value_ to evaluate if the timeout expires.

*** DONE 4.4.1. Racing without automatic cancelation - 75
    CLOSED: [2021-08-17 Tue 21:40]
    ~IO.race~ is built upon a _SIMPLER_ /combinator/, ~IO.racePair~,
    which *doesn't provide cancelation* of _the "losing" effect_.
      Instead you receive the _"winning" value_ along with the ~Fiber~ of the
    _race "loser"_, so you can decide what you want to do with it.
    #+begin_src scala
      def racePair[A, B](lh: IO[A], rh: IO[B])
                        (implicit cs: ContextShift[IO]): IO[Either[(A, Fiber[IO, B]), (Fiber[IO, A], B)]]
    #+end_src

    - With ~racePair~, we can complete our implementation of _cancelation-on-error_
      for ~myParMapN~:
      #+begin_src scala
        def myParMapN[A, B, C](ia: IO[A], ib: IO[B])(f: (A, B) => C): IO[C] =
          IO.racePair(ia, ib).flatMap {
            case Left((a, fb))  => (IO.pure(a), fb.join).mapN(f)
            case Right((fa, b)) => (fa.join, IO.pure(b)).mapN(f)
          }
      #+end_src

    - We're _DONE_ with ~myParMapN~:
      ☑ start both the ~ia~ and ~ib~ computations so they run concurrently (“fork” them);
      ☑ wait for each result;
      ☑ cancel the “other” effect if ~ia~ or ~ib~ fails; and
      ☑ finally combine the results with the ~f~ function.

    - If you feel a bit cheated relying on ~racePair~ to _REGISTER the cancelation_
      for us, that’s alright, you’re entitled to feeling that way --
      ~Fiber~ itself *doesn't give us enough control* to implement _cancelation-on-error_.
      (=from Jian= this is one sub-conclusion we get, but not the main
      conclusion, the implementation of ~myParMapN~, of this section).

** DONE 4.5. Summary - 76
   CLOSED: [2021-08-17 Tue 21:48]
   1. /Concurrency/ allows us to CONTROL running computations.

   2. A ~Fiber~ is our handle to this CONTROL.
      After we /start/ a concurrent computation,
      we can
      - /cancel/ it or
      - /join/ it (wait for completion).

   3. /Concurrently executing effects/ *can be* /cancelled/.
      _Cancelled effects_ are expected to *stop executing* VIA
      implicit or explicit /cancelation boundaries/.

   4. We can *race* two computations to know who finished first.
      /Higher-order effects/ like timeouts (~timeout~, ~timeoutTo~, etc.) can be
      _constructed_ using *races*.

* DONE 5. Shifting contexts - 77
  CLOSED: [2021-08-21 Sat 17:05]
  - Parallelism makes use of a set of resources to *execute* /effects/.
    * On the /JVM/, this is a /thread pool/:
      /effects/ *execute* on the AVAILABLE /threads/ SIMULTANEOUSLY.
      1. Scala's main abstraction for using /thread pools/ is the
         ~scala.concurrent.ExecutionContext~, and
      2. /Cats Effect/ builds on top of ~scala.concurrent.ExecutionContext~
         to implement /parallelism/ and /concurrency/.

  - In this chapter we'll explore
    * HOW these /contexts/ are used by our ~IOApp~ programs
    * HOW DIFFERENT kinds of work -- *blocking* vs. *non-blocking* -- can require
      _DIFFERENT /execution strategies/._ =IMPORTANT=

** DONE 5.1. How much parallelism can we get? - 77
   CLOSED: [2021-08-19 Thu 12:45]
   So far our /parallel/ and /concurrent/ code has used whatever /threads/ our
   ~IOApp~ gives us.
   - Q :: *How much* work can we really do with it?
   - Q :: For example,
          if we try to run a lot of /effects/ _in parallel_,
          *HOW Many ACTUALLY run _in parallel_?*

   Let's experiment:

   - Example 21. =contexts/Parallelism.scala=:
     How MANY /effects/ can run _in parallel_?
     #+begin_src scala
       package com.innerproduct.ee.contexts

       import cats.effect._
       import cats.implicits._
       import cats.innerproduct.ee.debug._

       object Parallelism extends IOApp {
         def run(args: List[String]): IO[ExitCode] =
           for {
             _ <- IO(s"number of CPUs: $numCpus").debug
             _ <- tasks.debug
           } yield ExitCode.Success

         val numCpus = Runtime.getRuntime().availableProcessors()
         val tasks = List.range(0, numCpus * 2).parTraverse(task)
         def task(i: Int): IO[Int] = IO(i).debug
       }

       // [ioapp-compute-0] number of CPUs: 8
       // [ioapp-compute-1] 1
       // [ioapp-compute-7] 7
       // [ioapp-compute-5] 5
       // [ioapp-compute-4] 4
       // [ioapp-compute-2] 2
       // [ioapp-compute-3] 3
       // [ioapp-compute-5] 8
       // [ioapp-compute-3] 9
       // [ioapp-compute-5] 11
       // [ioapp-compute-6] 6
       // [ioapp-compute-5] 15
       // [ioapp-compute-0] 0
       // [ioapp-compute-4] 14
       // [ioapp-compute-3] 13
       // [ioapp-compute-7] 12
       // [ioapp-compute-1] 10
       // [ioapp-compute-1] List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
     #+end_src
     * =from Jian=
       In my laptop, replace ~numCpus * 2~ with ~numCpus * 3~, and the result can
       be more informative.
         When using ~numCpus * 2~, because this number, compared with the ~numCpus~,
       is so small, most probably size of /threads/ are NOT all used.
       + The ~numCpus * 3~ is also good in my laptop, because it only has 4 cores,
         and ~numCpus * 3~ is not too big to view.

     * From our debug information we are using *EIGHT* /threads/, which is the
       *SAME* as our ~numCpus~.
         We had _more than_ ~numCpus~ tasks, so that must mean that our _UNDERLYING
       /thread pool/ has AT MOST ~numCpus~ /threads/._

     * At the same time, we ran ~parTraverse~ with ~numCpus * 2~ as many /effects/
       as CPUs.
       + Q :: How does the system ensure _ALL_ the /effects/ are run?

       + A :: When we *compose* /effects/ _in parallel_, during execution
         1. each /effect/ is ONLY *scheduled to be executed*, and
         2. a *separate* /asynchronous process/ is responsible for executing the
            _scheduled effects_ on an available /thread/.
         3. When a /thread/ finishes its work, *another* /effect/ is executed on it.

     * SUMMARY:
       + In Scala this exactly maps to an ~ExecutionContext~,
         which *encapsulates* both
         - a queue of scheduled tasks and
         - a set of /threads/ used to execute them.

       + In /Cats Effect/, every ~IOApp~ has a *default* ~ExecutionContext~, and
         on the JVM it is constructed as
         _a *fixed* /pool/ BASED ON the number of available CPUs._

         - In all of our ~IOApp~-based examples we've been using this *hidden*
           /thread pool/.

** DONE 5.2. The need for multiple contexts - 78
   CLOSED: [2021-08-21 Sat 00:07]
   - Q :: Our computers regularly do *more than* ~numCpus~ things at the same time.
          _How can we reconcile these disparate ideas?_

   - A :: The solution on the JVM is /threads/.
          We can have many /threads/ running, and their execution _is *multiplexed*
          across_ the available cores from the operating system.
            And we _can *pool* those /threads/ *into* logical groups_ with data
          types like ~ExecutionContext~, where /threads/ in one /pool/ are
          *isolated* from those in another.
     * =FIXME= available cores available

   - Two kinds of computations:
     * *CPU-bound*:
       Compute with /pure values/.
       /Threads/ will compete to be run and whatever priorities and fairness
       algorithms will be applied to ensure we make progress.

     * *I/O-bound* (Here I/O refers to input/output, not the ~cats.effect.IO~
       /effect type/):
       *Interact with* the _external environment_, like
       _reading from a file_ or _writing to the network_, our /threads/ can *become
       /blocked/:*
       + Data may not be available yet,
       + the network hasn't acknowledged receiving anything yet,
       + and so on.

   - When a /thread/ is *blocked*, the JVM *suspends* its execution.
     Then another /thread/ can be executed by the operating system.
     * Q :: However, since there can be _limits to the number_ of possible
            /threads/, what if ALL these /threads/ are *blocked*?
       + If that happens, we can't use any available cores to do /CPU-bound/ work.

     * A :: To ensure our programs make progress -- ensuring work proceeds when
            I/O-bound work is blocked --
            we'll *isolate* the /CPU-bound work/ *from* any _I/O-bound tasks_ by
            _having *separate* /pools/._

       + The _Cats Effect library_ supports this pattern by
         ENCOURAGING *separate* /contexts/:
         + /CPU-bound work/ will be scheduled on /a *fixed-size* thread pool/,
           where _the number of /threads/ is the number of /cores/ available to the
           JVM._
             All things being equal, you *can't* compute _more than <number of
           CPUs>_ things at a time, so don't try to do more.

         + _I/O-bound work_ will be scheduled on /an *unbounded* thread pool/
           so that
           /blocked threads/ MERELY take up /memory/
           _INSTEAD OF_ stopping the progress of other tasks.

     * In an ~IOApp~ on the JVM the *DEFAULT ~ExecutionContext~ is configured
       for /CPU-bound/ work.*
       + footnote 17:
         The default ~ExecutionContext~ in an ~IOApp~ uses /daemon threads/
         SO THAT
         if the /top-level effect/ (specified by the ~run~ method of ~IOApp~) completes,
         _ANY_ /concurrently executing effects/ do *NOT prevent* the application *from* exiting

       + =TODO= The next section will answer the question:
              what /context/ do we use for _I/O bound_ work?

** DONE 5.3. Contexts for I/O-bound actions - 80
   CLOSED: [2021-08-21 Sat 02:47]
   - The reason we define ~Blocker~:
     We could instantiate our own ~ExecutionContext~ to use for _blocking I/O effects_,
     configuring it to use an /unbounded thread pool/.
     *BUT*
     it would be _somewhat DIFFICULT to properly use_
     SINCE it has the *same type* as any other ~ExecutionContext~ value, like the
     /default context/ provided for /CPU-bound/ work by ~IOApp~.
     * We could easily *pass the WRONG* /context/ *to* a /method/.

     * If instead we had a /context/ with a *DIFFERENT* /type/ we couldn't make
       that mistake. Luckily _Cats Effect 2_ provides the exact solution: ~Blocker~

     * =NOTE=:
       Managing /blocking effects/ with _Cats Effect 3_ is discussed below.

   - ~Blocker~ is a _small wrapper_ around an ~ExecutionContext~.
     Let's demonstrate
     1. *creating* a ~Blocker~ and
     2. using it to *execute* /effects/ in the /blocking context/.

   - Example:
     #+begin_src scala
       package com.innerproduct.ee.contexts

       import cats.effect._
       import com.innerproduct.ee.debug._

       object Blocking extends IOApp {

         def run(args: List[String]): IO[ExitCode] =
           Blocker[IO].use { blocker =>  // 1
             withBlocker(blocker).as(ExitCode.Success)
           }

         def withBlocker(blocker: Blocker): IO[Unit] =
           for {
             _ <- IO("on default").debug
             _ <- blocker.blockOn(IO("on blocker").debug)  // 2
             _ <- IO("where am I?").debug  // 3
           } yield ()

       }
     #+end_src
     1. We *can't directly instantiate* a ~Blocker~,
        BUT we can use ~Blocker.apply[IO]~ to create a ~Resource[IO, Blocker]~
        that manages the UNDERLYING /thread pool/ used for /blocking computations/.
          To access the ~Blocker~ itself, we use the ~Resource~, passing it a
        function consumes the ~Blocker~ and produces an /effect/. We'll discuss
        ~Resource~ in more depth in _Chapter 7, Managing resources_.

     2. To execute our /effect/ on the /blocking context/, we provide it to the
        ~blockOn~ /method/ of the ~Blocker~.
        * We attach the ~debug~ call on ~IO("on blocker")~, and we can see it is
          running in a /blocking context/.

        * =from Jian=
          First add one more ~debug~ method:
          #+begin_src scala
            def debug(prefix: String): IO[A] =
              for {
                a <- ioa
                tn = Thread.currentThread.getName
                _ = println(s"[${Colorize.reversed(tn)}] ($prefix) $a") // <1>
              } yield a
          #+end_src

          Let's modify this line, and print more debug info:
          #+begin_src scala
            _ <- blocker.blockOn(IO("on blocker").debug("inside blockOn")).debug("from blockOn")
          #+end_src

          Then we can see logs like:
          #+begin_src text
            [ioapp-compute-0] on default
            [cats-effect-blocker-0] (inside blockOn) on blocker
            [ioapp-compute-1] (from blockOn) on blocker
            [ioapp-compute-1] where am I?
          #+end_src
          This is why we must call ~debug~ from ~IO("on blocker")~, and only call
          ~debug~ from ~blocker.blockOn(...)~ can't show us the /context/ _CHANGE_
          =[cats-effect-blocker-0]=.

     3. Subsequent /effects/ execute on _the ORIGINAL /context/,_ *not* the
        /blocking/ one.

   - We used the ~blockOn~ method of ~Blocker~ to declare an existent /effect/
     should run on the /blocking context/.
       However, if we wanted to *CREATE a /blocking effect/ directly*, in one
     step, we could use the ~delay~ method of ~Blocker~, analogous to the
     ~IO.delay~
     method: =FIXME= "f we wanted" should be "if we wanted".
     #+begin_src scala
       import scala.util.chaining._
       import scala.language.implicitConversions

       def blockingDebug[A](blocker: Blocker, a: => A): IO[A] =
         blocker.delay {
           a.tap {
             v => println(s"[${Thread.currentThread.getName}] $v")
           }
         }
     #+end_src

*** DONE 5.3.1. Declaring blocking effects in Cats Effect 3 - 82
    CLOSED: [2021-08-21 Sat 02:47]
    Instead of using a ~Blocker~ _backed by a special ~ExecutionContext~,_
    _Cats Effect 3_ gives us a dedicated /effect constructor/ to declare an
    /effect/ as /blocking/ as early as possible:
    #+begin_src scala
      val withBlocker: IO[Unit] =
        for {
          _ <- IO("on default").debug
          _ <- IO.blocking("on blocker").debug  // 1
        } yield ()
    #+end_src
    1. We *DIRECTLY declare* a /blocking effect/ with ~IO.blocking~.
       _But be careful_,
       the ~debug~ call here will run on the /default context/,
       *not* the /blocking/ one,
       _BECAUSE_ /blocking effects/ *ALWAYS shift BACK to the PREVIOUS /context/.*

** DONE 5.4. How do you know something is blocking? - 82
   CLOSED: [2021-08-21 Sat 02:55]
   - We now have a separate strategy for executing /blocking effects/ using ~Blocker~.
     But how do we know what we're doing is blocking or not? Daniel Spiewak,
     long-time Scala contributor and a maintainer of Cats Effect, offers us a
     heuristic:
     #+begin_quote
       if something does _NOT_ have a /callback API/, then you know it's /blocking/
                            -- Daniel Spiewak, Cats Effect gitter.im chatroom
     #+end_quote

   - The idea being:
     a /callback API/ allows the API to return immediately
     so _the caller is *NOT* /blocked/_ while the API is computing the result;

     THEREFORE if there isn't such a /callback API/, then the method is _PROBABLY_
     /blocking/.

     * /Methods/ that return values that themselves have a /callback API/,
       such as ~scala.concurrent.Future~ or ~IO~, would imply those /methods/ are
       *not blocking*.

   - *Exercise 3: Collect some blocking APIs*
     =TODO=

** DONE 5.5. Finer-grained control of contexts - 83
   CLOSED: [2021-08-21 Sat 16:59]
   - _Cats Effect_ encourages a *coarse* (but useful!) distinction for executing
     /effects/:
     they are either *CPU-bound* or *I/O bound*, and are assigned by the programmer
     to either the _default or blocking ~ExecutionContext~,_ respectively.

     * *BUT*
       there are TWO other scenarios that may occur that involve the relationship
       between an /effect/ and its /execution context/:
       *long-running /effects/,* and /effects/ that need to be executed in
       *neither* the /default/ *nor* /blocking context/.

   - Re-examine a /long-running effect/, the _ticking clock_:
     #+begin_src scala
       val tickingClock: IO[Unit] =
         for {
           _ <- IO(System.currentTimeMillis()).debug
           _ <- IO.sleep(1.second)
           _ <- tickingClock
         } yield ()
     #+end_src
     * Q :: Does this /effect/ execute on one /thread/, forever?
     * A :: (The answer is NO. Explain soon.)
            If it did, that would be *BAD*,
            BECAUSE ~tickingClock~ isn't really doing anything other than sleeping,
            and so to *hoard* the current /thread/ for such an /effect/ would *make
            _one LESS_ /thread/ available* for other /effects/ to execute on, reducing
            the amount of work our applications can perform.

   - TO ENSURE
     a /recursive loop/ does *NOT* steal a /thread/ and never give it back,

     * we'd like to be able to declare, as an /effect/ itself, *“reschedule the
       remainder of the computation”*.
       + Not only
         would this resume the computation on (potentially) *ANOTHER* /thread/
         when the resumption is executed by the context,

       + but
         it then allows other scheduled /effects/ to re-use the *PREVIOUS* /thread/.

     * In other words,
       the /CURRENT effect/ is
       1. “suspended” and
       2. sent “to the back of the line”,
       which _PREVENTS_ other /effects/ from being *“starved”* of a /thread/.

   - In _Cats Effect_,
     this notion of *“reschedule the remainder of the computation”* is an instance
     of a larger concept, an /asynchronous boundary/. =IMPORTANT= =IMPORTANT=

   - From the point of view of the *composed* /effect/,
     the /boundary/ *marks* WHERE the runtime could *reschedule* the computation
     to resume on ANOTHER /thread/.

   - We can produce an /asynchronous boundary/ with the ~IO.shift~ method:
     #+begin_src scala
       package com.innerproduct.ee.contexts

       import cats.effect._
       import com.innerproduct.ee.debug._

       object Shifting extends IOApp {

         def run(args: List[String]): IO[ExitCode] =
           for {
             _ <- IO("one").debug
             _ <- IO.shift
             _ <- IO("two").debug
             _ <- IO.shift
             _ <- IO("three").debug
           } yield ExitCode.Success
       }

       // [ioapp-compute-0] one
       // [ioapp-compute-1] two
       // [ioapp-compute-2] three
     #+end_src

   - It's time to return the _ticking clock_ example:
     Recall its output
     #+begin_src text
       [ioapp-compute-0] 1607561985119
       [ioapp-compute-1] 1607561986128
       [ioapp-compute-2] 1607561987130
     #+end_src
     We see /context shift/ even though we did _not explicitly add_ an
     /asynchronous boundary/ with ~IO.shift~.

     * Even though we can't see it,
       there are /asynchronous boundaries/ composed with our _ticking clock_.
         They are introduced by the ~IO.sleep~ /effect/, and
       _if you think about it, this MAKES SENSE,_
       SINCE if we actually *blocked* the CURRENT /thread/ for the duration of
       the ~sleep~, we'd be preventing that /thread/ from being used by other
       /effects/.

*** DONE 5.5.1. Shifting with multiple contexts - 85
    CLOSED: [2021-08-21 Sat 16:58]
    We've made /long-running effects/ more fair with respect to other concurrently
    executing effects.
      What about /effects/ we want to run
    *neither* on the /default context/, *nor* on a /blocking one/?

    - (You might need this, for example, when integrating with a library that
      MANAGES *its own /thread pools/.*)
        Luckily, we can expand the notion of an /asynchronous boundary/ so that
      we can *specify* a PARTICULAR /context/ to resume our computation on, rather
      than the “current” one.
      * Concretely, ~IO.shift~ can take an optional ~ExecutionContext~:
        #+begin_src scala
          package com.innerproduct.ee.contexts
          import cats.effect._
          import com.innerproduct.ee.debug._
          import java.util.concurrent.Executors
          import scala.concurrent.ExecutionContext

          object ShiftingMultiple extends IOApp {

            def run(args: List[String]): IO[ExitCode] =
              (ec("1"), ec("2")) match {
                case (ec1, ec2) =>
                  for {
                    _ <- IO("one").debug
                    _ <- IO.shift(ec1)
                    _ <- IO("two").debug
                    _ <- IO.shift(ec2)
                    _ <- IO("three").debug
                  } yield ExitCode.Success
              }

            // create a new single-threaded ExecutionContext
            def ec(name: String): ExecutionContext =  // 5
              ExecutionContext.fromExecutor(Executors.newSingleThreadExecutor { r =>
                val t = new Thread(r, s"pool-$name-thread-1")
                t.setDaemon(true)  // Need daemon threads so the JVM shuts down correctly.
                t
              })

          }
        #+end_src
        + The set /daemon threads/ to shuts down the JVM correctly will be
          explained in _Chapter 7, Managing resources_

** DONE 5.6. Example: contexts for database access in Doobie - 87 - =TODO= =NOTE=
   CLOSED: [2021-08-21 Sat 16:59]
   Try to learn Doodie

** DONE 5.7. Summary - 88
   CLOSED: [2021-08-21 Sat 17:05]
   1. /Threads/ abstract over what is concurrently executing atop the available set
      of /processors/, so we can have _many more /threads/ than CPUs._
      * A ~scala.concurrent.ExecutionContext~ represents a /scheduling queue/
        along with a set of /threads/ used for computation.

   2. /Asynchronous boundaries/ help to ensure applications make progress in the
      presence of /long-running effects/ by *rescheduling* the remainder of the
      /effect/.
        At the same time we can specify a computation to *resume on a DIFFERENT
      context* in order to isolate various workloads from one another.

   3. ~IOApp~ provides a *DEFAULT* ~ExecutionContext~ with a fixed number --
      the number of CPUs on the machine -- of /threads/.
        This is meant for /CPU-bound (non-blocking) work/.

   4. _I/O-bound work_, which is USUALLY SLOWER than _CPU-bound work_
      BECAUSE it will *block* the /thread/ it uses, should run in a pool _SEPARATE
      from_ _CPU-bound work_.
        _Blocking I/O-bound work_ should be run in an /UNBOUNDED thread pool/.

      * _Cats Effect 2_ provides the ~Blocker~ interface to declare /effects that block/.

* DONE 6. Integrating asynchrony - 90 - =TODO - NOTE=
  CLOSED: [2021-07-26 Mon 02:20]
  It is not possible to migrate to ~IO~ overnight.
    We need to work with the already used /built-in types/ like
  ~scala.concurrent.Future~, along with other libraries to write /parallel and
  concurrent code/.

  - Q :: How can we wrap them to instead produce ~IO~ values?
  - A :: To answer this we'l discuss Cats Effect ~IO.async~ method, which uses
         the general pattern of /continuation/ passing to integrate any kind of
         /asynchronous processing interface/.

** DONE 6.1. Asynchronous callbacks - 90 - =TODO= =RE-READ= =DON'T QUITE UNDERSTAND!!!=
   CLOSED: [2021-07-26 Mon 02:09]
   Use ~IO.async~ to construct an IO value from a /callback-based API/.

   - Remember:
     an API that provides /callbacks/ implies that computation is happening
     asynchronously.
       After you provide a /callback/, you can do other work, and the /callback/
     will typically be executed on some other /thread/ once the computation
     completes.

   - ~async~
     #+begin_src scala
       def async[A](k: (Either[Throwable, A] => Unit) => Unit): IO[A]
     #+end_src
     * Simplify the /type signature/ by create /type alias/:
       #+begin_src scala
         type Callback[A] = Either[Throwable, A] => Unit
         def async[A](k: CallBack[A] => Unit): IO[A]
       #+end_src

   - It's possible to use ~IO.async~ to specify a _completely synchronous_
     computation by immediately computing the result and passing it to the
     /callback/:
     #+begin_src scala
       def synchronousSum(l: Int, r: Int): IO[Int] =
         IO.async { cb =>
           cb(Right(l + r))
         }
     #+end_src

*** DONE 6.1.1. Tracing an asynchronous execution - 91
    CLOSED: [2021-07-26 Mon 02:09]
    To demonstrate ~IO.async~ let's create a new _asynchronous IO value_ that uses
    some /callback-based API/ -- in this case, ~Future~.
      We'll reproduce what ~IO.fromFuture~ does to adapt to the ~Future~ type
    using ~IO.async~:

    - Example 26. Using ~async~ to adapt a ~Future~ to ~IO~:
      #+begin_src scala
        trait API {
          def compute: Future[Int] = ???
        }

        def doSomething[A](api: API)(implicit ec: ExecutionContext): IO[Int] =
          IO.async[Int] { cb =>
            api.compute.onComplete {
              case Failure(t) => cb(Left(t))
              case Success(a) => cb(Right(a))
            }
          }.guarantee(IO.shift)
      #+end_src

    - Let's *walk through*
      _what happens when we execute an /effect/ built with ~IO.async~:_
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      =TODO= =RE-READ=
      #+begin_src scala
        val api = new API { ... }
        val ds = doSomething(api)

        ds.unsafeRunSync()
      #+end_src
      * In the following description, we call the block given to ~IO.async~ as ~k~.
        #+begin_src scala
          val k: (Either[Throwable, Int] => Unit) => Unit =
            cb => api.compute.onComplete {
              case Failure(t) => cb(Left(t))
              case Success(a) => cb(Right(a))
            }
        #+end_src

    - *Exercise 4:* ~java.util.concurrent.CompletableFuture~
    - *Exercise 5:* Never!

** DONE 6.2. Integrating with ~Future~ - 95
   CLOSED: [2021-07-26 Mon 02:15]
   ~scala.concurrent.Future~ is the most common legacy data type for
   asynchronous computation in Scala.

   - We've seen we can use ~IO.async~ to implement an IO value in terms of an
     asynchronously executing ~Future~.

   - However, since it's so common, Cats Effect provides a built-in method:
     ~IO.fromFuture~:
     #+begin_src scala
       def asFuture(): Future[String] =
         Future.successful("woo!")

       val asIO: IO[String] =
         IO.fromFuture(IO(asFuture))
     #+end_src

   - *Exercise 6*: Why does ~IO.fromFuture~ require a ~Future~ inside an ~IO~?

** DONE 6.3. Summary - 96
   CLOSED: [2021-07-26 Mon 02:19]
   1. ~IO.async~ allows us to *build* /effects/ that
      (1) can start asynchronous processes;
      (2) can emit one result on completion or can end in error.

   2. /Asynchronous effects/ fundamentally rely upon /continuation passing/,
      where the _ACTUAL asynchronous computation_ is given code to run
            when the computation completes.

   3. ~scala.concurrent.Future~ is a common source of /asychronous computation/.
      ~IO.fromFuture~ transforms a ~Future~ into a /referentially-transparent effect/.

* DONE 7. Managing resources - 97
  CLOSED: [2021-08-23 Mon 00:57]
  - Problem:
    Some state needs to be managed.
    For example,
    * a /thread pool/
      needs to be _allocated_ and _configured_ BEFORE it can be used,
      and
      once we're done with it the /threads/ need to be shut down.

    * A _network connection_ maintains a connection to a remote system over some
      socket networking abstraction.
        _Allocating sockets_ may be expensive, in addition to the time needed to
      actually establish a (remote) connection. And those /sockets/ need to be
      reclaimed when they aren't needed anymore.

    * A _database connection_, like a _network connection_, also needs to talk to
      a remote system, and will have similar costs like the previous example.
        It may also manage its own, _ADDITIONAL resources_, such as /threads/,
      involved in the connection protocol.

  - In _Cats Effect_, the ~Resource~ /data type/ represents this *acquire-use-release
    pattern* to manage /state/.

  - We'll explore
    * how to *create* our own ~Resource~ values,
    * how to *compose* them, and
      then learn how to use them in our applications for _dependency management_.

** DONE 7.1. Creating a ~Resource~ to manage state - 97
   CLOSED: [2021-08-22 Sun 23:44]
   - ~Resource.make~ takes _TWO_ /effectful arguments/:
     #+begin_src scala
       def make[A](aquire: IO[A])(relase: A => IO[Unit]): Resource[IO, A]
     #+end_src
     Two parameters
     1. produce (aquire) the /state/
     2. release the /state/

   - Example 27. Making and using a basic ~Resource~.
                 Code available at =resources/BasicResource.scala=.
     #+begin_src scala
       package com.innerproduct.ee.resources

       import cats.effect._
       import com.innerproduct.ee.debug._

       object BasicResource extends IOApp {
         def run(args: List[String]): IO[ExitCode] =
           stringResource.use { s =>
             IO(s"$s is so cool!").debug
           }.as(ExitCode.Success)

         val stringResource: Resource[IO, String] =
           Resource.make(
             IO("> acquiring stringResource").debug *>
               IO("String")
           )(_ => IO("< releasing stringResource").debug.void)
       }

       // [ioapp-compute-0] > acquiring stringResource
       // [ioapp-compute-0] String is so cool!
       // [ioapp-compute-0] < releasing stringResource
     #+end_src

   - It's important to note that ~Resource~ does *NOT* perform any /effects/ itself.
     When we use it, it produces a NEW /effect/.
     * You can think of ~Resource~ as a tiny DSL (domain-specific language) to
       + describe /resource management/ in terms of INDIVIDUAL ~IO~ /effects/ and
       + the use method then “compiles” those instructions into a single ~IO~ value.

   - Additionally the *release* /effect/ will
     _not only_ be executed if the use /effect/ completes successfully,
     _but also_ if it raises an error:
     * Example 28.
       A resource is properly released even when the use effect fails.
       Code available at =resources/BasicResourceFailure.scala=
       #+begin_src scala
         package com.innerproduct.ee.resources
         import cats.effect._
         import com.innerproduct.ee.debug._

         object BasicResourceFailure extends IOApp {
           def run(args: List[String]): IO[ExitCode] =
             stringResource
               .use(_ => IO.raiseError(new RuntimeException("oh noes!")))  // 1
               .attempt
               .debug
               .as(ExitCode.Success)
           val stringResource: Resource[IO, String] =
             Resource.make(
               IO("> acquiring stringResource").debug *> IO("String")
             )(_ => IO("< releasing stringResource").debug.void)
         }

         // [ioapp-compute-0] > acquiring stringResource
         // [ioapp-compute-0] < releasing stringResource
         // [ioapp-compute-0] Left(java.lang.RuntimeException: oh noes!)
       #+end_src

*** DONE 7.1.1. Example: Ensuring a file handle is closed - 99
    CLOSED: [2021-08-22 Sun 22:28]
    Example 29.
    Using a ~Resource~ to hide and manage the state of a file to be read.
    Code available at =resources/FileBufferReader.scala=.
    #+begin_src scala
      package com.innerproduct.ee.resources

      import cats.effect._
      import java.io.RandomAccessFile

      class FileBufferReader private (in: RandomAccessFile) {  // 1
        def readBuffer(offset: Long): IO[(Array[Byte], Int)] =  // 2
          IO {
            in.seek(offset)
            val buf = new Array[Byte](FileBufferReader.bufferSize)
            val len = in.read(buf)
            (buf, len)
          }

        private def close: IO[Unit] = IO(in.close())  // 3
      }

      object FileBufferReader {
        val bufferSize = 4096

        def makeResource(fileName: String): Resource[IO, FileBufferReader] =  // 4
          Resource.make {
            IO(new FileBufferReader(new RandomAccessFile(fileName, "r")))
          }(_.close)
      }
    #+end_src

*** DONE 7.1.2. Example: Canceling a background task - 101
    CLOSED: [2021-08-22 Sun 23:44]
    Use a ~Resource~ is to manage the lifecycle of a /background task/.

    - For example,
      we may want to
      1. *fork* some /(often non-terminating) effect/, and
      2. later *cancel* it when it isn't required to run anymore.

    - This suggests we can combine use of a ~Fiber~ with a ~Resource~,
      where the ~Resource~ effects are defined as:
      * *acauire* - _start_ the task, producing a ~Fiber~
      * *release* - _cancel_ the ~Fiber~

    - Then the lifetime of the background task would directly correspond to the
      execution of the ~use~ /effect/ of the ~Resource~:
      * Example 30.
        Scoping the lifetime of a background task with Resource.
        Source at =resources/ResourceBackgroundTask.scala=.
        #+begin_src scala
          package com.innerproduct.ee.resources

          import cats.effect._
          import cats.implicits._
          import com.innerproduct.ee.debug._
          import scala.concurrent.duration._

          object ResourceBackgroundTask extends IOApp {
            def run(args: List[String]): IO[ExitCode] =
              for {
                _ <- backgroundTask.use { _ =>
                  IO("other work while background task is running").debug *>
                    IO.sleep(200.millis) *>
                    IO("other work done").debug  // 1
                }
                _ <- io("all done").debug
              } yield ExitCode.Success

            val backgroundTask: Resource[IO, Unit] = {
              val loop =
                (IO("looping...").debug *> IO.sleep(100.millis))
                  .foreverM  // 2

              Resource
                .make(IO("> forking backgroundTask").debug *> loop.start)(  // 3
                  IO("< canceling backgroundTask").debug.void *> _.cancel  // 4
                ).void  // 5
            }
          }

          //  [ioapp-compute-0] > forking backgroundTask
          //  [ioapp-compute-1] looping...
          //  [ioapp-compute-0] other work while background task is running
          //  [ioapp-compute-2] looping...
          //  [ioapp-compute-3] looping...
          //  [ioapp-compute-4] other work done
          //  [ioapp-compute-4] < canceling backgroundTask
          //  [ioapp-compute-4] all done
        #+end_src
        * About ~foreverM~
          #+begin_src scala
            val loop: IO[Noting] = step.flatMap(_ => loop)

            // is equivalent to

            val loop: IO[Noting] = step.foreverM
          #+end_src

    - The "background task" pattern is common,
      _Cats Effect_ defines the ~background~ method on an ~IO~:
      #+begin_src scala
        def background: Resource[IO, IO[A]]
      #+end_src
      The we can replace ~Resource.make(loop.start)(_.cancel)~ with
      ~loop.background~.

    - =IMPORTANT=
      The ~background~ method *corrects*
      a latent problem with _manually managing Fibers_ --
      they may “leak” if they aren't properly canceled.
      * For example:
        #+begin_src scala
          def leaky[A, B](ia: IO[A], ib: IO[B]): IO[(A, B)] =
            for {
              fiberA <- ia.start
              fiberB <- ib.start
              a <- fiberA.join  // 1
              b <- fiberB.join
            } yield (a, b)
        #+end_src
        If ~ia~ raises an _error_, ~fiberA.join~ will fail and ~fiberB~ will still
        be allocated and running.

** DONE 7.2. Composing managed state - 104
   CLOSED: [2021-08-23 Mon 00:33]
   - Q :: Can we build a ~Resource~ from other ~Resource~(s)?
   - A :: We can compose in multiple ways:
     * ~Resource~ is a /functor/:
       #+begin_src scala
         val resA: Resource[IO, A] = ???
         val resB: Resource[IO, B] = resA.map(makeB)

         def makeB(a: A): B = ???
       #+end_src

     * ~Resource~ is a /applicative/:
       #+begin_src scala
         val resD: Resource[IO, D] =
           (resB, resC).mapN(makeD)

         def makeD(b: B, c: C): D = ???
       #+end_src

     * ~Resource~ is a /monad/:
       #+begin_src scala
         val resC: Resource[IO, C] =
           for {
             a <- resA
             c <- makeC(a)
           } yield c

         def makeC(a: A): Resource[IO, C] = ???
       #+end_src

   - Example 31.
     Composing multiple resources.
     Code available at =resources/BasicResourceComposed.scala=.
     #+begin_src scala
       package com.innerproduct.ee.resources

       import cats.effect._
       import cats.implicits._
       import com.innerproduct.ee.debug._

       object BasicResourceComposed extends IOApp {
         def run(args: List[String]): IO[ExitCode] =
           (stringResource, intResource).tupled  // 2
             .use {
               case (s, i) =>  // 2
                 IO(s"$s is so cool!").debug *>
                   IO(s"$i is also cool!").debug
             }.
             as(ExitCode.Success)

         val stringResource: Resource[IO, String] =
           Resource.make(
             IO("> acquiring stringResource").debug *> IO("String")
           )(_ => IO("< releasing stringResource").debug.void)

         val intResource: Resource[IO, Int] =  // 1
           Resource.make(
             IO("> acquiring intResource").debug *> IO(99)
           )(_ => IO("< releasing intResource").debug.void)
       }

       // [ioapp-compute-0] > acquiring stringResource
       // [ioapp-compute-0] > acquiring intResource
       // [ioapp-compute-0] String is so cool!
       // [ioapp-compute-0] 99 is also cool!
       // [ioapp-compute-0] < releasing intResource  // 1
       // [ioapp-compute-0] < releasing stringResource  // 1
     #+end_src
     *Note*:
     The resources are released in the opposite order in which they are acquired.
     =from Jian=
     This is because of the sequantial nature of /monad/, even if we use ~tupled~
     (~mapN((a, b) => (a, b))~), because this ~mapN~ is implemented with ~flatMap~,
     it is sequential.
     #+begin_src text
       acquire a
               acquire b
               release b
       release a
     #+end_src

*** DONE 7.2.1. Parallel resource composition - 106
    CLOSED: [2021-08-23 Mon 00:33]
    - Use ~parTupled~ or other ~par~ prefixed methods.

    - *Exercise 7: Early-release of Resources*
      * Solution: Use that resource as soon as possible, and don't use ~flatMap~.

** DONE 7.3. Resources for dependency management - 109
   CLOSED: [2021-08-23 Mon 00:54]
   - We have _NOT_ discussed
     * how to build *larger* applications with ~IOApp~,
       or
     * what issues may arise as we try to _compose_ sets of /effects/ _into_ a
       *larger* program.

   - Since a ~Resource~ perfectly encapsulates the /effectful allocation and
     clean-up/ of a value,
     _we can use it to *manage our application's dependencies* for us._

     * Our ~IOApp~-based application will then be structured into
       _THREE distinct concerns_:
       1. /Dependency lifecycles/ managed by a single, possibly composed, ~Resource~.

       2. /Application logic/ that uses the _dependencies_ -- *without* any concern
          over their lifecycle.

       3. The _top-level of the application_ *initiates the allocation* of the
          _dependencies_,
          a. they are used by the logic, and
          b. then cleans them up.

   - We've already been using this structure in the previous examples, but let's
     explicitly call out the concerns in another example ~IOApp~-based application:
     * Example 32.
       Structuring an ~IOApp~ into /dependencies/ and /logic/.
       Code available at =resources/ResourceApp.scala=.
       #+begin_src scala
         package com.innerproduct.ee.resources

         import cats.effect._
         import cats.implicits._

         object ResourceApp extends IOApp {
           def run(args: List[String]): IO[ExitCode] =
             resources  // 1
               .use {  // 3
                 case (a, b, c) =>
                   applicationLogic(a, b, c)  // 2
               }.as(ExitCode.Success)

           val resources: Resource[IO, (DependencyA, DependencyB, DependencyC)] =  // 1
             (resourceA, resourceB, resourceC).tupled

           val resourceA: Resource[IO, DependencyA] = ???
           val resourceB: Resource[IO, DependencyB] = ???
           val resourceC: Resource[IO, DependencyC] = ???

           def applicationLogic(  // 2
             a: DependencyA,
             b: DependencyB,
             c: DependencyC
           ): IO[ExitCode] =
             ???
         }

         trait DependencyA
         trait DependencyB
         trait DependencyC
       #+end_src
       1. We
          *compose* a set of /managed dependencies/
          *into* a single ~Resource~ value.

       2. The _application logic_ uses the /dependencies/ --
          BUT does *NOT manage* them.

       3. _At the BEGINNING of our application_
          we use our /managed dependencies/,
          providing them to the _application logic_.
          The /dependencies/ *only exist within* the /scope/ of the ~use~ block.

** DONE 7.4. Summary - 110
   CLOSED: [2021-08-23 Mon 00:57]
   1. The ~Resource~ data type
      captures the pattern
      where the code for /state/ *acquisition* and *release* is separated from code
      that uses the /state/.
      * A ~Resource~ can *be composed into* OTHER ~Resource~ values,
        both /serially/ and in /parallel/.

   2. We can use ~Resource~ to represent the _lifecycle of our application
      /dependencies/._
        We then use them in our ~IOApp~ to *acquire* them during the execution of
      the dependent code, and to ENSURE they are *released*.

* DONE 8. Testing effects - 112
  CLOSED: [2021-08-23 Mon 02:07]
  - /Testing/ is a tremendously complicated and nuanced subject.
    * Since we've been discussing effects like ~IO~,
      which can encapsulate /side effects/ that _by definition can't be observed_,
      testing anything involving ~IO~ is a very *open-ended proposition*.

  - Instead we're going to focus on _TWO_ areas:
    * one fairly simple:
      testing the values produced by an ~IO~ effect

    * the other rather complicated:
      controlling how ~IO~ effects interact with their /runtime dependencies/ like
      ~ExecutionContext~ so we can make assertions about *“when”* their execution
      occurs.

** DONE 8.1. Assertions on effectful values - 112
   CLOSED: [2021-08-23 Mon 01:22]
   - To test the values from /effects/, you have to run them by calling their
     _unsafe-prefixed methods_.
     * Example:
       Test the /effect/ result by running it and check the value
       #+begin_src scala
         def assertGreaterThanZero(i: IO[Int]) =
           assert(i.unsafeRunSync() > 0)
       #+end_src
       + Here we use the Scala build-in ~assert~.
       + You can also use your favorite testing or "matchers" library.

     * Example:
       Test the /effect/ result by composing it with ~assert~, and run it:
       #+begin_src scala
         def assertGreaterThanZero(i: IO[Int]) =
           i.map(j => assert(j > 0)).unsafeRunSync()
       #+end_src

   - =IMPORTANT=
     Remember,
     ~unsafeRunSync~ will *throw an exception*
     if the /effect/ *fails* or is *cancelled*.

     * If
       your testing framework doesn't treat _thrown exceptions_ as failures,
       or
       you want to assert that a _failure_ or _cancellation_ has happened,

       you can use ~attempt~ to
       *lift* the _success value_ or _failure/cancelation exception_
       *into* a successful ~Either~ value:
       #+begin_src scala
         def assertUnsuccessful[A](ia: IO[A]) =
           assert(ia.attempt.unsafeRunSync().isLeft)
       #+end_src

*** DONE 8.1.1. Faking effects with interfaces - 112
    CLOSED: [2021-08-23 Mon 01:21]
    - Q :: By executing the ~IO~ you cause it to perform its /effects/,
           _BUT_ what if during testing you *don't want the actual /effect/ to
           happen*?

    - A :: *Instead of* directly creating an /effect/ to send an email,
           we'll invoke a /method/ on an /interface/ to send it:
           #+begin_src scala
             // def send(to: EmailAddress, email: Email): IO[Unit] = ???

             trait EmailDelivery {
               def send(to: EmailAddress, email: Email): IO[Unit]
             }
           #+end_src
      * Create _FAKE_ ones for testing:
        #+begin_src scala
          class FailingEmailDelivery extends EmailDelivery {
            def send(to: EmailAddress, email: Email): IO[Unit] =
              IO.raiseError(new RuntimeException(s"couldn't send email to $to"))
          }
        #+end_src

      * For example,
        we may be testing the behavior of a *user registration service*
        which uses our ~EmailDelivery~ for additional /effects/:
        #+begin_src scala
          class UserRegistration(emailDelivery: EmailDelivery) {
            def register(email: EmailAddress): IO[Unit] =
              for {
                _ <- save(email)
                _ <- emailDelivery.send(to, new Email(???))
              } yield ()

            private def save(email: EmailAddress): IO[Unit] = ???
          }
        #+end_src
        + Pass the /interface/ as a /dependency/
          so we can choose a *real* or *fake* implementation.

        + A very basic test might assert that
          "registration should fail if the registration email could not be sent":
          #+begin_src scala
            def registrationFailsIfEmailDeliveryFails(email: EmailAddress) =
              new UserRegistration(new FailingEmailDelivery)
                .send(email)
                .attempt
                .map(result => assert(result.isLeft, s"expecting failure, but was $result"))
                .unsafeRunSync()
          #+end_src

** DONE 8.2. Testing effect scheduling by controlling its dependencies - 114
   CLOSED: [2021-08-23 Mon 02:02]
   *Faking* /effects/ with /interfaces/ works well to
   increase the /modularity/ and /testability/ of our own code,

   _BUT_
   - Q :: what about testing _aspects of Cats Effect_ itself?
   - A :: _Cats Effect_ itself uses the same technique:
          the ~TestContext~ /helper class/ lets us use *faked* ~ExecutionContext~
          and ~Timer~ instances in the _effectful code_ we want to test, and then
          _EXPLICITLY control_ the /effect scheduling/ in our tests.
     * We can then make assertions about the *relative execution order* of
       /effects/.

   - Here we
     *instantiate* a ~TestContext~ and
     *bring* its members *into* /scope/ so our /effects/ can reference them:
     #+begin_src scala
       import cats.effect.laws.util.TestContext

       val ctx = TestContext()  // 1

       implicit val cs: ContextShift[IO] = ctx.ioContextShift  // 2
       implicit val timer: Timer[IO] = ctx.timer  // 2
     #+end_src
     * In our tests we then can *advance* the /effect scheduling clock/ *MANUALLY*:
       #+begin_src scala
         val timeoutError = new TimeoutException
         val timeout = IO.sleep(10.seconds) *> IO.raiseError[Int](timeoutError)  // 1
         val f = timeout.unsafeToFuture()  // 2

         // Not yet
         ctx.tick(5.seconds)          // 3
         assertEquals(f.value, None)  // 3

         // Good to go:
         ctx.tick(6.seconds)                                 // 4
         assertEquals(f.value, Some(Failure(timeoutError)))  // 4
       #+end_src

** DONE 8.3. Summary - 115
   CLOSED: [2021-08-23 Mon 02:07]
   1. Asserting conditions on /effects/
      *REQUIRES* them to _be executed_.

   2. To test computations that use /effects/,
      we can *“fake”* those /effects/
      BY abstracting an /interface/ over their creation.

   3. To make assertions about /effect execution order/
      we can use ~TestContext~ from ~cats.effect.laws.util~ (_Cats Effect Laws_
      ~util~ package).
      * We then *schedule* the /effects/ for _execution_ --
        BY
        1) *transforming* them *to* ~Future~ -- and
        2) subsequently *advance* our “clock” to assert *“WHEN” /effects/ happen*.

* DONE 9. Concurrent coordination - 116
  CLOSED: [2021-08-24 Tue 03:43]
  - What we learned so far:
    1. Introduce /effects/, and in this stage /effects/ only allow us to
       - talk about the _values_ they produce, and
       - create NEW /effects/ by composing the outputs of other /effects/.

    2. Add the notion of a ~Fiber~ to _represent_ an /ALREADY-EXECUTING
       effect/, and with it we can start to
       _CONTROL /concurrent effects/ by *joining* or *canceling* them._

  - Until now
    we have *NOT* yet discussed *coordination* between /concurrent effects/.
    _This chapter will discuss it_!

  - coordination between /concurrent effects/ ::
    the behavior of one /effect/ should *depend on* another.
    * For example,
      + HOW can we *share* /state/ *between* /effects/
        WHEN that /state/ might be /concurrently updated/?

      + HOW can we *ensure* one /effect/ ONLY proceeds
        ONCE work is COMPLETE in another?

  - In this chapter, we'll
    1. Discuss the first issue by using the ~Ref~ /data type/ for *sharing*
       /mutable state/ (=from Jian= in a functional way).

    2. Show how the ~Deferred~ /data type/ can provide /concurrent effect
       serialization/ *WITHOUT blocking* ANY actual /threads/
       (=from Jian= semantic blocking).

    3. Model even more complex behavior
       by
       *composing* two /concurrency primitives/ mentioned in 1 and 2 together
       *to form* a /concurrent state machine/.

** DONE 9.1. Atomic updates with ~Ref~ - 116
   CLOSED: [2021-08-23 Mon 17:48]
*** DONE 9.1.1. Using ~Ref~: getting and setting state - 116
    CLOSED: [2021-08-23 Mon 17:48]
    - ~Ref~ usage:
      * *Get* the CURRENT /state/:
        #+begin_src scala
          def get: IO[A]
        #+end_src

      * *Set* the /state/:
        #+begin_src scala
          def set(value: A): IO[Unit]

          def getAndSet(value: A): IO[A]  // 1
        #+end_src
        1. ~getAndSet~
          *sets* the value to ~A~ and
          *returns* the _previous_ value of the ~Ref~.

      * *Update* the /state/:
        #+begin_src scala
          def update(f: A => A): IO[Unit]

          def getAndUpdate(f: A => A): IO[A]  // 1
          def updateAndGet(f: A => A): IO[A]  // 2
        #+end_src
        1. *Updates* the value with the function ~f~ and
           *returns* the _PREVIOUS_ value.

        2. *Updates* the value with the function ~f~ and
           *returns* the _NEW_ value.

      * You can also
        *update* the /state/
        BUT *return* a value of a _DIFFERENT_ /type/ using ~modify~:
        #+begin_src scala
          def modify[B](f: A => (A, B)): IO[B]
        #+end_src

    - It's important to note that _the ~update~-style methods_ are given
      _a *pure* (side effect-free) function_,
      BECAUSE it's possible that the function you provide may be run _more than
      once_. WAT?
      * Q :: We previously said that updates were atomic, so wouldn’t the update
             function be run only once?
      * A :: It could be that the implementation of the /atomic update/ only runs
             the function once: we could _pessimistically_ assume
             1. there are multiple updates happening, and
             2. use some /lower-level mutual exclusion mechanism/ to ensure only
                one operation is allowed to proceed.

             Instead, *the actual implementation* of ~Ref~ uses an _optimistic_
             strategy where it assumes
             1. only one update is happening concurrently.
             2. However if another concurrent update succeeds,
                our update operation will be *retried*.

      * Therefore
        we do *NOT* want to perform ANY /side effects/ with our function.

    - _To show_
      the function passed to ~modify~ runs more than once
      WHEN multiple writers are racing to “win” the /atomic update/,
      let's performing a /side effect/ within a ~modify~:
      * Example 35.
        Update functions passed to a ~Ref~ may get invoked multiple times.
        Code available at =coordination/RefUpdateImpure.scala=.
        #+begin_src scala
          package com.innerproduct.ee.coordination

          import cats.effect._
          import cats.effect.concurrent.Ref
          import cats.implicits._

          object RefUpdateImpure extends IOApp {
            def run(args: List[String]): IO[ExitCode] =
              for {
                ref <- Ref[IO].of(0)
                _   <- List(1, 2, 3).parTraverse(task(_, ref))  // 1
              } yield ExitCode.Success

            def task(id: Int, ref: Ref[IO, Int]): IO[Unit] =
              ref
                .modify(previous => id -> println(s"previous->$id"))  // 2
                .replicateA(3)  // 3
                .void
          }

          // 0->1
          // 0->3
          // 1->3
          // 0->2
          // 3->2
          // 2->3
          // 2->2
          // 3->1
          // 3->3
          // 1->1
          // 3->2
          // 1->3
          // 1->2
          // 3->2
          // 2->2
        #+end_src
        + We *EXPECT NINE* /atomic updates/ (three elements with three ~modify~
          /effects/ each),
          BUT see *more than nine calls* to the function we supply to ~modify~!
          This can be understood if we check the source code:
          #+begin_src scala
            // The exact return type is actually `F[B]`
            def modify(f: A => (A, B)): IO[B] = {
              @tailrec
              def spin: B = {
                val current = ar.get  // 1
                val (updated, b) = f(current)  // 2
                if (!ar.compareAndSet(current, updated)) spin  // 3
                else                                     b
              }
              IO.deplay(spin)
            }
          #+end_src

        + To ensure we execute an /effect/ only once for a given update,
          - =from Jian= - start
            elimiate /side effects/ from the function passed to ~modify~
            =from Jian= - end

          - =from Jian=
            We can wrap the ~println~ /side effect/ inside an ~IO~.

          - =Book=
            We can replace the ~println~ /side effect/ with an ~IO~ value (and
            call ~debug~ from it to check the updates).
            #+begin_src scala
              // ref.modify(previous => id -> println(s"$previous->$id"))

              ref
                .modify(previous => id -> IO(s"$previous->$id").debug)
                .flatten

              // [ioapp-compute-1] 2->1
              // [ioapp-compute-2] 0->2
              // [ioapp-compute-3] 1->3
              // [ioapp-compute-1] 2->1
              // [ioapp-compute-3] 3->3
              // [ioapp-compute-1] 1->1
              // [ioapp-compute-2] 3->2
              // [ioapp-compute-3] 1->3
              // [ioapp-compute-2] 3->2
            #+end_src
            =from Jian=
            Check the right hand side of ->, we can see the expected number of
            values -- three 1, three 2, and three 3.

** DONE 9.2. Write-once synchronization with ~Deferred~ - 124
   CLOSED: [2021-08-24 Tue 01:37]
   - Q :: Here's a different issue:
          HOW can we know *when* our _concurrently updated counter_ is *in a particular
          state?*

   - Let's pretend it's extremely important to print "BEEP!" when the counter
     from the previous example reaches 13, so we poll the counter every second:
     #+begin_src scala
       def beepWhen13(ticks: Ref[IO, Long]): IO[Unit] =  // 1
         for {
           t <- ticks.get  // 2
           _ <- if (t >= 13) IO("BEEP!").debug  // 3
                else         IO.sleep(1.second) *> beepWhen13(ticks)  // 4
         } yield ()
     #+end_src
     * It's a good exercise to think through
       + Q :: How long we should wait to check the condition we are interested in --
               it certainly depends on the true _update frequency_ of the /state/.
         - On one hand if we _poll too often_ we're being *inefficient*, and
         - on the other hand if we _poll too infrequently_ then our reaction to the
           /state/ change is *delayed*.

       + A :: _INSTEAD OF_ having to guess a polling interval,
               we can push this responsibility behind an abstraction that will,
               from the outside,
               *block* subsequent execution *until* the condition is fulfilled.
         - In _Cats Effect_ this abstraction is the ~Deferred~ data type.
           * Let's replace the previous /synchronization/ that used polling with
             the blocking ~Deferred~:
             + Example 36.
               Using ~Deferred~ for /blocking synchronization/.
               Code available at =coordination/IsThirteen.scala=.
               #+begin_src scala
                 package com.innerproduct.ee.coordination

                 import cats.effect._
                 import cats.effect.concurrent._
                 import cats.implicits._
                 import com.innerproduct.ee.debug._
                 import scala.concurrent.duration._

                 object IsThirteen extends IOApp {
                   def run(args: List[String]): IO[ExitCode] =
                     for {
                       ticks <- Ref[IO].of(0L)
                       is13  <- Deferred[IO, Unit]  // 1
                       _     <- (beepWhen13(is13), tickingClock(ticks, is13)).parTupled  // 2
                     } yield ExitCode.Success

                   private def beepWhen13(is13: Deferred[IO, Unit]): IO[Unit] =
                     for {
                       _ <- is13.get  // 3
                       _ <- IO("BEEP!").debug
                     } yield ()

                   private def tickingClock(ticks: Ref[IO, Long], is13: Deferred[IO, Unit]): IO[Unit] =
                     for {
                       _     <- IO.sleep(1.second)
                       _     <- IO(System.currentTimeMillis()).debug
                       count <- ticks.updateAndGet(_ + 1)
                       _     <- if (count >= 13) is13.complete(()) else tickingClock(ticks, is13)
                     } yield ()
                 }
               #+end_src
               =FIXME= the last line!!!

   - Such a useful data type ~Deferred~ with ONLY *two* /methods/: ~get~ and ~complete~!

   - Q :: One final issue, however --
          WHEN _execution is /blocked/ from invoking ~Deferred.get~,_
          does that mean that the underlying /thread/ is *blocked*?

   - A :: Fortunately for us, the answer is *“NO”*.
          _Instead of_ *blocking* execution _at the /thread/ level_,

          _Cats Effect_ uses so-called /semantic blocking/:
          the /effect/ is _SUSPENDED_ *at a logical level*,
          but the underlying /thread/ *can be reused* for executing other
          /concurrent effects/.

   - *Synchronization*
     * In Computer Science
       the term /synchronization/ is used to talk about _enforcing a relationship
       between /effects/._
       + Here are some /synchronization constraint/ examples:
         - serialization :: /effect/ B should happen after /effect/ A.
         - mutual-exclusion :: /effect/ A should _never_ happen at the same time as
                              /effect/ B.

     * In _Cats Effect_,
       + /synchronization/ is expressed with the ~flatMap~ of ~IO~.

       + In contrast,
         using ~parMapN~ expresses *NO* a /priori relationship/ between /effects/
         other than transforming each /effect's/ “output”.
         - NO /synchronization/ is required.

     * The notions of /synchronization/ that are presented by /fibers/:
       1. When we ~start~ a /fiber/, we're expressing an /INDEPENDENT, concurrent effect/.
       2. When we ~join~, _ANY SUBSEQUENT_ /effect/ *must happen AFTER* the _fiber COMPLETES_.

     * Let's now consider more /complex effects/
       that *are NOT necessarily easily expressed with* ~flatMap~, ~parMapN~, or
       by using ~Fiber~'s.
       + Q :: What kinds of /synchronization/ do they require?
       + A :: They are:
         - *updating shared state*:
           Updates must be /atomic/,
           i.e., /mutually exclusive/ with respect to other updates, otherwise
           updates may be “lost”.

         - *reading shared state*:
           Concurrent reads do *NOT require any /synchronization/.*
           We read whatever the “current” value is, independent of anything
           else.

         - *blocking*:
           _SUBSEQUENT_ /effects/ *must happen after* the /“blocking” effect/
           “unblocks” (serialization).

** DONE 9.3. Concurrent state machines - 128
   CLOSED: [2021-08-24 Tue 03:28]
   - ~Ref~ and ~Deferred~ are the _building blocks_ of /concurrency/.
     * With ~Ref~ we can ensure /atomic updates/ of /shared state/, and

     * ~Deferred~ gives us the ability to *serialize* _the execution of an /effect/_
       with respect to some _newly-produced_ /state/.

   - Together we can *build* LARGER and MORE COMPLEX _concurrent behaviors_.
     * One technique to do this is to create a /concurrent state machine/:
       + To *build* one we:
         1. *Define* an /interface/
            whose /methods/ return /effects/.

         2. *Implement* the /interface/ *by building* a /state machine/ where:
            a. /state/ (with type ~S~) is ATOMICALLY managed via a ~Ref[IO, S]~
               value;

            b. EACH /interface method/ is implemented by a /state transition
               function/ affecting the ~Ref~; and

            c. ANY /state-dependent blocking behavior/ is controlled
               via ~Deferred~ values.

   - As an illustration example,
     we'll follow this recipe to *build* a structure called a _countdown latch_.

*** TODO 9.3.1. Example: countdown latch - 128
    - The behavior we'd like to model is to
      *block* SUBSEQUENT /effects/
      *until* a certain number of (possibly concurrent) /effects/ have occurred.

    - The metaphor of a *latch* is used
      BECAUSE a _latch_ is used to keep a door closed until the _latch_ is opened.

    - The term *countdown* refers to the algorithm for how the _latch_ is opened:
      a counter is decremented, and when the counter reaches zero, the _latch_
      opens.

    - There are _TWO_ *logical roles* that
      _concurrently coordinate_ through the /SHARED latch/
      1. /readers/ *wait for* the _latch_ to OPEN; and
      2. /writers/ *decrement* the _latch counter_.

    - Let's fulfill step one of our recipe (“define an /interface/ whose /methods/
      return /effects/”)
      by _encapsulating_ the actions of the TWO roles _as_ methods
      on a SHARED ~CountdownLatch~ /interface/:
      #+begin_src scala
        trait CountdownLatch {
          def await: IO[Unit]  // 1
          def decrement: IO[Unit]  // 2
        }
      #+end_src
      1. /Readers/ will *await* the opening of the _latch_.
         The /caller/ will be *blocked*
         and
         no value will be produced until the _latch_ *opens*.

      2. /Writers/ will *decrement* the _latch counter_,
         which may *open* the _latch_.

    - A "reader" will be waiting for the _latch_ to *open*, perhaps denoting a
      set of prerequisite actions have occurred:
      #+begin_src scala
        def actionWithPrerequisites(latch: CountdownLatch) =
          for {
            _      <- IO("waiting for prerequisities").debug
            _      <- latch.await  // 1
            result <- IO("action").debug  // 2
          } yield result
      #+end_src
      1. We *block until* the ~latch~ *opens*.
      2. Once the ~latch~ *opens*, we can run the action.

    - At the same time, a “writer” is fulfilling one or more of those prerequisites:
      #+begin_src scala
        def runPrerequisite(latch: CountdownLatch) =
          for {
            result <- IO("prerequisite").debug
            _      <- latch.decrement  // 1
          } yield result
      #+end_src
      1. *Once* the prerequisite action is *completed*,
         we *decrement* the _latch counter_.

    - Other code would _run_ each of these roles _CONCURRENTLY_:
      #+begin_src scala
        val prepareAndRun =
          for {
            latch <- CountdownLatch(1)
            _     <- (actionWithPrerequisites(latch), runPrerequisite(latch)).parTupled
          } yield ()
      #+end_src
      =from Jian=
      See the below Example 37 to see the ~apply~ method of ~CountdownLatch~, and
      the subtypes of ~CountdownLatch~.

    - =TODO=
      DISCUSSION

    - Example 37. =coordination/CountdownLatch.scala=
      #+begin_src scala
        object CountdownLatch {
          def apply(n: Long)(implicit cs: ContextShift[IO]): IO[CountdownLatch] =
            for {
              whenDone <- Deferred[IO, Unit]
              state    <- Ref[IO].of[State](Outstanding(n, whenDone))
            } yield new CountdownLatch {
              def await: IO[Unit] =
                state.get.flatMap {
                  case Outstanding(_, whenDone) => whenDone.get
                  case Done                     => IO.unit
                }

              def decrement: IO[Unit] =
                state.modify {
                  case Outstanding(1, whenDone) => Done -> whenDone.complete(())
                  case Outstanding(n, whenDone) => Outstanding(n - 1, whenDone) -> IO.unit
                  case Done                     => Done -> IO.unit
                }.flatten
            }

          sealed trait State
          final case class Outstanding(n: Long, whenDone: Deferred[IO, Unit]) extends State
          case object Done extends State
        }
      #+end_src

    - *Exercise 8: Fixing a bug in* ~ConcurrentLatch~
      =TODO= =TODO= =TODO= =!!!=

*** DONE 9.3.2. Using a latch for synchronization - 133 - =TODO= - ~IsThirteenLatch~ behavior is inconsistent with ~IsThirteen~
    CLOSED: [2021-08-24 Tue 03:26]
    Our use of ~Deferred~ for the /synchronization/ of
    _the “has there been 13 ticks” /state/_ has a small *drawback*:
    it
    *pushes* the logic of the _condition_ (the ~if (count >= 13)~ expression)
    *into* the ~tickingClock~ /effect/.

    - Q :: Can we make the _clock_ *unaware of* this _condition_,
           *BUT STILL* provide a /blocking method/ of /synchronization/ like
           ~Deferred~ gave us?

    - A :: We can do exactly that with our ~CountdownLatch~.
           There are THREE interacting concerns:
      1. A /SHARED latch/ is initialized with the desired number of ticks,
         in this case ~13~.

      2. The ~beeper~ is given the _latch_ and invokes ~await~ to be *blocked
         until* the _latch_ *opens*.

      3. The _ticking clock_ is also given the _latch_ and will ~decrement~ it
         on every tick.

    - Example 38.
      Using ~CountdownLatch~ for /blocking synchronization/.
      Code available at =coordination/IsThirteenLatch.scala=.
      #+begin_src scala
        package com.innerproduct.ee.coordination

        import cats.effect._
        import cats.implicits._
        import com.innerproduct.ee.debug._
        import scala.concurrent.duration._

        object IsThirteenLatch extends IOApp {
          def run(args: List[String]): IO[ExitCode] =
            for {
              latch <- CountdownLatch(13)
              _     <- (beeper(latch), tickingClock(latch)).parTupled
            } yield ExitCode.Success

          private def beeper(latch: CountdownLatch): IO[Unit] =
            for {
              _ <- latch.await
              _ <- IO("BEEP!").debug
            } yield ()

          private def tickingClock(latch: CountdownLatch): IO[Unit] =
            for {
              _ <- IO.sleep(1.second)
              _ <- IO(System.currentTimeMillis).debug
              _ <- latch.decrement
              _ <- tickingClock(latch)
            } yield ()
        }
      #+end_src

** DONE 9.4. Summary - 134
   CLOSED: [2021-08-24 Tue 03:43]
   1. We may want to
      *coordinate* /concurrently executing effects/
      SO the behavior of one /effect/ should depend on another.
      - The _constraints_ of that *coordination* can be described by
        /synchronization/:
        ensuring a relationship between /concurrent events (effects)/.

   2. _UNLESS_ we can *synchronize* the _updates_ of /shared mutable state/ to be
      /atomic (mutually-exclusive)/,
      _modifications_ of that state can be *lost* during concurrent actions.

      - In _Cats Effect_, ~Ref~ provides /atomic updates/ of a /shared value/.

   3. We may also want to ENSURE that
      an /effect/ is evaluated
      *only after*
      _some deferred value has been produced (serialization)._

      - In _Cats Effect_,
        * ~Deferred~ provides
          + write-once,
          + blocking
          /synchronization/ of a /shared value/:
          *before* the value is available, all _readers_ are /blocked/.
          Only *when a value has been written* are _readers_ /unblocked/.

   4. /Blocking/ means the current execution *stops (=from Jian= "pause" is better???)*
      _UNTIL_ some /(unblocking) condition/ is triggered.
      - Semantic blocking :: (a term used by _Cats Effect_) where the “logical”
                             execution of the current /effect/ is *blocked* from
                             further execution,
                             =IMPORTANT=
                             BUT *NOT via the blocking mechanism* of the
                             _underlying concurrency mechanism_, i.e., /thread blocking/.

   5. We can *model* _more complex concurrent behavior_
      *by constructing* /concurrent state machines/.
      - They *combine* /atomic updates/ with /blocking synchronization/.

* TODO 10. Case study: job scheduler - 136
  To put programming with /effects/ into practice, let's design and implement a
  /job scheduler/ -- a process that executes user-submitted jobs with its
  available resources.

  - We'll first talk about jobs, how they are represented and how they changed
    state. Then we’ll manage those jobs so that more than one can run concurrently.

  - As we build up the scheduler's design, we’ll also need to answer questions
    about:

** 10.1. Jobs - 136
** 10.2. Job scheduler - 140
** 10.3. Reacting to job state changes - 142
*** (WIP) 10.3.1. Implementing the reactor - 144
*** 10.3.2. A binary sleeping state machine - 147
    - *Exercise 9: Implement ~Zzz~ as a concurrent state machine*

*** 10.3.3. Making the reactor sleep and awaken - 148

** 10.4. Putting everything together - 149
** 10.5. Summary - 151

* TODO 11. Conclusion - 152
** 11.1. Next steps - 154

* TODO Glossary - 156
* TODO Appendix A: Cheatsheets - 160
** (WIP) A.1. Cats typeclasses and extension methods - 160
** (WIP) A.2. Cats Effect data types - 162

* TODO Appendix B: Abstracting effects with typeclasses - 165
* TODO Appendix C: Changes in Cats Effect 3 - 168
** C.1. Method changes - 168
** C.2. Data type changes - 169
** C.3. Package changes - 169

* TODO Appendix D: Solutions to selected exercises - 170
** D.1. Effects: evaluation and execution - 170
** D.2. Cats Effect ~IO~ - 171
** D.3. Parallel execution - 172
** D.4. Concurrent Control - 172
** D.5. Shifting Contexts - 172
** D.6. Integrating asynchrony - 173
** D.7. Managing Resources - 174
** D.8. Testing Effects - 174
** D.9. Concurrent Coordination - 174

* TODO References - 176
