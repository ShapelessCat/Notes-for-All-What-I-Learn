#+TITLE: Learning Apache Spark 2
#+COMMENT: allaboutscala.com
#+VERSION: 2018
#+AUTHOR: ???
#+STARTUP: overview
#+STARTUP: entitiespretty

* Chapter 12 Learn Apache Spark 2
** Part 1 ~DataFrame~ SQL Query (with Project Setup)
*** Project Setup:
**** 1. Using StackOverflow dataset
**** 2. Add Apache Spark 2 SBT dependencies
**** 3. Bootstrap a SparkSession

*** DataFrame SQL Query:
**** 1. DataFrame Introduction
**** 2. Create DataFrame from a CSV file
**** 3. DataFrame schema
**** 4. Select columns
**** 5. Filter by column value
**** 6. Count rows
**** 7. SQL like
**** 8. Filter chaining
**** 9. SQL In
**** 10. SQL Group By
**** 11. SQL Group By with filter
**** 12. SQL order by
**** 13. Cast column data type
**** 14. Filtered dataframe
**** 15. Dataframe Join
**** 16. Join and select columns
**** 17. Join on explicit columns
**** 18. Inner Join
**** 19. Left Outer Join
**** 20. Right Outer Join
**** 21. Distinct

** Part 2 Spark SQL
*** Spark SQL Introduction
*** Register temporary table
*** List all tables in Spark's catalog
*** List catalog tables using Spark SQL
*** Select columns
*** Filter by column value
*** Count number of rows
*** SQL like
*** SQL where with and clause
*** SQL In clause
*** SQL Group by
*** SQL Group by with having clause
*** SQL Order by
*** Typed columns, filter and create temp table
*** SQL Inner join
*** SQL Left outer join
*** SQL Right outer join
*** SQL Distinct
*** Register User Defined Function (UDF)

** Part 3 ~DataFrame~ Statistics
*** 1. DataFrame Statistics Introduction
*** 2. Create DataFrame from CSV
*** 3. Average
*** 4. Maximum
*** 5. Minimum
*** 6. Mean
*** 7. Sum
*** 8. Group by query with statistics
*** 9. DataFrame Statistics using describe()
*** 10. Correlation
*** 11. Covariance
*** 12. Frequent Items
*** 13. Crosstab
*** 14. Stratified sampling using sampleBy()
*** 15. Approximate Quantile
*** 16. Bloom Filter
*** 17. Count Min Sketch
*** 18. Sampling With Replacement

** Part 4 ~DataFrame~ Operations
*** ~DataFrame~ Operations Introduction
*** Setup ~DataFrame~'s
*** Convert ~DataFrame~ row to Scala Case class
*** Convert ~DataFrame~ row to Scala Case class using ~map()~ method
*** Create ~DataFrame~ from Collection
*** ~DataFrame~ Union
*** ~DataFrame~ Intersection
*** Append column to ~DataFrame~ using ~withColumn()~ method

** Part 5 Spark Functions
*** Create ~DataFrame~ from ~Tuple~'s
*** Get ~DataFrame~ column names
*** ~DataFrame~ column names and types
*** Json into ~DataFrame~ using ~explode()~
*** Concatenate ~DataFrame~ using ~join()~
*** Search ~DataFrame~ column using ~array_contains()~
*** Check ~DataFrame~ column exists
*** Split ~DataFrame~ Array column
*** Rename ~DataFrame~ column
*** Create ~DataFrame~ constant column
*** ~DataFrame~ new column with User Defined Function (UDF)
*** ~DataFrame~ first row
*** Format ~DataFrame~ column
*** ~DataFrame~ column hashing
*** ~DataFrame~ String functions
*** ~DataFrame~ drop ~null~
