#+TITLE: Data Algorithms with Spark
#+VERSION: 3.0.0 -> 3.0.1
#+AUTHOR: Mahmoud Parsian
#+RELEASE DATE: 2021-12
#+STARTUP: overview
#+STARTUP: entitiespretty

* 1. Transformations in Action
** DNA Base Count Example
** DNA Base Count Problem
*** FASTA Format
*** FASTA Example
*** FASTA Data Download
    
** DNA Base Count Solution 1
*** Step 1: Create an RDD of String from Input
*** Step 2: Define a Mapper Function
*** Step 3: Find Frequencies of DNA Letters
*** Pros and Cons of Solution Version 1
    
** DNA Base Count Solution 2
*** Step-1: Create an RDD[String] from Input
*** Step 2: Define a Mapper Function
*** Step 3: Find Frequencies of DNA Letters
*** Pros and Cons of Solution 2
    
** DNA Base Count Solution 3
*** Understanding mapPartitions() Transformation
*** Step 1: Create an RDD of String from Input
*** Step 2: Define a Function to Handle a Partition
*** Step-3: Apply Custom Function to Each Partition
*** Pros and Cons of Solution 3
    
** Handling Empty Partitions
** DNA Base Count by PySpark — FASTQ Version
*** FASTQ Format
*** FASTQ Format Example
*** FASTQ Data Download
*** High-Level Solution for FASTQ
    
** Summary
   
* 2. Mapper Transformations
** Data Abstractions and Mappers
** What are Transformations?
** Creating New RDDs
** Lazy Transformations
** Semantics of Mapper Transformations
** The map() Transformation
*** Custom Map Functions
    
** The flatMap() Transformation
** map() vs. flatMap()
** The mapValues() Transformation
** The flatMapValues() Transformation
** The mapPartitions() Transformation
** Handling Empty Partitions
** Benefits of mapPartitions() Transformation
** Summary
   
* 3. Reductions in Spark
** Creating Pair RDDs
*** Example: Using Collections
*** Example: Using map() Transformation
    
** Reducer transformations
** Spark’s Reductions
** What is a Reduction?
** Spark’s Reduction Transformations
** Simple Warmup Example
** Solution by reduceByKey()
*** Using Lambda Expressions
*** Using Functions
    
** Solution by groupByKey()
** Solution by aggregateByKey()
** Solution by combineByKey()
** What is a Monoid?
** Monoid Examples
** Non-Monoid Examples
** Movie Problem
** Input Data Set to Analyze
** Ratings Data File Structure (ratings.csv)
** Solution Using aggregateByKey()
** Results
** How does aggregateByKey() work?
** PySpark Solution using aggregateByKey()
*** Step 1: Read Data and Create Pairs
*** Step 2: Use aggregateByKey() to Sum Up Ratings
*** Step 3: Find Average Rating
    
** Complete PySpark Solution by groupByKey()
** PySpark Solution using groupByKey()
*** Step 1: Read Data and Create Pairs
*** Step 2: Use groupByKey() to Group Ratings
*** Step 3: Find Average Rating
    
** Shuffle Step in Reductions
** Shuffle Step for groupByKey()
** Shuffle Step for reduceByKey()
** Complete PySpark Solution using reduceByKey()
*** Step 1: Read Data and Create Pairs
*** Step 2: Use reduceByKey() to Sum up Ratings
*** Step 3: Find Average Rating
    
** Complete PySpark Solution using combineByKey()
** PySpark Solution using combineByKey()
*** Step 1: Read Data and Create Pairs
*** Step 2: Use combineByKey() to Sum up Ratings
*** Step 3: Find Average Rating
    
** Comparison of Reductions
** Summary
   
* 4. Data Design Patterns
** InMapper Combining
** Basic MapReduce Design Pattern
** InMapper Combining Per Record
** InMapper Combiner Per Partition
** Top-10
** Top-N Formalized
** PySpark Solution
** Implementation in PySpark
** How to Find Bottom-10
** MinMax
*** Solution-1: Classic MapReduce
*** Solution-2: Sorting
*** Solution-3: Spark’s mapPartitions()
*** Solution-3 Input
*** PySpark Solution
    
** The Composite Pattern and Monoids
*** Composite Pattern
    
** Monoids
*** Definition of Monoid
*** How to form a Monoid?
*** Monoidic and Non-Monoidic Examples
*** Non-Commutative Example
*** Median over Set of Integers
*** Concatenation over Lists
*** Union and Intersection over Integers
*** Matrix Example
    
** Not a Monoid Example
** Monoid Example
** PySpark Implementation of Monodized Mean
*** Input
*** PySpark Solution
    
** Conclusion on Using Monoids
*** Functors and Monoids
    
** Map-Side Join
** Efficient Joins using Bloom filters
** Bloom filter
*** A Simple Bloom Filter Example
*** Bloom Filter in Python
*** Using Bloom Filter in PySpark
    
** Binning: Data Organization Pattern
** Summary
