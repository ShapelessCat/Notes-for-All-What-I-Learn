#+TITLE: Computer Architecture
#+SUBTITLE: A Quantitative Approach
#+VERSION: 6th
#+AUTHOR: John L. Hennessy, David A. Patterson
#+STARTUP: overview
#+STARTUP: entitiespretty

* Foreword - ix
* Preface - xvii
** Why We Wrote This Book - xvii
** This Edition - xviii
** Topic Selection and Organization - xviii
** An Overview of the Content - xix
** Navigating the Text - xxi
** Chapter Structure - xxi
** Case Studies With Exercises - xxii
** Supplemental Materials - xxii
** Helping Improve This Book - xxiii
** Concluding Remarks - xxiii

* Acknowledgments - xxv
* TODO Chapter 1. Fundamentals of Quantitative Design and Analysis - 1
** TODO 1.1 Introduction - 2
   - The rapid improvement of the performance of computers has come both
     + from advances in the technology used to build computers
     + from innovations in computer design.

   - =TODO=

   - RISC :: Reduced Instruction Set Computer

   - soar - 飙升

   - RISC x86 such as cell phones ARM

   - 

** TODO 1.2 Classes of Computers - 6
*** Internet of Things/Embedded Computers - 6
*** Peronal Mobile Device - 7
*** Desktop Computing - 8
*** Servers - 8
*** Clusters/Warehouse-Scale Computers - 9
*** Classes of Parallelism and Parallel Architectures - 10
    - =TODO=

    - When Flynn (1966) studied the /parallel computing/ efforts in the 1960s, he
      found a simple *classification* whose abbreviations we still use today.

      They target /data-level parallelism/ and /task-level parallelism/. He looked
      at the parallelism in the /instruction/ and /data streams/ called for by the
      instructions at the most constrained component of the multiprocessor and
      _placed all computers in one of *four* categories_:
      1. SISD

      2. SIMD

      3. MISD

      4. MIMD

      This taxonomy is a _COARSE_ model, as many parallel processors are *hybrids*
      of the /SISD/, /SIMD/, and /MIMD/ classes.
        Nonetheless, it is useful to put a framework on the design space for the
      computers we will see in this book. =TODO=

** TODO 1.3 Defining Computer Architecture - 11
*** Instruction Set Architecture: The Myopic View of Computer Architecture - 12
*** Genuine Computer Architecture: Designing the Organization and Hardware to Meet Goals and Functional Requirements - 17

** TODO 1.4 Trends in Technology - 18
*** Performance Trends: Bandwidth Over Latency - 20
*** Scaling of Transistor Performance and Wires - 21

** TODO 1.5 Trends in Power and Energy in Integrated Circuits - 23
*** Power and Energy: A Systems Perspective - 23
*** Energy and Power Within a Microprocessor - 25
*** The Shift in Computer Architecture Because of Limits of Energy - 28

** TODO 1.6 Trends in Cost - 29
*** The Impact of Time, Volume, and Commoditization - 30
*** Cost of an Integrated Circuit - 31
*** Cost Versus Price - 35
*** Cost of Manufacturing Versus Cost of Operation - 36

** TODO 1.7 Dependability - 36
** TODO 1.8 Measuring, Reporting, and Summarizing Performance - 39
*** Benchmarks - 40
*** Reporting Performance Results - 45
*** Summarizing Performance Results - 45

** TODO 1.9 Quantitative Principles of Computer Design - 48
*** Take Advantage of Parallelism - 48
*** Principle of Locality - 48
*** Focus on the Common Case - 49
*** Amdahl's Law - 49
*** The Processor Performance Equation - 52

** TODO 1.10 Putting It All Together: Performance, Price, and Power - 55
** TODO 1.11 Fallacies and Pitfalls - 58
** TODO 1.12 Concluding Remarks - 64
** TODO 1.13 Historical Perspectives and References - 67
** TODO Case Studies and Exercises by Diana Franklin - 67
*** Case Study 1: Chip Fabrication Cost - 67
*** Case Study 2: Power Consumption in Computer Systems - 69
*** Exercises - 72

* TODO Chapter 2. Memory Hierarchy Design - 77
** TODO 2.1 Introduction - 78
*** Basics of Memory Hierarchies: A Quick Review - 81

** TODO 2.2 Memory Technology and Optimizations - 84
*** SRAM Technology - 85
*** DRAM Technology - 85
*** Improving Memory Performance Inside a DRAM Chip: SDRAMs - 87
*** Graphics Data RAMs - 90
*** Packaging Innovation: Stacked or Embedded DRAMs - 91
*** Flash Memory - 92
*** Phase-Change Memory Technology - 93
*** Enhancing Dependability in Memory Systems - 93

** TODO 2.3 Ten Advanced Optimizations of Cache Performance - 94
*** First Optimization: Small and Simple First-Level Caches to Reduce Hit Time and Power - 95
*** Second Optimization: Way Prediction to Reduce Hit Time - 98
*** Third Optimization: Pipelined Access and Multibanked Caches to Increase Bandwidth - 99
*** Fourth Optimization: Nonblocking Caches to Increase Cache Bandwidth - 100
*** Fifth Optimization: Critical Word First and Early Restart to Reduce Miss Penalty - 104
*** Sixth Optimization: Merging Write Buffer to Reduce Miss Penalty - 105
*** Seventh Optimization: Compiler Optimizations to Reduce Miss Rate - 107
*** Eighth Optimization: Hardware Prefetching of Instructions and Data to Reduce Miss Penalty or Miss Rate - 109
*** Ninth Optimization: Compiler-Controlled Prefetching to Reduce Miss Penalty or Miss Rate - 111
*** Tenth Optimization: Using HBM to Extend the Memory Hierarchy - 114

** TODO 2.4 Virtual Memory and Virtual Machines - 118
*** Protection via Virtual Memory - 119
*** Protection via Virtual Machines - 120
*** Requirements of a Virtual Machine Monitor - 122
*** Instruction Set Architecture Support for Virtual Machines - 122
*** Impact of Virtual Machines on Virtual Memory and I/O - 123
*** Extending the Instruction Set for Efficient Virtualization and Better Security - 124
*** An Example VMM: The Xen Virtual Machine - 126

** TODO 2.5 Cross-Cutting Issues: The Design of Memory Hierarchies - 126
*** Protection, Virtualization, and Instruction Set Architecture - 126
*** Autonomous Instruction Fetch Units - 127
*** Speculation and Memory Access - 127
*** Special Instruction Caches - 128
*** Coherency of Cached Data - 128

** TODO 2.6 Putting It All Together: Memory Hierarchies in the ARM Cortex-A53 and Intel Core i7 6700 - 129
*** The ARM Cortex-A53 - 129
*** Performance of the Cortex-A53 Memory Hierarchy - 132
*** The Intel Core i7 6700 - 134
** TODO 2.7 Fallacies and Pitfalls - 142
** TODO 2.8 Concluding Remarks: Looking Ahead - 146
** TODO 2.9 Historical Perspectives and References - 148
** TODO Case Studies and Exercises by Norman P. Jouppi, Rajeev Balasubramonian, Naveen Muralimanohar, and Sheng Li - 148
*** Case Study 1: Optimizing Cache Performance via Advanced Techniques - 148
*** Case Study 2: Putting It All Together: Highly Parallel Memory Systems - 150
*** Case Study 3: Studying the Impact of Various Memory System Organizations - 153
*** Exercises - 155

* TODO Chapter 3. Instruction-Level Parallelism and Its Exploitation - 167
** 3.1 Instruction-Level Parallelism Concepts and Challenges - 168
*** What Is Instruction-Level Parallelism? - 169
*** Data Dependences and Hazards - 170
**** Data Dependences - 170
**** Name Dependences - 172
**** Data Hazards - 173

*** Control Dependences - 174

** 3.2 Basic Compiler Techniques for Exposing ILP - 176
*** Basic Pipeline Scheduling and Loop Unrolling - 177
*** Summary of the Loop Unrolling and Scheduling - 181

** 3.3 Reducing Branch Costs With Advanced Branch Prediction - 182
*** Correlating Branch Predictors - 182
*** Tournament Predictors: Adaptively Combining Local and Global Predictors - 184
*** Tagged Hybrid Predictors - 188
*** The Evolution of the Intel Core i7 Branch Predictor - 190

** 3.4 Overcoming Data Hazards With Dynamic Scheduling - 191
*** Dynamic Scheduling: The Idea - 193
*** Dynamic Scheduling Using Tomasulo's Approach - 195

** 3.5 Dynamic Scheduling: Examples and the Algorithm - 201
*** Tomasulo's Algorithm: The Details - 204
*** Tomasulo's Algorithm: A Loop-Based Example - 204

** 3.6 Hardware-Based Speculation - 208
** 3.7 Exploiting ILP Using Multiple Issue and Static Scheduling - 218
*** The Basic VLIW Approach - 218

** 3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, and Speculation - 222
** 3.9 Advanced Techniques for Instruction Delivery and Speculation - 228
*** Increasing Instruction Fetch Bandwidth - 228
**** Branch-Target Buffers - 228

*** Specialized Branch Predictors: Predicting Procedure Returns, Indirect Jumps, and Loop Branches - 232
**** Integrated Instruction Fetch Units - 233

*** Speculation: Implementation Issues and Extensions - 234
**** Speculation Support: Register Renaming Versus Reorder Buffers - 234
**** The Challenge of More Issues per Clock - 236
**** How Much to Speculate - 237
**** Speculating Through Multiple Branches - 238
**** Speculation and the Challenge of Energy Efficiency - 238
**** Address Aliasing Prediction - 239

** 3.10 Cross-Cutting Issues - 240
*** Hardware Versus Software Speculation - 240
*** Speculative Execution and the Memory System - 241

** 3.11 Multithreading: Exploiting Thread-Level Parallelism to Improve Uniprocessor Throughput - 242
*** Effectiveness of Simultaneous Multithreading on Superscalar Processors - 245

** 3.12 Putting It All Together: The Intel Core i7 6700 and ARM Cortex-A53 - 247
*** The ARM Cortex-A53 - 249
*** The Intel Core i7 - 252
**** Performance of the i7 - 255

** 3.13 Fallacies and Pitfalls - 258
** 3.14 Concluding Remarks: What's Ahead? - 264
** 3.15 Historical Perspective and References - 264
** Case Studies and Exercises by Jason D. Bakos and Robert P. Colwell - 266
*** Case Study: Exploring the Impact of Microarchitectural Techniques - 266
*** Exercises - 273

* TODO Chapter 4. Data-Level Parallelism in Vector, SIMD, and GPU Architectures - 281
** TODO 4.1 Introduction - 282
** TODO 4.2 Vector Architecture - 283
*** RV64V Extension - 283
*** How Vector Processors Work: An Example - 288
*** Vector Execution Time - 290
*** Multiple Lanes: Beyond One Element per Clock Cycle - 293
*** Vector-Length Registers: Handling Loops Not Equal to 32 - 294
*** Predicate Registers: Handling IF Statements in Vector Loops - 296
*** Memory Banks: Supplying Bandwidth for Vector Load/Store Units - 298
*** Stride: Handling Multidimensional Arrays in Vector Architectures - 299
*** Gather-Scatter: Handling Sparse Matrices in Vector Architectures - 301
*** Programming Vector Architectures - 302

** TODO 4.3 SIMD Instruction Set Extensions for Multimedia - 304
*** Programming Multimedia SIMD Architectures - 307
*** The Roofline Visual Performance Model - 307

** TODO 4.4 Graphics Processing Units - 310
*** Programming the GPU - 310
*** NVIDIA GPU Computational Structures - 313
*** NVIDA GPU Instruction Set Architecture - 320
*** Conditional Branching in GPUs - 323
*** NVIDIA GPU Memory Structures - 326
*** Innovations in the Pascal GPU Architecture - 328
*** Similarities and Differences Between Vector Architectures and GPUs - 331
*** Similarities and Differences Between Multimedia SIMD Computers and GPUs - 335
*** Summary - 336

** TODO 4.5 Detecting and Enhancing Loop-Level Parallelism - 336
*** Finding Dependences - 341
*** Eliminating Dependent Computations - 344

** TODO 4.6 Cross-Cutting Issues - 345
*** Energy and DLP: Slow and Wide Versus Fast and Narrow - 345
*** Banked Memory and Graphics Memory - 346

** TODO 4.7 Putting It All Together: Embedded Versus Server GPUs and Tesla Versus Core i7 - 346
*** Comparison of a GPU and a MIMD With Multimedia SIMD - 347
*** Comparison Update - 353

** TODO 4.8 Fallacies and Pitfalls - 353
** TODO 4.9 Concluding Remarks - 357
** TODO 4.10 Historical Perspective and References - 357
** TODO Case Study and Exercises by Jason D. Bakos - 357
*** Case Study: Implementing a Vector Kernel on a Vector Processor and GPU - 358
*** Exercises - 360

* Chapter 5. Thread-Level Parallelism
** 5.1 Introduction - 368
** 5.2 Centralized Shared-Memory Architectures - 377
** 5.3 Performance of Symmetric Shared-Memory Multiprocessors - 393
** 5.4 Distributed Shared-Memory and Directory-Based Coherence - 404
** 5.5 Synchronization: The Basics - 412
** 5.6 Models of Memory Consistency: An Introduction - 417
** 5.7 Cross-Cutting Issues - 422
** 5.8 Putting It All Together: Multicore Processors and Their Performance - 426
** 5.9 Fallacies and Pitfalls - 438
** 5.10 The Future of Multicore Scaling - 442
** 5.11 Concluding Remarks - 444
** 5.12 Historical Perspectives and References - 445
** Case Studies and Exercises by Amr Zaky and David A. Wood - 446

* Chapter 6. Warehouse-Scale Computers to Exploit Request-Level and Data-Level Parallelism
** 6.1 Introduction - 466
** 6.2 Programming Models and Workloads for Warehouse-Scale Computers - 471
** 6.3 Computer Architecture of Warehouse-Scale Computers - 477
** 6.4 The Efficiency and Cost of Warehouse-Scale Computers - 482
** 6.5 Cloud Computing: The Return of Utility Computing - 490
** 6.6 Cross-Cutting Issues - 501
** 6.7 Putting It All Together: A Google Warehouse-Scale Computer - 503
** 6.8 Fallacies and Pitfalls - 514
** 6.9 Concluding Remarks - 518
** 6.10 Historical Perspectives and References - 519
** Case Studies and Exercises by Parthasarathy Ranganathan - 519

* Chapter 7. Domain-Specific Architectures
** 7.1 Introduction - 540
** 7.2 Guidelines for DSAs - 543
** 7.3 Example Domain: Deep Neural Networks - 544
** 7.4 Google’s Tensor Processing Unit, an Inference Data Center Accelerator - 557
** 7.5 Microsoft Catapult, a Flexible Data Center Accelerator - 567
** 7.6 Intel Crest, a Data Center Accelerator for Training - 579
** 7.7 Pixel Visual Core, a Personal Mobile Device Image Processing Unit - 579
** 7.8 Cross-Cutting Issues - 592
** 7.9 Putting It All Together: CPUs Versus GPUs Versus DNN Accelerators - 595
** 7.10 Fallacies and Pitfalls - 602
** 7.11 Concluding Remarks - 604
** 7.12 Historical Perspectives and References - 606
** Case Studies and Exercises by Cliff Young - 606

* Appendix A. Instruction Set Principles
** A.1 Introduction - A-2
** A.2 Classifying Instruction Set Architectures - A-3
** A.3 Memory Addressing - A-7
** A.4 Type and Size of Operands - A-13
** A.5 Operations in the Instruction Set - A-15
** A.6 Instructions for Control Flow - A-16
** A.7 Encoding an Instruction Set - A-21
** A.8 Cross-Cutting Issues: The Role of Compilers - A-24
** A.9 Putting It All Together: The RISC-V Architecture - A-33
** A.10 Fallacies and Pitfalls - A-42
** A.11 Concluding Remarks - A-46
** A.12 Historical Perspective and References - A-47
** Exercises by Gregory D. Peterson - A-47

* Appendix B. Review of Memory Hierarchy
** B.1 Introduction - B-2
** B.2 Cache Performance - B-15
** B.3 Six Basic Cache Optimizations - B-22
** B.4 Virtual Memory - B-40
** B.5 Protection and Examples of Virtual Memory - B-49
** B.6 Fallacies and Pitfalls - B-57
** B.7 Concluding Remarks - B-59
** B.8 Historical Perspective and References - B-59
** Exercises by Amr Zaky - B-60
 
* Appendix C. Pipelining: Basic and Intermediate Concepts
** TODO C.1 Introduction - C-2
*** What Is Pipelining? - C-2
*** The Basics of the RISC V Instruction Set - C-3
*** A Simple Implementation of a RISC Instruction Set - C-4
*** The Classic Five-Stage Pipeline for a RISC Processor - c-6
*** Basic Performance Issues in Pipelining - c-8

** TODO C.2 The Major Hurdle of Pipelining—Pipeline Hazards - C-10
*** Performance of Pipelines With Stalls - C-11
*** Data Hazards - C-12
**** Minimizing Data Hazard Stalls by Forwarding - C-14
**** Data Hazards Requiring Stalls - C-16

*** Branch Hazards - C-18
*** Reducing the Cost of Branches Through Prediction - C-22
*** Static Branch Prediction - C-22
*** Dynamic Branch Prediction and Branch-Prediction Buffers - C-23

** C.3 How Is Pipelining Implemented? - C-26
** C.4 What Makes Pipelining Hard to Implement? - C-37
** C.5 Extending the RISC V Integer Pipeline to Handle Multicycle Operations - C-45
** C.6 Putting It All Together: The MIPS R4000 Pipeline - C-55
** C.7 Cross-Cutting Issues - C-65
** C.8 Fallacies and Pitfalls - C-70
** C.9 Concluding Remarks - C-71
** C.10 Historical Perspective and References - C-71
** Updated Exercises by Diana Franklin - C-71

* Online Appendices
* Appendix D Storage Systems
* Appendix E Embedded Systems by Thomas M. Conte
* Appendix F Interconnection Networks by Timothy M. Pinkston and José Duato
* Appendix G Vector Processors in More Depth by Krste Asanovic
* Appendix H Hardware and Software for VLIW and EPIC
* Appendix I Large-Scale Multiprocessors and Scientific Applications
* Appendix J Computer Arithmetic by David Goldberg
* Appendix K Survey of Instruction Set Architectures
* Appendix L Advanced Concepts on Address Translation by Abhishek Bhattacharjee
* Appendix M Historical Perspectives and References
* References - R-1
* Index - I-1
