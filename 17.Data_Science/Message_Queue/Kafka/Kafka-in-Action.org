#+TITLE: Kafka in Action
#+SUBTITLE: Real-Time Data and Stream Processing at Scale
#+VERSION: 2022
#+AUTHOR: Dylan Scott, Viktor Gamov, Dave Klein
#+FOREWORD BY: Jun Rao
#+STARTUP: entitiespretty
#+STARTUP: indent
#+STARTUP: overview

* foreword - xv
* preface - xvi
* acknowledgments - xviii
* about this book - xx
* about the authors - xxiii
* about the cover illustration - xxiv
* PART 1 GETTING STARTED - 1
** 1 Introduction to Kafka - 3
*** 1.1 What is Kafka? - 4
*** 1.2 Kafka usage - 8
**** Kafka for the developer - 8
**** Explaining Kafka to your manager - 9

*** 1.3 Kafka myths - 10
**** Kafka only works with Hadoop® - 10
**** Kafka is the same as other message brokers - 11

*** 1.4 Kafka in the real world - 11
**** Early examples - 12
**** Later examples - 13
**** When Kafka might not be the right fit - 14

*** 1.5 Online resources to get started - 15
*** References - 15

** 2 Getting to know Kafka - 17
*** 2.1 Producing and consuming a message - 18
*** 2.2 What are brokers? - 18
*** 2.3 Tour of Kafka - 23
**** Producers and consumers - 23
**** Topics overview - 26
**** ZooKeeper usage - 27
**** Kafka’s high-level architecture - 28
**** The commit log - 29

*** 2.4 Various source code packages and what they do - 30
**** Kafka Streams - 30
**** Kafka Connect - 31
**** AdminClient package - 32
**** ksqlDB - 32

*** 2.5 Confluent clients - 33
*** 2.6 Stream processing and terminology - 36
**** Stream processing - 37
**** What exactly-once means - 38

*** References - 39

* PART 2 APPLYING KAFKA - 41
** 3 Designing a Kafka project - 43
*** 3.1 Designing a Kafka project - 44
**** Taking over an existing data architecture - 44
**** A first change - 44
**** Built-in features - 44
**** Data for our invoices - 47

*** 3.2 Sensor event design - 49
**** Existing issues - 49
**** Why Kafka is the right fit - 51
**** Thought starters on our design - 52
**** User data requirements - 53
**** High-level plan for applying our questions - 54
**** Reviewing our blueprint - 57

*** 3.3 Format of your data - 57
**** Plan for data - 58
**** Dependency setup - 59

*** References - 64

** 4 Producers: Sourcing data - 66
*** 4.1 An example - 67
**** Producer notes - 70

*** 4.2 Producer options - 70
**** Configuring the broker list - 71
**** How to go fast (or go safer) - 72
**** Timestamps - 74

*** 4.3 Generating code for our requirements - 76
**** Client and broker versions - 84

*** References - 85

** 5 Consumers: Unlocking data - 87
*** 5.1 An example - 88
**** Consumer options - 89
**** Understanding our coordinates - 92

*** 5.2 How consumers interact - 96
*** 5.3 Tracking - 96
**** Group coordinator - 98
**** Partition assignment strategy - 100

*** 5.4 Marking our place - 101
*** 5.5 Reading from a compacted topic - 103
*** 5.6 Retrieving code for our factory requirements - 103
**** Reading options - 103
**** Requirements - 105

*** References 108

** 6 Brokers - 111
*** 6.1 Introducing the broker - 111
*** 6.2 Role of ZooKeeper - 112
*** 6.3 Options at the broker level - 113
**** Kafka’s other logs: Application logs - 115
**** Server log - 115
**** Managing state - 116

*** 6.4 Partition replica leaders and their role - 117
**** Losing data - 119

*** 6.5 Peeking into Kafka - 120
**** Cluster maintenance - 121
**** Adding a broker - 122
**** Upgrading your cluster - 122
**** Upgrading your clients - 122
**** Backups - 123

*** 6.6 A note on stateful systems - 123
*** 6.7 Exercise - 125
*** References - 126

** 7 Topics and partitions - 129
*** 7.1 Topics - 129
**** Topic-creation options - 132
**** Replication factors - 134

*** 7.2 Partitions - 134
**** Partition location - 135
**** Viewing our logs - 136

*** 7.3 Testing with EmbeddedKafkaCluster - 137
**** Using Kafka Testcontainers - 138

*** 7.4 Topic compaction - 139
*** References - 142

** 8 Kafka storage - 144
*** 8.1 How long to store data - 145
*** 8.2 Data movement - 146
**** Keeping the original event - 146
**** Moving away from a batch mindset - 146

*** 8.3 Tools - 147
**** Apache Flume - 147
**** Red Hat® DebeziumTM - 149
**** Secor - 149
**** Example use case for data storage - 150

*** 8.4 Bringing data back into Kafka - 151
**** Tiered storage - 152

*** 8.5 Architectures with Kafka - 152
**** Lambda architecture - 153
**** Kappa architecture - 154

*** 8.6 Multiple cluster setups - 155
**** Scaling by adding clusters - 155

*** 8.7 Cloud- and container-based storage options - 155
**** Kubernetes clusters -156

*** References 156

** 9 Management: Tools and logging - 158
*** 9.1 Administration clients - 159
**** Administration in code with AdminClient - 159
**** kcat - 161
**** Confluent REST Proxy API - 162

*** 9.2 Running Kafka as a systemd service - 163
*** 9.3 Logging - 164
**** Kafka application logs - 164
**** ZooKeeper logs - 166

*** 9.4 Firewalls - 166
**** Advertised listeners - 167

*** 9.5 Metrics - 167
**** JMX console - 167

*** 9.6 Tracing option - 170
**** Producer logic - 171
**** Consumer logic - 172
**** Overriding clients - 173

*** 9.7 General monitoring tools - 174
*** References - 176

* PART 3 CODING FURTHER - 179
** 10 Protecting Kafka - 181
*** 10.1 Security basics - 183
**** Encryption with SSL - 183
**** SSL between brokers and clients - 184
**** SSL between brokers - 187

*** 10.2 Kerberos and the Simple Authentication and Security Layer (SASL) - 187
*** 10.3 Authorization in Kafka - 189
**** Access control lists (ACLs) - 189
**** Role-based access control (RBAC) - 190

*** 10.4 ZooKeeper - 191
**** Kerberos setup - 191

*** 10.5 Quotas - 191
**** Network bandwidth quota - 192
**** Request rate quotas - 193

*** 10.6 Data at rest - 194
**** Managed options - 194

*** References - 195

** 11 Schema registry - 197
*** 11.1 A proposed Kafka maturity model - 198
**** Level0 - 198
**** Level1 - 199
**** Level2 - 199
**** Level3 - 200

*** 11.2 The Schema Registry - 200
**** Installing the Confluent Schema Registry - 201
**** Registry configuration - 201

*** 11.3 Schema features - 202
**** REST API - 202
**** Client library - 203

*** 11.4 Compatibility rules - 205
**** Validating schema modifications - 205

*** 11.5 Alternative to a schema registry - 207
*** References - 208

** 12 Stream processing with Kafka Streams and ksqlDB - 209
*** 12.1 Kafka Streams - 210
**** KStreams API DSL - 211
**** KTable API - 215
**** GlobalKTable API - 216
**** Processor API - 216
**** Kafka Streams setup - 218

*** 12.2 ksqlDB: An event-streaming database - 219
**** Queries - 220
**** Local development - 220
**** ksqlDB architecture - 222

*** 12.3 Going further - 223
**** Kafka Improvement Proposals (KIPs) - 223
**** Kafka projects you can explore - 223
**** Community Slack channel - 224

*** References - 224

** appendix A Installation - 227
** appendix B Client example - 234
** index - 239
