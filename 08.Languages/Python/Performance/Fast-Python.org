#+TITLE: Fast Python
#+SUBTITLE: High performance techniques for large datasets
#+VERSION: 2023
#+AUTHOR: Tiago Rodrigues Antão
#+STARTUP: entitiespretty
#+STARTUP: indent
#+STARTUP: overview

* preface ix
* acknowledgments xi
* about this book xii
* about the author xvii
* about the cover illustration xviii
* PART 1 FOUNDATIONAL APPROACHES - 1
In part 1 of this book, we will discuss foundational approaches regarding
performance with Python.

We will cover
- native Python libraries and fundamental data structures, and
- how Python can -- without external libraries -- make use of parallel
  processing techniques.
- An entire chapter on _NumPy optimization_ is also included.
  While NumPy is an external library, it's so crucial to modern data processing
  that it's as foundational as pure Python approaches.

** 1 An urgent need for efficiency in data processing - 3
This chapter covers
- The challenges of dealing with the exponential growth of data
- Comparing traditional and recent computing architectures
- The role and shortcomings of Python in modern data analytics
- Techniques for delivering efficient Python computing solutions

*** 1.1 How bad is the data deluge? - 4
*** 1.2 Modern computing architectures and highperformance computing - 7
**** 1.2.1 Changes inside the computer - 8
**** 1.2.2 Changes in the network - 10
**** 1.2.3 The cloud - 11

*** 1.3 Working with Python's limitations - 12
**** 1.3.1 The Global Interpreter Lock - 13

*** 1.4 A summary of the solutions - 14

** 2 Extracting maximum performance from built-in features - 17
- This chapter covers
  * *Profiling* code to *find* _speed and memory bottlenecks_
  * *Making more efficient use* of existing Python data structures
  * Understanding Python's _memory cost_ of allocating typical data structures
  * Using /lazy programming techniques/ to process large amounts of data

*** 2.1 Profiling applications with both IO and computing workloads - 18
**** 2.1.1 Downloading data and computing minimum temperatures - 19
**** 2.1.2 Python's built-in profiling module - 21
**** 2.1.3 Using local caches to reduce network usage - 22

*** 2.2 Profiling code to detect performance bottlenecks - 23
**** 2.2.1 Visualizing profiling information - 24
**** 2.2.2 Line profiling - 25
**** 2.2.3 The takeaway: Profiling code - 27

*** 2.3 Optimizing basic data structures for speed: Lists, sets, and dictionaries - 28
**** 2.3.1 Performance of list searches - 28
**** 2.3.2 Searching using sets - 29
**** 2.3.3 List, set, and dictionary complexity in Python - 30

*** 2.4 Finding excessive memory allocation - 32
**** 2.4.1 Navigating the minefield of Python memory estimation - 32
**** 2.4.2 The memory footprint of some alternative representations - 35
**** 2.4.3 Using arrays as a compact representation alternative to lists - 37
**** 2.4.4 Systematizing what we have learned: Estimating memory usage of Python objects - 38
**** 2.4.5 The takeaway: Estimating memory usage of Python objects - 39

*** 2.5 Using laziness and generators for big-data pipelining - 39
**** Using generators instead of standard functions - 40

** 3 Concurrency, parallelism, and asynchronous processing - 43
*** 3.1 Writing the scaffold of an asynchronous server - 46
**** Implementing the scaffold for communicating with clients - 48
**** Programming with coroutines - 49
**** Sending complex data from a simple synchronous client - 50
**** Alternative approaches to interprocess communication - 52
**** The takeaway: Asynchronous programming - 52

*** 3.2 Implementing a basic MapReduce engine - 52
**** Understanding MapReduce frameworks - 53
**** Developing a very simple test scenario - 54
**** A first attempt at implementing a MapReduce framework - 54

*** 3.3 Implementing a concurrent version of a MapReduce engine - 55
**** Using concurrent.futures to implement a threaded server - 55
**** Asynchronous execution with futures - 57
**** The GIL and multithreading - 59

*** 3.4 Using multiprocessing to implement MapReduce - 60
**** A solution based on concurrent.futures - 60
**** A solution based on the multiprocessing module - 62
**** Monitoring the progress of the multiprocessing solution - 63
**** Transferring data in chunks - 65

*** 3.5 Tying it all together: An asynchronous multithreaded and multiprocessing MapReduce server - 68
**** Architecting a complete high-performance solution - 68
**** Creating a robust version of the server - 72

** 4 High-performance NumPy - 75
*** 4.1 Understanding NumPy from a performance perspective - 76
**** Copies vs. views of existing arrays - 76
**** Understanding NumPy's view machinery - 81
**** Making use of views for efficiency - 86

*** 4.2 Using array programming - 88
**** The takeaway - 89
**** Broadcasting in NumPy - 90
**** Applying array programming - 92
**** Developing a vectorized mentality - 94

*** 4.3 Tuning NumPy's internal architecture for performance - 97
**** An overview of NumPy dependencies - 97
**** How to tune NumPy in your Python distribution - 99
**** Threads in NumPy - 100

* PART 2 HARDWARE - 103
** 5 Re-implementing critical code with Cython - 105
*** 5.1 Overview of techniques for efficient code re-implementation - 106
*** 5.2 A whirlwind tour of Cython - 107
**** A naive implementation in Cython - 108
**** Using Cython annotations to increase performance - 110
**** Why annotations are fundamental to performance - 111
**** Adding typing to function returns - 113

*** 5.3 Profiling Cython code - 114
**** Using Python's built-in profiling infrastructure - 115
**** Using _line_profiler_ - 116

*** 5.4 Optimizing array access with Cython memoryviews - 119
**** The takeaway - 121
**** Cleaning up all internal interactions with Python - 121

*** 5.5 Writing NumPy generalized universal functions in Cython - 122
**** The takeaway - 124

*** 5.6 Advanced array access in Cython - 124
**** Bypassing the GIL's limitation on running multiple threads at a time - 127
**** Basic performance analysis - 130
**** A spacewar example using Quadlife - 131

*** 5.7 Parallelism with Cython - 132

** 6 Memory hierarchy, storage, and networking - 135
*** 6.1 How modern hardware architectures affect Python performance - 137
**** The counterintuitive effect of modern architectures on performance - 137
**** How CPU caching affects algorithm efficiency - 138
**** Modern persistent storage - 139

*** 6.2 Efficient data storage with Blosc - 140
**** Compress data; save time - 140
**** Read speeds (and memory buffers) - 142
**** The effect of different compression algorithms on storage performance - 143
**** Using insights about data representation to increase compression - 144

*** 6.3 Accelerating NumPy with NumExpr - 144
**** Fast expression processing - 145
**** How hardware architecture affects our results - 146
**** When NumExpr is not appropriate - 147

*** 6.4 The performance implications of using the local network - 147
**** The sources of inefficiency with REST calls - 148
**** A naive client based on UDP and msgpack - 148
**** A UDP-based server - 150
**** Dealing with basic recovery on the client side - 151
**** Other suggestions for optimizing network computing - 152

* PART 3 APPLICATIONS AND LIBRARIES FOR MODERN DATA PROCESSING - 155
** 7 High-performance pandas and Apache Arrow - 157
*** 7.1 Optimizing memory and time when loading data - 158
**** Compressed vs. uncompressed data - 158
**** Type inference of columns - 160
**** The effect of data type precision - 162
**** Recoding and reducing data - 164

*** 7.2 Techniques to increase data analysis speed - 166
**** Using indexing to accelerate access - 166
**** Row iteration strategies - 167

*** 7.3 pandas on top of NumPy, Cython, and NumExpr - 170
**** Explicit use of NumPy - 170
**** pandas on top of NumExpr - 171
**** Cython and pandas - 173

*** 7.4 Reading data into pandas with Arrow - 174
**** The relationship between pandas and Apache Arrow - 175
**** Reading a CSV file - 176
**** Analyzing with Arrow - 178

*** 7.5 Using Arrow interop to delegate work to more efficient languages and systems - 179
**** Implications of Arrow's language interop architecture - 179
**** Zero-copy operations on data with Arrow's Plasma server - 180

** 8 Storing big data - 186
*** 8.1 A unified interface for file access: fsspec - 187
**** Using fsspec to search for files in a GitHub repo - 187
**** Using fsspec to inspect zip files - 189
**** Accessing files using fsspec - 189
**** Using URL chaining to traverse different filesystems transparently - 190
**** Replacing filesystem backends - 190
**** Interfacing with PyArrow - 191

*** 8.2 Parquet: An efficient format to store columnar data - 191
**** Inspecting Parquet metadata - 192
**** Column encoding with Parquet - 194
**** Partitioning with datasets - 196

*** 8.3 Dealing with larger-than-memory datasets the old-fashioned way - 197
**** Memory mapping files with NumPy - 198
**** Chunk reading and writing of data frames - 199

*** 8.4 Zarr for large-array persistence - 201
**** Understanding Zarr's internal structure - 202
**** Storage of arrays in Zarr - 204
**** Creating a new array - 206
**** Parallel reading and writing of Zarr arrays - 208

* PART 4 ADVANCED TOPICS - 211
** 9 Data analysis using GPU computing - 213
*** 9.1 Making sense of GPU computing power - 215
**** Understanding the advantages of GPUs - 215
**** The relationship between CPUs and GPUs - 217
**** The internal architecture of GPUs - 218
**** Software architecture considerations - 219

*** 9.2 Using Numba to generate GPU code - 220
**** Installation of GPU software for Python - 220
**** The basics of GPU programming with Numba - 221
**** Revisiting the Mandelbrot example using GPUs - 224
**** A NumPy version of the Mandelbrot code - 227

*** 9.3 Performance analysis of GPU code: The case of a CuPy application - 228
**** GPU-based data analysis libraries - 228
**** Using CuPy: A GPU-based version of NumPy - 229
**** A basic interaction with CuPy - 229
**** Writing a Mandelbrot generator using Numba - 230
**** Writing a Mandelbrot generator using CUDA C - 232
**** Profiling tools for GPU code - 234

** 10 Analyzing big data with Dask - 238
*** 10.1 Understanding Dask's execution model - 240
**** A pandas baseline for comparison - 240
**** Developing a Dask-based data frame solution - 241

*** 10.2 The computational cost of Dask operations - 243
**** Partitioning data for processing - 244
**** Persisting intermediate computations - 245
**** Algorithm implementations over distributed data frames - 246
**** Repartitioning the data - 249
**** Persisting distributed data frames - 251

*** 10.3 Using Dask’s distributed scheduler - 252
**** The dask.distributed architecture - 253
**** Running code using dask.distributed - 257
**** Dealing with datasets larger than memory - 262

* appendix A Setting up the environment - 265
* appendix B Using Numba to generate efficient low-level code - 269
* index - 275
