#+TITLE: Mastering Concurrency in Python
#+AUTHOR: Quan Nguyen
#+RELEASE DATE: Nov. 2018
#+STARTUP: entitiespretty
#+STARTUP: indent
#+STARTUP: overview

* Preface
** Who this book is for
** What this book covers
** To get the most out of this book
*** Download the example code files
*** Download the color images
*** Code in Action
*** Conventions used

** Get in touch
*** Reviews

* DONE Chapter 1: Advanced Introduction to Concurrent and Parallel Programming - 8
  CLOSED: [2021-04-18 Sun 02:25]
  - The following topics will be covered in this chapter:
    * The *concept* of /concurrency/

    * Why some programs *cannot* be made concurrent, and
      how to differentiate them from programs that can

    * The *history* of /concurrency/ in computer science:
      how it is used in the industry today, and what can be expected in the future

    * The specific topics that will be covered in each section/chapter of the book

    * How to *set up* a _Python environment_, and
      how to check out/download code from GitHub

** DONE Technical requirements - 9
   CLOSED: [2021-04-21 Wed 03:37]
** DONE What is concurrency? - 9
   CLOSED: [2021-04-17 Sat 03:17]
   - It is estimated that
     the amount of data that needs to be processed by computer programs doubles
     every two years.

*** DONE Concurrent versus sequential - 9
    CLOSED: [2021-04-17 Sat 03:07]
    - Understand /concurrent programming/ by comparing it to /sequential programming/.
      * A /sequential program/ is in one place at a time,
        WHILE a in a /concurrent program/, different components are in /states/
        of _independent_, or semi-independent.

    - The following diagram illustrates the basic differences between these
      _TWO_ types:
      #+NAME: Difference between concurrent and sequential programs
      #+begin_src text
        Concurrent
        ==========

        BEGIN
         |--> Statement -->|
         |--> Statement -->|
         |--> Statement -->|
                           |
                          \|/
                          END

        Sequential
        ==========

           BEGIN
             |
         Statement
             |
         Statement
             |
         Statement
             |
            \|/
            END
      #+end_src

    - One immediate advantage of /concurrency/ is an improvement in
      execution performance -- *REDUCE execution time*.

*** TODO Example 1 - checking whether a non-negative number is prime - 10 - =NOTE=
*** DONE Concurrent versus parallel - 13
    CLOSED: [2021-04-17 Sat 03:17]
    - /parallel programs/:
      there are a number of processing flows (mainly CPUs and cores) working
      *independently* all at once,

    - /concurrent programs/:
      there might be different processing flows (mostly /threads/) accessing and
      using a *shared resource* at the same time.

**** A quick metaphor - 14

** DONE Not everything should be made concurrent - 14
   CLOSED: [2021-04-18 Sun 00:26]
*** Embarrassingly parallel - 15
*** Inherently sequential - 15
    - Famous examples of *inherent sequentiality* include
      /iterative algorithms/:
      * Newton's method,
      * iterative solutions to the three-body problem, or
      * iterative numerical approximation methods.

**** Example 2 - inherently sequential tasks - 16
     *BAD and WRONG example!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!*

     =from Jian=
     - The concurrent program in this example has a *WRONG result* because of
       /race condition/.

     - I tried to add lock to it, and for rank 20, two methods take the same
       time in my computer. Increase the rank to 25, and the concrrent method is
       even faster -- this is *complete OPPOSITE to the conclusion* of this
       section.

*** I/O bound - 18 - =TODO= =read other materials=

** DONE The history, present, and future of concurrency - 18 - =NOTE=
   CLOSED: [2021-04-18 Sun 02:11]
*** The history of concurrency - 19
*** The present - 20
*** The future - 22

** DONE A brief overview of mastering concurrency in Python - 24
   CLOSED: [2021-04-18 Sun 01:40]
   Python is a popular and widely used programming language.
   Though it is criticized by developers about its /Global Interpreter Lock (GIL)/
   and the difficulty of implementing concurrent and parallel programs that it
   leads to.

   - /Concurrency/ and /parallelism/ do behave differently in Python than in other
     common programming languages.

   - This book will divided into *SIX* main sections.
     * Section 1:
       It will start with _the *idea* BEHIND concurrency and concurrent programming_
       -- *the history*, how it is being used in the industry today, and finally,
       a mathematical analysis of the speedup that concurrency can potentially
       provide.

     * Section 2, 3, 4:
       Three main implementation approaches in /concurrent programming/ will be
       covered:
       + Chapter 3, 4, 5 - /threads/
       + Chapter 6, 7, 8 - /processes/
       + Chapter 9, 10, 11 - _asynchronous I/O_

     * Section 5:
       Introduce readers to some of the _most common problems_ that engineers and
       programmers face in /concurrent programming/:
       + Chapter 12 - /deadlock/,
       + Chapter 13 - /starvation/, and
       + Chapter 14 - /race conditions/.

       + Readers will learn about the
         1. _theoretical foundations_ and _CAUSES for each problem_,
         2. _analyze and replicate each of them in Python_, and
         3. finally IMPLEMENT potential solutions.

       + The last chapter (Chapter 15) in this section will discuss the
       aforementioned *GIL*, which is specific to the Python language. It will
       cover
       + the GIL's _integral role_ in the Python ecosystem,
       + some _challenges_ that the GIL poses for concurrent programming, and
       + how to implement _effective workarounds_.

     * Section 6:
       In the last section of the book, we will be working on various advanced
       applications of concurrent Python programming.
       + These applications will include the design of
         - CHPATER 16 - *lock-free* and *lock-based* concurrent data structures,
         - CHPATER 17 - /memory models/ and operations on /atomic types/, and
         - CHPATER 18 - how to *build* a server that supports _concurrent request
                        processing_ *from scratch.*

       + Chapter 19 covers the the best practices when
         - _TESTING_,
         - _DEBUGGING_, and
         - _SCHEDULING concurrent Python applications_.

*** DONE Why Python? - 25 - =NOTE=
    CLOSED: [2021-04-18 Sun 02:10]
    =RE-FORMAT and summarize notes below=
    - EVEN THOUGH the GIL prevents /multithreaded CPython programs/ from taking
      full advantage of _multiprocessor systems_ *in certain situations*,
      _MOST blocking or long-running operations, such as I/O, image processing,
      and NumPy number crunching, happen *outside* the GIL._

        Therefore, the GIL only becomes a _potential bottleneck_ for /multithreaded
      programs/ that spend significant time *inside the GIL*.

      As you will see in future chapters,
      /multithreading/ is _only *one* form of /concurrent programming/,_ and,
      while the GIL poses some challenges for /multithreaded CPython programs/
      that allow more than one thread to access shared resources,

      other forms of concurrent programming do not have this problem.

      For example, multiprocessing applications that do not share any common
      resources among processes, such as I/O, image processing, or NumPy number
      crunching, can work seamlessly with the GIL. We will discuss the GIL and
      its place in the Python ecosystem in greater depth in Chapter 15, The
      Global Interpret Lock.

** DONE Setting up your Python environment - 27
   CLOSED: [2021-04-18 Sun 02:11]
*** General setup - 27
*** Downloading example code - 28

** Summary - 29
** TODO Questions - 30
** TODO Further reading - 30

* DONE Chapter 2: Amdahl's Law - 31
  CLOSED: [2021-04-21 Wed 01:33]
  The following topics will be covered in this chapter:
  - /Amdahl's Law/
  - /Amdahl's Law/: its formula and interpretation
  - The relationship between /Amdahl's Law/ and _the law of diminishing returns_
  - Simulation in Python, and the practical applications of /Amdahl's Law/

** Technical requirements - 31
** TODO Amdahl's Law - 32 - =NOTE=
*** TODO Terminology - 32 - =NOTE=

** TODO Formula and interpretation - 33 - =NOTE=
*** DONE The formula for Amdahl's Law - 33
    CLOSED: [2021-04-19 Mon 02:05]
    - Now, let B denote the fraction of the program that is strictly serial, and
      consider the following:
      * B * T(1) is the time it takes to execute the parts of the program that are
      inherently sequential.
      * T(1) - B * T(1) = (1 - B) * T(1) is the time it takes to execute the parts
        of the program that are parallelizable, with one processor:
        + Then, (1 - B) * T(1) / N is the time it takes to execute these parts
          with N processors
      * So, B * T(1) + (1 - B) * T(1) / N is the total time it takes to execute
        the whole program with N processors.

    - Coming back to the formula for the speedup quantity, we have the following:
      #+begin_src text
        S = T(1) / T(j)
          = T(1) / (B * T(1) + (1 - B) * T(1) / j)
          = 1 / (B + (1 + B)/j)
      #+end_src

**** A quick example - 34

*** TODO Implications - 34 - =NOTE=

** DONE Amdahl's Law's relationship to the law of diminishing returns - 35
   CLOSED: [2021-04-19 Mon 02:16]
   - /Amdahl's Law/ is often conflated with /the law of diminishing returns/,
     which is a rather popular concept in economics.
       *However*, /the law of diminishing returns/ is only a special case of
     applying /Amdahl's Law/, _depending on the order of improvement:_
     * optimal
     * reverse-optimal

   - Another similarity between /Amdahl's Law/ and /the law of diminishing returns/
     concerns the improvement in speedup obtained through adding more processors
     to a system:
       As we discussed in the last section, the improvement in this situation
     strictly decreases as the number of processors increases, and the total
     throughout approaches the *upper boundary* of _1/B_.

   - It is important to note that this analysis _does NOT_ take into account other
     potential bottlenecks, such as /memory bandwidth/ and _I/O bandwidth_.
     * In fact,
       if these resources do not scale with the number of processors, then
       simply adding processors results in even lower returns.

** DONE How to simulate in Python - 36
   CLOSED: [2021-04-19 Mon 02:16]
** TODO Practical applications of Amdahl's Law - 40
** TODO Summary - 41
** TODO Questions - 42
** TODO Further reading - 42

* DONE Chapter 3: Working with Threads in Python - 43
  CLOSED: [2021-04-24 Sat 23:06]
  The following topics will be covered in this chapter:
  - The concept of a /thread/ in the context of /concurrent programming/
    in computer science

  - The _basic API_ of _the ~threading~ module_ in Python

  - How to *create* a new /thread/ via _the ~threading~ module_

  - The *concept* of a /lock/ and
    _HOW TO_ use DIFFERENT /locking mechanisms/ to *synchronize* /threads/

  - The *concept* of a /queue/ in the context of /concurrent programming/, and
    _HOW TO_ use the ~Queue~ module to work with queue objects in Python

** DONE Technical requirements - 44
   CLOSED: [2021-04-24 Sat 23:05]
** DONE The concept of a thread - 44 - =NOTE=
   CLOSED: [2021-04-21 Wed 02:44]
   In the field of computer science,
   - thread ::
     the *smallest unit* of programming commands (code) that a /scheduler/ (usually
     as part of an operating system) can process and manage.
     * Depending on the operating system, the implementation of /threads/ and
       /processes/ (which we will cover in /future/ chapters) *varies*, but a
       /thread/ is typically an element (a component) of a /process/.

*** DONE Threads versus processes - 44
    CLOSED: [2021-04-21 Wed 02:27]
    - *More than one* /thread/ can be implemented within the same /process/, most
      often executing concurrently and accessing/sharing the same resources,
      such as memory; *separate /processes/ do _NOT_ do this.*

    - /Threads/ in the same /process/ share the latter's
      * /instructions/ (its code) and
      * /context/ (the values that its variables reference at any given moment).

    - The key difference between the two concepts is that
      a /thread/ is typically a _component_ of a /process/.

    - One /process/ can include *multiple* /threads/, which can be executing
      _SIMULTANEOUSLY_.

    - /Threads/ also usually allow for *shared resources*, such as memory and
      data, while _it is *fairly rare* for /processes/ to do so._
      * In short, a /thread/ is an independent component of computation that is
        similar to a /process/, but _the /threads/ within a /process/_ can share
        the *address space*, and hence the data, of that /process/.

    - HISTORY:
      * /Threads/ were reportedly first used for a variable number of tasks in
        OS/360 multiprogramming, which is a discontinued batch processing system
        that was developed by IBM in 1967.
          At the time, threads were called tasks by the developers.

      * The term /thread/ became popular later on and has been attributed to
        Victor A. Vyssotsky, a mathematician and computer scientist who was the
        founding director of Digital's Cambridge Research Lab.

*** TODO Multithreading - 44 - =NOTE=
*** DONE An example in Python - 44
    CLOSED: [2021-04-21 Wed 02:44]

** DONE An overview of the threading module - 51
   CLOSED: [2021-04-21 Wed 02:01]
*** DONE The ~thread~ module in Python 2 - 51
    CLOSED: [2021-04-21 Wed 01:48]
    Python 2 uses ~thread~, and it is renamed to ~_thread~ in Python 3.

    - ~thread~ (in Python 2) / ~_thread~ (in Python 3) include *low-level API*.
      * It is *NOT* deprecated.

    - Migration:
      * Use ~2to3~
      * Replace ~import thread~ with ~import _thread as thread~ manually

    - The main feature of the ~thread~ module is its *fast* and SUFFICIENT method
      of creating new /threads/ to execute functions:
      the ~thread.start_new_thread()~ function.

      * Aside from this, the /module/ only supports a number of *low-level ways*
        to work with _multithreaded primitives_ and *share* their _global data
        space_.
          Additionally, _simple lock objects_ (for example, /mutexes/ and
        /semaphores/) are provided for _synchronization purposes_.

*** DONE The ~threading~ module in Python 3 - 51
    CLOSED: [2021-04-21 Wed 02:01]
    - The old ~thread~ (~_thread~ in Python 3) module has been _considered
      deprecated_ (but *not* actually deprecated) by Python developers for a
      long time, mainly because of its rather _low-level functions_ and _limited
      usage_.

    - The ~threading~ module, on the other hand, is built _on top of_ the ~thread~
      module, providing easier ways to work with /threads/ through powerful,
      *higher-level APIs*.
      * Python users have actually been encouraged to utilize the new ~threading~
        module over the ~thread~ module in their programs.

    - Compare:
      * The ~thread~ module:
        *consider each /thread/ a function;*
        when the ~thread.start_new_thread()~ is called, it actually takes in a
        separate function as its main argument, in order to spawn a new
        /thread/.

      * The ~threading~ module:
        It is designed to be user-friendly for those that come from the
        object-oriented software development paradigm, treating each thread that
        is created *as an object.*

    - In addition to all of the functionality for working with /threads/ that the
      ~thread~ module provides, the ~threading~ module supports a number of *extra*
      methods, as follows:
      * ~threading.activeCount()~:
        This function returns the number of currently active thread objects in
        the program

      * ~threading.currentThread()~:
        This function returns the number of thread objects
        _in the current thread control_ from the caller

      * ~threading.enumerate()~:
        This function returns a list of all of the _currently active_ thread
        objects in the program

    - Following the object-oriented software development paradigm, the
      ~threading~ module also provides a ~Thread~ /class/ that supports the
      object-oriented implementation of /threads/.
      The following methods are supported in this class:
      * ~run()~:
        This method is executed
        when a new thread is _initialized_ and _started_

      * ~start()~:
        This method starts the initialized calling thread object by calling the
        ~run()~ method

      * ~join()~:
        This method waits for the calling thread object to terminate
        *before*
        continuing to execute the rest of the program

      * ~isAlive()~:
        This method returns a Boolean value,
        indicating whether the calling thread object is currently executing

      * ~getName()~:
        This method returns the name of the calling thread object

      * ~setName()~:
        This method sets the name of the calling thread object

** DONE Creating a new thread in Python - 52
   CLOSED: [2021-04-24 Sat 17:25]
*** DONE Starting a thread with the ~thread~ module - 53
    CLOSED: [2021-04-24 Sat 16:49]
    ~thread.start_new_thread(function, args[, kwargs])~

    - This function will return a thread id.

    - ~input('Type something to quit: \n')~ in the =chapter03/example2.py= is
      important. Without it, the the program will terminate without waiting for
      the spawned /threads/ to finish their tasks.
    
*** DONE Starting a thread with the ~threading~ module - 55
    CLOSED: [2021-04-24 Sat 17:25]
    - To *create* and *customize* a NEW /thread/ using the ~threading~ module,
      there are specific steps that need to be followed:
      1. Define a /subclass/ of the ~threading.Thread~ /class/ in your program;

      2. _OVERRIDE_ the default ~__init__(self [,args])~ method inside of the
         /subclass/, in order to add *custom* arguments for the /class/;

      3. _OVERRIDE_ the default ~run(self [,args])~ method inside of the /subclass/,
         in order to *customize* the behavior of the /thread/ class _WHEN a NEW
         thread is *initialized* and *started*._
         
    - Customized /thread/:
      #+NAME: chapter03/example3.py
      #+begin_src python
        import threading


        class MyThread(threading.Thread):
            def __init__(self, x):
                threading.Thread.__init__(self)
                self.x = x

            def run(self):
                print(f"Starting processing {x}...")
                is_prime(self.x)


        def main():
            my_input = [2, 193, 323, 1327, 433785907]
            threads = []
            for x in my_input:
                t = MyThread(x)
                t.start()
                threads.append(t)

            for thread in threads:
                thread.join()

            print('Finished.')
      #+end_src

    - ~threading~ has the higher-level API, for example, the ~join()~.

** DONE Synchronizing threads - 58
   CLOSED: [2021-04-24 Sat 20:29]
   Look at using the ~threading~ /module/ in /thread synchronization/.
   
*** DONE The concept of thread synchronization - 58
    CLOSED: [2021-04-24 Sat 20:22]
    Before we jump into an actual Python example, let's explore the concept of
    /synchronization/ in computer science.

    - As you saw in previous chapters,
      SOMETIMES, it is *undesirable* to have all portions of a program execute
      in a parallel manner.
      * In fact, in most contemporary concurrent programs,
        + there are _sequential portions_ and _concurrent portions_ of the code;
        + furthermore, even inside of a _concurrent portion_, some form of
          *coordination* between DIFFERENT _threads/processes_ is also required.

    - Thread/process synchronization :: 
      various mechanisms to ensure that *no more than one* concurrent
      thread/process can process and execute a particular program portion _at a
      time;_ this portion is known as the /critical section/,

      * =TODO=
        we will discuss it in further detail when we consider common problems in
        concurrent programming in Chapter 12, Starvation, and Chapter 13, Race
        Conditions.

*** DONE The ~threading.Lock~ class - 59
    CLOSED: [2021-04-24 Sat 20:29]
    One of the most common ways to apply /thread synchronization/ is through the
    implementation of a /locking mechanism/.
    
    - The ~threading.Lock~ class provides a simple and intuitive approach to
      creating and working with /locks/.

    - The main usage of ~threading.Lock~:
      * ~threading.Lock()~:
        This method initializes and returns a new _lock object_.

      * ~acquire(blocking)~:
        When this method is called, all of the threads will run synchronously
        (that is, only one thread can execute the critical section at a time):
        + The optional argument blocking allows us to specify whether the _CURRENT
          thread_ should wait to acquire the lock
          - When ~blocking = 0~, the _current thread_ does *NOT wait* for the
            /lock/ and simply returns
            * 0 if the lock cannot be acquired by the thread, or
            * 1 otherwise

          - When ~blocking = 1~, the _current thread_ *blocks and waits* for the
            /lock/ to be _released_ and _acquires_ it afterwards

      * ~release()~:
        When this method is called, the /lock/ is released.
      
*** DONE An example in Python - 59
    CLOSED: [2021-04-24 Sat 20:29]
    =chapter03/example4.py=

** DONE Multithreaded priority queue - 61
   CLOSED: [2021-04-24 Sat 23:02]
   =from Jian=
   Bad title.
   Only the last sub-section discussed the /priority queue/.
   Suggested one: *Multithreaded queue*.
   
*** DONE A connection between real-life and programmatic queues - 61
    CLOSED: [2021-04-24 Sat 20:47]
*** DONE The ~queue~ module - 62
    CLOSED: [2021-04-24 Sat 20:53]
    Each /queue/ in the ~queue.Queue~ class
    - _can hold a specific amount of element,_
      and
    - _can have the following methods as its high-level API:_
      * ~get()~:
        return and remove the next element of the calling queue object

      * ~put()~:
        add a new element to the calling queue object

      * ~qsize()~:
        return the number of current elements in the calling queue
        object (that is, its size)

      * ~empty()~:
        Check if a queue is empty.

      * ~full()~:
        Check if a queue is full.

*** DONE Queuing in concurrent programming - 63
    CLOSED: [2021-04-24 Sat 23:02]
    - In this example (=chapter03/example5.py=), we have implemented the structure
      that we discussed earlier:
      1. a /task queue/ that holds all the tasks to be executed and
      2. a /thread pool/ (threads A, B, and C) that interacts with the /queue/ to
         process its elements individually.

*** DONE Multithreaded priority queue - 67
    CLOSED: [2021-04-24 Sat 23:02]
    =from Jian= No code, just explain the concept of /priority queu/ and its usage.

** DONE Summary - 68 - =NOTE=
   CLOSED: [2021-04-24 Sat 23:06]
** TODO Questions - 69
** TODO Further reading - 69

* DONE Chapter 4: Using the ~with~ Statement in Threads - 70
  CLOSED: [2021-04-25 Sun 01:11]
  The following topics will be covered in this chapter:
  - The concept of /context management/ and the options that the ~with~ statement
    provides as a /context manager/, _SPECIFICALLY in concurrent and parallel
    programming_

  - The syntax of the ~with~ statement and
    HOW TO use it effectively and efficiently

  - The different ways of using the ~with~ statement in /concurrent programming/

** DONE Technical requirements - 70
   CLOSED: [2021-04-24 Sat 23:11]
** DONE Context management - 71
   CLOSED: [2021-04-25 Sun 00:52]
   The new ~with~ statement was first introduced in Python 2.5.
   
*** DONE Starting from managing files - 71
    CLOSED: [2021-04-25 Sun 00:51]
    - ~ulimit -n~ give you the number of opening files that your system can handle.

    - /file descriptor leakage/

    - Instead of
      #+begin_src python
        n_files = 10
        files = []

        for i in range(n_files):
            files.append(open(f"output1/sample{i}.txt", 'w'))
      #+end_src

      write (assume you don't know the usage of ~with~)
      #+begin_src python

        n_files = 10
        files = []

        for i in range(n_files):
            f = open(f"output1/sample{i}.txt", 'w')
            files.append(f)
            f.close()
      #+end_src
      
*** DONE The ~with~ statement as a context manager - 72
    CLOSED: [2021-04-25 Sun 00:45]
*** DONE The syntax of the with statement - 74
    CLOSED: [2021-04-25 Sun 00:46]

** DONE The ~with~ statement in concurrent programming - 74
   CLOSED: [2021-04-25 Sun 01:07]
*** Example of deadlock handling - 75
    Use ~with~ to handle /lock/ management.
    It can handle _lock release_ when an exception is raised.
    
    For example,
    #+begin_src python
      from threading import Lock

      my_lock = Lock()


      # Can't release the lock when an exception is raised.
      def get_data_from_file_v1(filename):
          my_lock.acquire()
          with open(filename, 'r') as f:
              data.append(f.read())
          my_lock.release()

      data = []

      try:
          get_data_from_file('output2/sample0.txt')
      except FileNotFoundError:
          print('Encountered an exception...')

      my_lock.acquire()
      print('Lock can still be acquired.')
    #+end_src
    - This is an illustration example, it want to show that that
      LAST _lock acquirement_ can NEVER happen, therefore you can never see the
      last ~print~ output.

    - Fix this by managing the /lock/ with ~with~, as what we did to manage a
      file:
      #+begin_src python
        def get_data_from_file_v2(filename):
            my_lock.acquire()
            with my_lock, open(filename, 'r') as f:
                data.append(f.read())
      #+end_src

** DONE Summary - 77
   CLOSED: [2021-04-25 Sun 01:08]
** TODO Questions - 77
** TODO Further reading - 78

* DONE Chapter 5: Concurrent Web Requests - 79
  CLOSED: [2021-05-01 Sat 02:43]
  In this chapter, we will cover the following concepts:
  - The basics of _web requests_
  - The ~requests~ module
  - _Concurrent web requests_
  - The problem of /timeout/
  - Good practices in making _web requests_

** DONE Technical requirements - 79
   CLOSED: [2021-04-30 Fri 22:33]
** DONE The basics of web requests - 80
   CLOSED: [2021-05-01 Sat 00:23]
   Often, we need to collect the data we need from web pages.

   - The worldwide capacity to generate data is estimated to double in size every
     two years.

   - /Web scraping/ is a *data extraction method* that AUTOMATICALLY
     + _makes requests_ to web pages and
     + _downloads_ specific information.

   - In this chapter, we will mainly work with the ~requests~ module to make
     _client-side web requests_ from our Python programs.

*** DONE HTML - 80
    CLOSED: [2021-04-30 Fri 22:47]
*** DONE HTTP requests - 82
    CLOSED: [2021-04-30 Fri 23:44]
    - Generally, _request methods_ are defined as *verbs* that indicate the
      desired action to be performed while the HTTP client (web browsers) and the
      server communicate with each other:
      * ~GET~,
      * ~HEAD~,
      * ~POST~,
      * ~PUT~,
      * ~DELETE~,
      * and so on.

    - ~GET~ and ~POST~ are two of the _MOST common request methods_ used in
      web-scraping applications; their function is described in the following
      list:
      * The ~GET~ method makes a /request/ for a specific data from the server.
        This method _ONLY retrieves data_ and has *no other effect* on the
        server and its databases.

      * The ~POST~ method sends data in a specific form that is accepted by the
        server.

    - All general-purpose HTTP servers that we commonly see on the internet are
      actually required to implement at least the ~GET~ (and ~HEAD~) method,
      while the ~POST~ method is considered optional.

*** DONE HTTP status code - 83
    CLOSED: [2021-05-01 Sat 00:23]
    - As a way to categorize these problems as well as provide the most information
      as possible during the communication resulting from a /web request/,
      HTTP requires servers to respond to each request from its clients an
      /HTTP response status code/.

      * status code :: typically a _three-digit number_ that indicates the
        specific characteristics of the response that the server sends back to a
        client.

    - There are in total *FIVE large categories* of /HTTP response status codes/,
      *indicated by the first digit of the code.* They are as follows:
      * 1xx (informational status code) ::
        The request was received and the server is processing it.
        + For example,
          - *100* means the request header has been received and the server is
            waiting for the request body;

          - *102* indicates that the request is currently being processed (this
            is used for large requests and to prevent clients from timing out).

      * 2xx (successful status code) ::
        The /request/ was _SUCCESSFULLY_ *received*, *understood*, and *processed*
        by the server.
        + For example,
          - *200* means the /request/ was _successfully fulfilled_;

          - *202* indicates that the /request/
            1. has _been accepted for processing_,
            2. but the processing itself is _not complete_.

      * 3xx (redirectional status code) ::
        Additional actions need to be taken so that the request can be
        successfully processed.
        + For example,
          - *300* means that
            there are *multiple options* regarding how the response from the
            server should be processed (for example, giving the client multiple
            video format options when a video file is to be downloaded);

          - *301* indicates that
            * *the server has been moved permanently* and
            * *all requests should be directed to another address (provided in
              the response from the server).*

      * 4xx (error status code for the client) ::
        The request was *incorrectly formatted* _by the client_ and could not be
        processed.
        + For example,
          - *400* means that the client sent in a *bad request* (for example,
            _syntax error_ or _the size of the request is too large_);

          - *404* (arguably the most _well-known status code_) indicates that the
            request method is *NOT supported by the server.*

      * 5xx (error status code for the server) ::
        The request, although valid, could not be processed by the server.
        + For example,
          - *500* means there is an _internal server error_ in which an unexpected
            condition was encountered;

          - *504* (Gateway Timeout) means that the server, which was acting as a
            gateway or a proxy, _did not receive a response_ from the final server
            in time.

    - The *Internet Assigned Numbers Authority (IANA)* maintains the official
      registry of HTTP status codes:
      [[https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml#http-status-codes-1][Hypertext Transfer Protocol (HTTP) Status Code Registry]]

** DONE The ~requests~ module - 84
   CLOSED: [2021-05-01 Sat 00:54]
*** DONE Making a request in Python - 85
    CLOSED: [2021-05-01 Sat 00:30]
    ~requests.get()~

    - There is other information that is stored on the server that web pages of
      that server make reference to.
        This means that *not all of the information that an online web page
      provides can be downloaded via a ~GET~ request,* and this is why offline
      HTML code sometimes fails to contain all of the information available on
      the online web page that it was downloaded from.
      * For example,
        the downloaded HTML code in the preceding screenshot does not display
        the Google icon correctly.

*** DONE Running a ping test - 87
    CLOSED: [2021-05-01 Sat 00:54]

** DONE Concurrent web requests - 88
   CLOSED: [2021-05-01 Sat 01:00]
*** DONE Spawning multiple threads - 89
    CLOSED: [2021-05-01 Sat 01:01]
    Use ~threading.Thread~ to create /threads/.
    We can observe performance improvement.

*** DONE Refactoring request logic - 91
    CLOSED: [2021-05-01 Sat 01:01]
    Create the ~MyThread~ class, which inherit ~threading.Thread~.

** DONE The problem of timeout - 93
   CLOSED: [2021-05-01 Sat 01:59]
   In this section, we will explore a potential improvement to be made to our
   ping test application: *timeout handling*.

   - /Timeouts/ typically occur when
     * the server takes an *unusually long time* to process a specific request, and
     * the connection between the server and its client is *terminated*.

*** DONE Support from httpstat.us and simulation in Python - 93
    CLOSED: [2021-05-01 Sat 01:53]
*** DONE Timeout specifications - 94
    CLOSED: [2021-05-01 Sat 01:53]
    #+NAME: chapter05/example6.py
    #+begin_src python
      import threading
      import time

      import requests

      UPDATE_INTERVAL = 0.01


      class MyThread(threading.Thread):
          def __init__(self, url):
              threading.Thread.__init__(self)
              self.url = url
              self.result = f'{self.url}: Custom timeout'

          def run(self):
              res = requests.get(self.url)
              self.result = f'{self.url}: {res.text}'


      def process_requests(threads, timeout=5):
          def alive_count():
              alive = [1 if t.is_alive() else 0 for t in threads]
              return sum(alive)

          while alive_count() > 0 and timeout > 0:
              timeout -= UPDATE_INTERVAL
              time.sleep(UPDATE_INTERVAL)

          for thread in threads:
              print(thread.result)


      urls = [
          'http://httpstat.us/200',
          'http://httpstat.us/200?sleep=4000',
          'http://httpstat.us/200?sleep=20000',
          'http://httpstat.us/400'
      ]


      def main():
          start = time.time()
          threads = [MyThread(url) for url in urls]
          for thread in threads:
              thread.setDaemon(True)
              thread.start()
          process_requests(threads)

          print(f"Took {time.time() - start : .2f} seconds")

          print('Done.')


      if __name__ == '__main__':
          main()


      ## http://httpstat.us/200: 200 OK
      ## http://httpstat.us/200?sleep=4000: 200 OK
      ## http://httpstat.us/200?sleep=20000: Custom timeout
      ## http://httpstat.us/400: 400 Bad Request
      ## Took 5.70 seconds
      ## Done.
    #+end_src
    - Check the ~MyThread~ class and
      the output 
      #+begin_src text
        http://httpstat.us/200?sleep=20000: Custom timeout
      #+end_src
      we can see the ping o this URL is time out -- ~self.result~ is not
      updated.

    - The ~thread.setDaemon(True)~ is important, or the program will be blocked
      until all the requests are done.

** DONE Good practices in making web requests - 98
   CLOSED: [2021-05-01 Sat 02:41]
   In this section, we will be going over
   - A few aspects of making concurrent web requests that require
     _CAREFUL CONSIDERATION and IMPLEMENTATION._ and

   - Some of the _best practices_ that you should use
     when developing your applications.

*** DONE Consider the terms of service and data-collecting policies - 98
    CLOSED: [2021-05-01 Sat 02:21]
    =IMPORTANT=

*** DONE Error handling - 98
    CLOSED: [2021-05-01 Sat 02:40]
    - Error is something that no one can easily avoid in the field of programming,
      and this is especially true in making web requests.
      * Errors in these programs can include
        + making _bad requests_ (invalid requests or even bad internet connections),
        + mishandling downloaded HTML code, or unsuccessfully parsing HTML code.

    - Specifically in concurrent web scraping, it might be possible for some
      threads to collect data successfully, while others fail. =IMPORTANT=
      * By implementing error-handling functionalities in multithreaded parts of
        your program, you can make sure that a failed thread will not be able to
        crash the entirety of your program and ensure that successful threads
        can still return their results.

    - However,
      _it is important to note that blind error-catching is still undesirable._

*** DONE Update your program regularly - 99
    CLOSED: [2021-05-01 Sat 02:27]
    Since the websites request-handling logic may change, and the webpages HTML
    keep changing, your program should also keep updating.

*** DONE Avoid making a large number of requests - 99
    CLOSED: [2021-05-01 Sat 02:33]
    =IMPORTANT=
    Check the terms and policies of a website or contact the website manager to
    learn the _requests frequency limit_.

    - High frequency requests that exceed the limit can be considered as
      improper or attack (Denial of Service or Distributed Denial of Service).
      Then, the requests might be delayed, refused, and banned.

** TODO Summary - 101
** TODO Questions - 101
** TODO Further reading - 101

* DONE Chapter 6: Working with Processes in Python - 102 - =NOTE=
  CLOSED: [2021-04-26 Mon 03:33]
  The following topics will be covered in this chapter:
  - The concept of a /process/ in the context of /concurrent programming/
    in computer science

  - The _basic API_ of the ~multiprocessing~ module in Python

  - How to interact with /processes/ and the advanced functionalities that the
    ~multiprocessing~ module provides

  - HOW the ~multiprocessing~ module supports /interprocess communication/

  - The *key differences* between /multiprocessing/ and /multithreading/
    in concurrent programming

** DONE Technical requirements - 103
   CLOSED: [2021-04-25 Sun 01:19]
** TODO The concept of a process - 103
*** Processes versus threads - 105
*** Multiprocessing - 106
*** Introductory example in Python - 108

** TODO An overview of the ~multiprocessing~ module - 110
   The ~multiprocessing~ module is one of the most commonly used implementations
   of multiprocessing programming in Python.

   - ~multiprocessing~ offers methods to *spawn* and *interact with* /processes/
     using an API similar to the ~threading~ module (as we saw with the
     ~start()~ and ~join()~ methods in the preceding example).

   - According to its documentation website, the ~multiprocessing~ module
     * allows _both *local* and *remote* concurrency_ and

     * effectively avoids the *global interpreter lock (GIL)* in Python (which we
       will discuss in more detail later in _Chapter 15, The Global Interpreter
       Lock)_ by *using /subprocesses/ INSTEAD OF /threads/.*

*** DONE The ~Process~ class - 110
    CLOSED: [2021-04-25 Sun 02:19]
    - In the ~multiprocessing~ module, /processes/ are typically *spawned* and
      *managed* through the ~Process~ class.
      * EACH ~Process~ /object/ represents an activity that executes in a *separate*
        /process/.

      * Conveniently,
        the ~Process~ /class/ has *EQUIVALENT* methods and APIs that can be found
        in the ~threading.Thread~ /class/.

    - Specifically, utilizing an object-oriented programming approach, the
      ~Process~ class from ~multiprocessing~ provides the following resources:
      * ~run()~:
        This method is executed
        when a new process is initialized and started

      * ~start()~:
        This method *starts* the initialized calling ~Process~ object
        by calling the ~run()~ method

      * ~join()~:
        This method waits for the calling ~Process~ object to *terminate
        BEFORE* continuing with the execution of the rest of the program

      * ~isAlive()~:
        check whether the calling ~Process~ object is currently executing

      * ~name~:
        the name of the calling ~Process~ object

      * ~pid~:
        the /process ID/ of the calling ~Process~ object

      * ~terminate()~:
        terminates the calling ~Process~ object

*** DONE The ~Pool~ class - 111
    CLOSED: [2021-04-25 Sun 02:41]
    - In the ~multiprocessing~ module, the ~Pool~ class is mainly used to
      implement *a pool of processes,* each of which will carry out tasks
      submitted to a ~Pool~ object.
      * Generally,
        _the ~Pool~ class_ is *more convenient than* _the ~Process~ class_,
        especially
        if the results returned from your concurrent application *should be
        ordered.*

    - About the order:
      * One possible solution to this is to create _TUPLES of /processes/ and their
        outputs_, and to sort them by /process ID/.

      * This problem is addressed by the ~Pool~ /class/:
        + the ~Pool.map()~ and ~Pool.apply()~ methods follow the convention of
          Python's traditional ~map()~ and ~apply()~ methods,
          *ENSURING* that _the returned values are ordered in the same way that
          the input is._

        + These methods, however, *block* the main program until a process has
          finished processing. The ~Pool~ class, therefore, also has the
          ~map_async()~ and ~apply_async()~ functions to better assist
          concurrency and parallelism.

*** TODO Determining the current process, waiting, and terminating processes - 112
**** TODO Determining the current process - 112
**** TODO Waiting for processes - 115
**** TODO Terminating processes - 118

** DONE Interprocess communication - 118 - =NOTE=
   CLOSED: [2021-04-26 Mon 03:32]
   While /locks/ are one of _the MOST COMMON synchronization primitives_ that are
   used for communication among /threads/,
   /pipes/ and /queues/ are the main way of communicating between different
   /processes/.

   - Specifically, /pipes/ and /queues/ provide *message-passing* options to
     facilitate communication between processes:
     * /pipes/ for connections between *two* /processes/
     * /queues/ for *multiple* /producers/ and /consumers/.

   - In this section, we will be exploring the usage of /queues/, specifically the
     ~Queue~ class from the ~multiprocessing~ module.
     * =from Jian=
       WHY NOT ~queue.Queue~, the one we used in the /thread/ related chapters.

     * The implementation of the ~Queue~ class is, in fact,
       both *thread- and process-safe*, and we have already seen the use of
       queues in Chapter 3, Working with Threads in Python.

       + All pickleable (=from Jian= can I replace this with serializable???)
         objects in Python can be passed through a ~Queue~ object;
         in this section, we will be using /queues/ to *pass messages* back and
         forth between /processes/.
       
   - Using a /message queue/ for interprocess communication
     *is preferred over*
     having /shared resources/
     * Reason:
       + if certain /processes/ *mishandle and corrupt* /shared resources/
         while those resources are being shared, then there will be numerous
         *undesirable* and *unpredictable* consequences.

       + if a /process/ failed to handle its message correctly, other items in the
         queue will *remain intact*.
       
*** DONE Message passing for a single worker - 119 - =NOTE=
    CLOSED: [2021-04-25 Sun 03:33]
*** DONE Message passing between several workers - 121 - =NOTE=
    CLOSED: [2021-04-26 Mon 03:31]
    - Have a structure where there are *several* /processes/ constantly executing
      workers from a /queue/, and if a process finishes executing one worker,
      then it will pick up another.
        To do this, we will be utilizing a /subclass/ of ~Queue~ called
      ~JoinableQueue~, which will provide the additional ~task_done()~ and
      ~join()~ methods, as described in the following list:
      * ~task_done()~:
        Tell the program that the calling ~JoinableQueue~ object is complete

      * ~join()~:
        Block until all items in the calling ~JoinableQueue~ object have been
        processed

    - =from Jian=
      The usage of /poison pill/ is not explained clearly.
      Why can it balance the loads between processes?
      =TODO=
      =TODO=
      =TODO=
      =TODO=
      =TODO=
      =TODO=

** TODO Summary - 127
** TODO Questions - 128
** TODO Further reading - 128

* DONE Chapter 7: Reduction Operators in Processes - 129 - =NOTE=
  CLOSED: [2021-04-26 Mon 11:24]
  - The concept of /reduction operators/ -- in which many or all elements of an
    array are reduced into one single result -- is closely associated with
    concurrent and parallel programming.
    * Specifically, because of the /associative/ and /communicative/ nature of
      the operators, concurrency and parallelism can be applied to greatly
      improve their execution time.

  - This chapter discusses the _theoretical concurrent approach_ to _designing_
    and writing a /reduction operator/ from the perspective of programmers and
    developers.

  - The following topics will be covered in this chapter:
    * The concept of a /reduction operator/ in computer science

    * The *communicative* and *associative* properties of /reduction operators/,
      and therefore the reason why /concurrency/ can be applied

    * HOW TO *identify* problems that are equivalent to a /reduction operator/
      and HOW TO *apply* concurrent programming in such cases

** DONE Technical requirements - 129
   CLOSED: [2021-04-26 Mon 03:38]
** DONE The concept of reduction operators - 130
   CLOSED: [2021-04-26 Mon 03:52]
   - Reduction Operators ::
     take a set or an array of elements and
     perform some form of computation to return only one single result.

*** DONE Properties of a reduction operator - 130
    CLOSED: [2021-04-26 Mon 03:48]
    - An operator is a /reduction operator/
      if it satisfies the following conditions:
      * The operator can *reduce* an array of elements *into one scalar value*.
      * The operator must be /communicative/ and /associative/.

    - For example,
      number addition and multiplication

*** DONE Examples and non-examples - 131
    CLOSED: [2021-04-26 Mon 03:52]
    - Examples:
      * Number addition
      * Number multiplication
      * Logical AND

    - With /reduction operators/, the number of operations doesn't change.
      However, if use parallelism and consider the time overlaps, the time will
      becomes log2 N, rather than N.

    - Non-examples:
      Power of numbers

** TODO Example implementation in Python - 133 - =NOTE=
** DONE Real-life applications of concurrent reduction operators - 138
   CLOSED: [2021-04-26 Mon 03:59]
   Some real-world applications for concurrent bitwise reduction operators include
   the following:
   - /Finite state machines/,
     which commonly take advantage of logic operators while processing logic
     gates.
       /Finite state machines/ can be found in both hardware structures and
     software designs.

   - _Communication across sockets/ports_,
     which typically involves /parity/ and /stop bits/ to check for data errors,
     or flow control algorithms.
       These techniques utilize logic values of individual bytes to process
     information through the use of /logic operators/.

   - /Compression and encryption techniques/,
     which heavily depend on /bitwise algorithms/.

** TODO Summary - 138
** TODO Questions - 139
** TODO Further reading - 139

* TODO Chapter 8: Concurrent Image Processing - 140
** Technical requirements - 140
** Image processing fundamentals - 141
*** Python as an image processing tool - 141
**** Installing OpenCV and NumPy - 142

*** Computer image basics - 143
**** RGB values - 143
**** Pixels and image files - 144
**** Coordinates inside an image - 144

*** OpenCV API - 145
*** Image processing techniques - 147
**** Grayscaling - 148
**** Thresholding - 150

** Applying concurrency to image processing - 155
** Good concurrent image processing practices - 159
*** Choosing the correct way (out of many) - 159
*** Spawning an appropriate number of processes - 162
*** Processing input/output concurrently - 162

** Summary - 162
** Questions - 163
** Further reading - 163

* DONE Chapter 9: Introduction to Asynchronous Programming - 164 - =NOTE=
  CLOSED: [2021-04-30 Fri 03:31]
  - =from Jian=
    /Asynchronous Programming/ is usually used for IMPROVING *responsiveness*.
    _An asynchronous implementation_ *may take more time than*
    _a sequential (or synchronous) implementation_.
    * This chapter examples is a good illustration of this:
      in my computer they take time like
      _4.96 (example1.py - sequential) vs 7.18 (example2.py - asynchronous)_

  - =from Jian=
    I seems *responsiveness* is only use full when an application has
    a server side and a client side -- a better *responsiveness* can give the
    clients a good experience, especially when different clients send requests,
    that have differnt calculation load, to a server.

  - The following topics will be covered in this chapter:
    * The *CONCEPT* of /asynchronous programming/
    * The key *DIFFERENCES* between
      /asynchronous programming/ and _other programming models_

** DONE Technical requirements - 164
   CLOSED: [2021-04-30 Fri 03:34]
** TODO A quick analogy - 165
** TODO Asynchronous versus other programming models - 166
*** TODO Asynchronous versus synchronous programming - 167
*** TODO Asynchronous versus threading and multiprocessing - 168

** TODO An example in Python - 169
** TODO Summary - 172
** TODO Questions - 172
** TODO Further reading - 173

* TODO Chapter 10: Implementing Asynchronous Programming in Python - 174
** TODO Technical requirements - 174
** TODO The ~asyncio~ module - 175
*** TODO Coroutines, event loops, and futures - 175
*** TODO Asyncio API - 177

** TODO The ~asyncio~ framework in action - 178
*** TODO Asynchronously counting down - 179
*** TODO A note about blocking functions - 183
*** TODO Asynchronous prime-checking - 184
*** TODO Improvements from Python 3.7 - 188
*** TODO Inherently blocking tasks - 189

** TODO ~concurrent.futures~ as a solution for blocking tasks - 190
*** TODO Changes in the framework - 191
*** TODO Examples in Python - 191

** TODO Summary - 195
** TODO Questions - 196
** TODO Further reading - 197

* TODO Chapter 11: Building Communication Channels with ~asyncio~ - 198
** TODO Technical requirements - 199
** TODO The ecosystem of communication channels - 199
*** TODO Communication protocol layers - 199
*** TODO Asynchronous programming for communication channels - 201
*** TODO Transports and protocols in ~asyncio~ - 202
*** TODO The big picture of ~asyncio~'s server client - 204

** TODO Python example - 205
*** TODO Starting a server - 205
*** TODO Installing Telnet - 207
*** TODO Simulating a connection channel - 208
*** TODO Sending messages back to clients - 209
*** TODO Closing the transports - 210

** TODO Client-side communication with ~aiohttp~ - 212
*** TODO Installing ~aiohttp~ and ~aiofiles~ - 213
*** TODO Fetching a website's HTML code - 213
*** TODO Writing files asynchronously - 215

** TODO Summary - 217
** TODO Questions - 218
** TODO Further reading - 218

* DONE Chapter 12: Deadlocks - 219
  CLOSED: [2021-05-06 Thu 02:28]
  The following topics will be covered in this chapter:
  - The idea behind /deadlock/, and
    how to simulate it in Python

  - _Common solutions_ to /deadlock/, and
    how to implement them in Python

  - The concept of /livelock/, and
    its _connection_ to /deadlock/

** DONE Technical requirements - 219
   CLOSED: [2021-04-26 Mon 11:30]
** DONE The concept of deadlock - 220
   CLOSED: [2021-04-27 Tue 00:57]
*** DONE The Dining Philosophers problem - 220
    CLOSED: [2021-04-27 Tue 00:54]
    - Now, a potential approach to this problem would be the following set of
      instructions:
      1. A philosopher must think until the fork on their left side becomes
         available. When that happens, the philosopher is to pick it up.
      2. A philosopher must think until the fork on their right side becomes
         available. When that happens, the philosopher is to pick it up.
      3. If a philosopher is holding two forks, they will eat a specific amount
         of food from the bowl in front of them, and then the following will
         apply:
         * Afterwards, the philosopher has to put the right fork down in its
           original place
         * Afterwards, the philosopher has to put the left fork down in its
           original place
      4. The process repeats from the first bullet point.

    - This approach leads to /deadlock/.

*** DONE Deadlock in a concurrent system - 223
    CLOSED: [2021-04-27 Tue 00:54]
    - Coffman conditions (deadlock conditions):
      * Mutual exclusion (with non-shareable resource hold) ::
        + /Mutual exclusion/ is normal, but if one individual process (or thread)
          holds a non-shareable resource permantly, and other processes (or
          threads) also need this non-shareable resource, the work can't be done
          becuase of the /mutual exclusion/ of this resource,

      * Hold and wait ::
        + The process (or thread) _A_ is holding the resource _a_, and
          waiting for resource _b_.

        + The process (or thread) _B_ is holding the resource _b_, and
          waiting for resource _a_.

      * No preemption condition ::
        Resources can only be released by a process (or a thread) holding them if
        there are *specific instructions* for the process (or thread) to do so.
        This is to say that unless the process (or thread) voluntarily and
        actively releases the resource, that resource remains in a non-shareable
        state.

      * circular wait ::
        Simutaneously, 
        + The process (or thread) _A_ is waiting for resource _a_,
          which is being held by the process (or thread) _B_.

        + The process (or thread) _B_ is waiting for resource _b_,
          which is being held by the process (or thread) _C_.

        + ...

        + The process (or thread) _X_ is waiting for resource _x_,
          which is being held by the process (or thread) _A_.

*** DONE Python simulation - 224
    CLOSED: [2021-04-27 Tue 00:54]
    =chapter12/example1.py=

** DONE Approaches to deadlock situations - 228
   CLOSED: [2021-05-06 Thu 02:28]
   We will be discussing potential approaches to *prevent* /deadlocks/ from
   occurring.
     Intuitively, *EACH* approach looks to eliminate *one of the four* /Coffman
   conditions/ from our program, in order to *prevent* /deadlocks/.

*** DONE Implementing ranking among resources - 228
    CLOSED: [2021-05-06 Thu 01:56]
    From both the _Dining Philosophers problem_ and _our Python example_,
    we can see that the *last condition* of the /four Coffman conditions/,
    /circular wait/, is at the heart of the problem of /deadlock/.

    - the root cause for this condition is
      _the *order* (or lack thereof) in which the /processes (or threads)/
      access the resources._

    - _CONCLUSION_:
      instead of accessing the resources arbitrarily,
      the /processes (or threads)/ were to access them in _a PREDETERMINED,
      STATIC *order*,_
      the circular nature of the way that they acquire and wait for the
      resources will be ELIMINATED.

    - Examples (resolve the /deadlock/ issues):
      * The fix is in =Chapter12/example2.py= - page 228 ~ 230
        This example include two /threads/ and two /locks/, and it is easy to
        enforce an order on the locks.

      * The fix to the code of _philosopher problem_ is more complicated.
        The first try =Chapter12/example3.py= - page 230 ~ 231 fail.
        + We need to find a way to order the /locks/ in this non-trival context
          (locks are placed in a circle, or some other more complicated context
          in other questions):
          Use the ~id~ built-in function.
          #+NAME: Chapter12/example4.py
          #+begin_src python
            import threading


            class acquire(object):
                def __init__(self, *locks):
                    self.locks = sorted(locks, key=lambda x: id(x))

                def __enter__(self):
                    for lock in self.locks:
                        lock.acquire()

                def __exit__(self, ty, val, tb):
                    for lock in reversed(self.locks):
                        lock.release()
                    return False


            # The philosopher thread
            def philosopher(left, right):
                while True:
                    with acquire(left, right):
                        print(f"Philosopher at {threading.currentThread()} is eating.")


            # The chopsticks
            N_FORKS = 5
            forks = [threading.Lock() for n in range(N_FORKS)]

            # Create all of the philosophers
            phils = [threading.Thread(
                target=philosopher,
                args=(forks[n], forks[(n + 1) % N_FORKS])
            ) for n in range(N_FORKS)]

            # Run all of the philosophers
            for p in phils:
                p.start()
          #+end_src

    - Examples (check the efficiency of the no-deadlock code examples):
      * The TWO /threads/ and TWO /locks/:
        The TWO /locks/ are enough to make the code essentially sequential.
        + We should write this example in concurrent code.
          This _essentially sequential_ is *slower* than the no-lock sequential
          code because of the *overhead* from the concurrent programming
          functionalities.

      * The _Dining Philosophers problem_:
        However, we did not see in the _Dining Philosophers problem_ (simulated
        in Python) this /sequentiality/ created by /locks/.
        + FIVE /locks/ were *not enough* to do the same as in _the TWO /threads/
          and TWO /locks/ example_ for the _Dining Philosophers problem_.
          =WHY= =WHY= =WHY= =???= =TODO=

    - =TODO=
      We will explore another instance of this phenomenon
      in _Chapter 14, Race Conditions._

*** DONE Ignoring locks and sharing resources - 234
    CLOSED: [2021-05-06 Thu 02:28]
    - /Locks/ are undoubtedly an important tool in /synchronization tasks/, and
      in concurrent programming in general.

      1. However, if the use of /locks/ leads to an undesirable situation, such
         as a /deadlock/, then it is quite natural for us to _explore the option
         of *simply not using /locks/* in our concurrent programs._

      2. By ignoring /locks/,
         our program's resources effectively become shareable among different
         _processes/threads_ in a concurrent program, thus eliminating the first
         of the four Coffman conditions: /mutual exclusion/.

    - *Simply remove all locks* is good if the resource are _shared and immutable
      (or read only)._ There is no /mutual exclusion/ limitation.
      * Do this in our _two threads two locks problem_ is good, and this change
        can make it run more efficiently.

      * We can't remove all locks without adding other synchronization control
        for the _Dining Philosophers problem_. The forks in this problem is not
        sharable, and there must be synchronization.
        
    - So, *by ignoring locks*, it is relatively likely that we will need to
      *completely redesign and restructure* our concurrent program.

      * If the shared resources still need to be accessed and manipulated in an
        organized way, other synchronization methods will need to be used.

      * The result of altering the synchronization methods:
        + DIFFERENT synchronization methods need DIFFERENT implementation logic;
        + the execution time might be *negatively affected* by this change;
        + *other* potential synchronization problems might also arise.

**** DONE An additional note about locks - 236
     CLOSED: [2021-05-06 Thu 02:20]
     - Remove locks completely in a piece of code is applicable in some context.

     - /Locks/, then, do not actually lock anything -- they are simply *flags*
       that help to indicate whether a resource should be accessed at a given
       time;
       * if a poorly instructed, or even malicious, _process/thread_ attempts to
         access that resource without checking the lock object exists, it will
         most likely be able to do that without difficulty.

       * In other words, *locks are NOT AT ALL CONNECTED to the resources that
         they are supposed to lock,* and they most certainly *do not block*
         processes/threads from accessing those resources.

     - _The simple use of /locks/_ is therefore *inefficient* to design and
       implement a secure, dynamic, concurrent data structure.
       * To achieve that, we would need to
         + either
           *add more concrete links* between the locks and their corresponding
           resources,

         + or
           utilize a _DIFFERENT synchronization tool_ altogether (for example,
           /atomic message queues/).

**** DONE Concluding note on deadlock solutions - 237
     CLOSED: [2021-05-06 Thu 02:28]
     - You have seen *TWO of the most common approaches* to the problem of
       /deadlock/.
       * *Each ADDRESSES* one of the _FOUR Coffman conditions_, and, while both
         (somewhat) successfully prevent /deadlocks/ from occurring in our
         examples,

       * *Each RAISES* different, additional problems and concerns.
           It is therefore important to truly understand the nature of your
         concurrent programs, in order to know which of the two is applicable,
         if either of them are.

     - Not all problems are suitable to be made concurrent:
       * some programs are better left sequential, and will be made worse with
         forced concurrency.

     - In situations of /deadlock/,
       you can try to resolve the /deadlocks/, or you may need to use other
       mechanism to do synchronization.
         Try to also consider the latter one -- change the mechanism, rather than
       put all your effort on resolving the exising /deadlocks/.

** DONE The concept of livelock - 237
   CLOSED: [2021-05-01 Sat 03:06]
   The concept of /livelock/ is connected to /deadlock/;
   some even consider it _an alternate version_ of /deadlock/.

   - livelock :: the /processes/ (or /threads/) in the concurrent program are
     *able* to _switch their states_;
     in fact, they _switch states_ *constantly*.
       Yet, *they simply switch back and forth infinitely,* and no progress is
     made.

   - We will now consider an actual scenario of /livelock/:
     A pair of spouses are eating dinner together, and only one fork is
     available.
     * the spouses are really polite to each other, so even if one spouse is
       hungry and wants to eat their food, they will leave the fork on the table
       if their partner is also hungry.

   - Example:
     #+Name: chapter12/example8.py
     #+begin_src python
       import threading

       import time


       fork = threading.Lock()


       class Spouse(threading.Thread):

           def __init__(self, name, partner):
               threading.Thread.__init__(self)
               self.name = name
               self.partner = partner
               self.hungry = True

           def run(self):
               while self.hungry:
                   print(f"{self.name} is hungry and wants to eat.")

                   if self.partner.hungry:
                       print(f"{self.name} is waiting for their partner to eat first...")
                   else:
                       with fork:
                           print(f"{self.name} has stared eating.")
                           time.sleep(5)

                           print(f"{self.name} is now full.")
                           self.hungry = False


       def main():
           partner1 = Spouse('Wife', None)
           partner2 = Spouse('Husband', partner1)
           partner1.partner = partner2

           partner1.start()
           partner2.start()

           partner1.join()
           partner2.join()

           print('Finished.')


       if __name__ == '__main__':
           main()
     #+end_src

** TODO Summary - 240
** TODO Questions - 240
** TODO Further reading - 240

* DONE Chapter 13: Starvation - 241
  CLOSED: [2021-05-10 Mon 01:36]
  - The following topics will be covered in this chapter:
    * The basic idea behind /starvation/,
      its root causes, and
      some more relevant concepts

    * A detailed analysis of *the readers-writers problem*, which is used to
      illustrate the complexity of /starvation/ in a concurrent system

** DONE Technical requirements - 241
   CLOSED: [2021-05-06 Thu 10:00]
** TODO The concept of starvation - 242 - =TODO: NOTE=
   - Starvation ::
     a problem in concurrent systems, in which a process (or a thread) cannot
     gain access to the necessary resources in order to proceed with its
     execution and, therefore, cannot make any progress.

*** DONE What is starvation? - 242
    CLOSED: [2021-05-08 Sat 00:56]
    - It is quite common for a concurrent program to implement some sort of
      *ordering* between the _DIFFERENT_ /processes/ in its execution.
      * For example,
        consider a program that has *THREE* separate /processes/, as follows:
        + One /process/ is responsible for handling extremely pressing instructions
          that need to be run as soon as the necessary resources become available

        + Another /process/ is responsible for other important executions, which
          are *NOT* as essential as the tasks in the first /process/

        + The last /process/ handles miscellaneous, very infrequent tasks
      
      * Furthermore, these *THREE* /process/ need to *utilize the same resources*
        in order to execute their respective instructions.

    - _Intuitively_,
      we have every reason to implement a specification that allows
      implement a specification that allows the first process to have the
      highest priority of execution and access to resources, then the second
      process, and then the last process, with the lowest priority.
      _However_,
      in this design the third process has possibility that it can (almost) never
      get the required resources -- higher priority processes keeps using them.
      
    - This is a situation of /starvation/:
      the third /process/ is given *no opportunity* to execute and, therefore,
      *no progress* can be made with that /process/.

*** TODO Scheduling - 243
    - In the next few subsections, we will be discussing the potential candidates
      that cause starvation situations. Most of the time, a poorly coordinated
      set of scheduling instructions is the main cause of starvation. For
      example, a considerably naive algorithm that deals with three separate
      tasks might implement constant communication and interaction between the
      first two tasks.

    - This setup leads to the fact that the execution flow of the algorithm switches
      solely between the first and second tasks, while the third finds itself
      idle and unable to make any progress with its execution; in this case,
      because it is starved of CPU execution flow. Intuitively, we can identify
      the root of the problem as the fact that the algorithm allows the first
      two tasks to always dominate the CPU, and hence, effectively prevents any
      other task to also utilize the CPU. A characteristic of a good scheduling
      algorithm is the ability to distribute the execution flow and allocate the
      resources equally and appropriately.

    - As mentioned previously, many concurrent systems and programs implement a
      specific order of priority, in terms of process and thread execution. This
      implementation of ordered scheduling may very likely lead to the
      starvation of processes and threads of lower priorities and can result in
      a condition called priority inversion.

    - Suppose that, in your concurrent program, you have process A of the highest
      priority, process B of a medium priority, and finally, process C of the
      lowest priority; process C would most likely be put in the situation of
      starvation. Additionally, if the execution of process A, the prioritized
      process, is dependent on the completion of process C, which is already in
      starvation, then process A might never be able to complete its execution,
      either, even though it is given the highest priority in the concurrent
      program.

*** TODO Causes of starvation - 244
*** TODO Starvation's relationship to deadlock - 245
    
** DONE The readers-writers problem - 246
   CLOSED: [2021-05-09 Sun 22:11]
   The _readers-writers problem_ is one of _the CLASSIC and MOST COMPLEX examples_
   in the field of computer science, illustrating problems that might occur in a
   concurrent program.

*** DONE Problem statement - 246
    CLOSED: [2021-05-08 Sat 09:07]
    - Setup:
      1. A shared resource
      2. Several /threads/ that interact with this shared resource, and each
         /thread/ is
         * either a /reader/
         * or a /reader/
         
    - Reader :: access the shared resource without modifying it.
    - Writer :: access the shared resource, and possibly mutates it.
         
    - Goal:
      FIND a correct and efficient way to design and coordinate the scheduling of
      these /reader and writer threads/.
      * A successful implementation:
        + Not only the execution is in the most optimized way,
        + But also *all* /threads/ are given sufficient opportunity to execute
          their instructions and _no /starvation/ can occur._
      
*** DONE The first readers-writers problem - 247
    CLOSED: [2021-05-08 Sat 11:17]
    - Naive solution:
      Each reader or writer need to acquire the lock before accessing a resource.
      * _Result_:
        This is essentially a sequential implementation.

    - Optimal solution:
      Use _resource lock_ and _reader counter (and reader counter lock)_ to schedule
      the exectuion.
      
      * _Result_:
        The _reader accesses_ to the resource is efficient.
        However, the /writers/ are experiencing /starvation/, as none of them
        are able to access and use the resource.

    - Conclusion:
      This scheduling algorithm inadvertently _gives priority to the /readers/ over
      the /writers/,_ and is therefore called *readers-preference*.
      *So, this design is UNDESIRABLE.*
      
*** DONE The second readers-writers problem - 251
    CLOSED: [2021-05-08 Sat 18:29]
    The first solution make the /writers/ in /starvation/ status.

    - Solution:
      To address this problem, we will implement the specification that,
      _once a /writer/ makes a request to access the file, *no* /reader/ should
      be able to jump in line and access the file before that /writer/._

    - _RESULT_:
      * Writers-preference
      * Readers starvation

    - Analysis:
      The _priority_ that /writers/ were given over /readers/ resulted from the
      fact that,
      * while _only the first and the last_ /writers/ have to *acquire* and
        *release* _the ~read_try~ lock_, respectively,

      * *each and every* /reader/ wanting to access the text file have to interact
        with that lock object _INDIVIDUALLY_.

      * CONCLUSION:
        Once ~read_try~ is locked by a /writer/, no /reader/ can even attempt to
        execute its instructions, let alone try to access the text file.

        1. There are cases in which some /readers/ are able to gain access to the
           text file, if the /readers/ are initialized and executed *before* the
           /writers/ (for example, in our program, the /readers/ were the first
           three elements, and the /writers/ were the last two, in our list of
           threads).

        2. However, once a /writer/ is able to access the file and *acquire* _the
           ~read_try~ lock_ during its execution, /starvation/ will most likely
           occur for the /readers/.
    
*** DONE The third readers-writers problem - 254
    CLOSED: [2021-05-09 Sun 22:10]
    Both of the solutions that we tried to implement can result in /starvation/,
    by not giving equal priorities to the separate /threads/;
    one can *starve* the /writers/, and
    the other can *starve* the /readers/.

    - In the _SECOND approach_,
      we are placing a /lock/ on _reader's attempt_, requiring that no /writer/ be
      starved once it starts waiting for the file.
      _This result in /reader starvation/._
      * However,
        we can adjust it to eliminate the possibility of both /starvation/
        (*Solution*):
          Apply a lock to both /readers/ and /writers/ -- all of the /treads/ will
        then be subjected to the constraints of the lock, and *equal priority* will
        hence be achieved among the separate /threads/.

    - We call the lock mentioned in the above *Solution* the /service lock/.
      * A /writer/, having *obtained* this /service lock/, will also *attempt to
        obtain* the /resource lock/ and *release* the /service lock/ _IMMEDIATELY_
        thereafter.
          The /writer/ will then execute its writing logic and finally *release*
        the /resource lock/ at the end of its execution.

      * A /reader/, on the other hand, will also need to *acquire* the /service
        lock/ *first*. Since we are still allowing _multiple_ /readers/ to
        access the resource at the same time, we are implementing the /reader
        counter and its corresponding lock/ as we did in the first two solutions.

** DONE Solutions to starvation - 256
   CLOSED: [2021-05-10 Mon 01:30]
   - Through an analysis of different approaches to the readers-writers problem,
     you have seen *the key to SOLVING /starvation/:*
     since some /threads/ will be starved if they are not given a high priority in
     accessing the shared resources, _implementing *fairness* in the execution of
     ALL of the /threads/ will prevent starvation from occurring._

     * Fairness, in this case,
       + *does not require* a program to forgo any /order/ or /priority/ that it
         has imposed on the different /threads/;
       + BUT _to implement fairness,_
         a program needs to ensure that ALL /threads/ are given *sufficient*
         opportunities to execute their instructions.

   - Keeping this idea in mind, we can potentially address the problem of
     /starvation/ by implementing *one (or a combination)* of the following
     approaches:
     * *Increasing the priority of low-priority threads:*
       As we did with the /writer threads/ in the _2nd approach_ and the /reader
       threads/ in the _3rd approach_ to the readers-writers problem,
       prioritizing the /threads/ that would otherwise not have any opportunity
       to access the shared resource can successfully *eliminate* /starvation/.

     * *FIFO thread queue:*
       To ensure that a /thread/ that started waiting for the _shared resource_
       *before* another /thread/ will be able to acquire the resource *before*
       the other /thread/, we can keep track of the /threads/ requesting access
       in a _FIFO queue._

     * *Other methods:*
       Several methods can also be implemented to *balance the selection
       frequency* of different /threads/.
       + For example,
         - a /priority queue/ that also gives gradually increasing priority to
           /threads/ that have been waiting in the /queue/ for a long time, or

         - if a /thread/ has been able to access the shared resource for _many
           times_, it will be given _less priority_, and so on.

   - Solving /starvation/ in your concurrent program can be a rather *complex* and
     involved process, and a *deep understanding of its scheduling algorithm*,
     combined with an understanding of how /processes/ and /threads/ interact with
     the /shared resources/, is necessary during the process.
       As you saw in the example of the readers-writers problem, it can also
     _take several implementations and revisions of different approaches to
     arrive at a good solution to /starvation/._
       
** DONE Summary - 257
   CLOSED: [2021-05-10 Mon 01:36]
   - /Starvation/ is a problem in concurrent systems in which a /process (or thread)/
     *cannot gain access* to the necessary resources to proceed with its execution
     and, *therefore, cannot make any progress.*
     * Most of the time,
       - a *poorly coordinated set of scheduling* instructions is the MAIN CAUSE
         of /starvation/;
         =from Jian= this kind of /starvation/ happens in a dynamic process.

       - /deadlock/ situations can also lead to /starvation/.
         =from Jian= this kind of /starvation/ happens in a non-dynamic process.

   - */Fairness/ is an essential element of a good scheduling algorithm,* and,
     * by making sure that the priority is distributed appropriately among
       different /processes/ and /threads/, /starvation/ can be eliminated.

** TODO Questions - 258
** TODO Further reading - 258

* TODO Chapter 14: Race Conditions - 259
  - The following topics will be covered in this chapter:
    * The basic concept of a /race condition/, and
      how it occurs in concurrent applications,
        along with the definition of /critical sections/

    * A simulation of a /race condition/ in Python and
      how to _implement /race condition/ solutions_

    * The real-life computer science concepts that commonly interact and work with
      /race conditions/

** DONE Technical requirements - 259
   CLOSED: [2021-05-10 Mon 03:14]
** DONE The concept of race conditions - 260
   CLOSED: [2021-05-11 Tue 01:28]
   - Race conditon :: a phenomenon during which the output of a system is
     *indeterminate* and
     *dependent on the scheduling algorithm* and
     *the order* in which tasks are scheduled and executed.

   - When the data becomes mishandled and corrupted during this process, a /race
     condition/ *becomes* a _bug_ in the system.
     * Given the nature of this problem, it is quite common for a /race condition/
       to occur in concurrent systems, which emphasize scheduling and
       coordinating independent tasks.

     * CAUTION: =from Jian=
       The *becomes* in the last paragraph is IMPORTANT!!!
       /Race conditions/ are not always bugs.
       + For example,
         if we only want to know the _rough lower limit_ of a sequence of
         accumulation operations of non-negative numbers, we can *ignore* /race
         conditions handling/.

   - Race conditon can occur in _BOTH an electronic hardware system AND a software
     application_. We only discuss the race condition in the context of software
     development -- specifically, concurrent software applications.

   - This section will cover
     * the _theoretical foundations_ of /race conditions/
       and
     * their *root causes*
       and
       the concept of /critical sections/.
     
*** DONE Critical sections - 260
    CLOSED: [2021-05-11 Tue 00:52]
    - /Critical sections/ indicate /shared resources/ that are accessed by multiple
      /processes/ or /threads/ in a concurrent application, which can lead to
      unexpected, and even erroneous, behavior.

      * We have seen that there are multiple methods to protect the integrity of
        the data contained in these resources, and
        *we call these _protected sections_ /critical sections/.*

    - Mutual Exclusion ::
      Disallow multiple agents to go into a /critical section/ at the same time.
      * _Rationale_:
        The data in /critical sections/, when interacted with and altered
        concurrently or in parallel, can become mishandled or corrupted.
          This is especially true when the threads and processes interacting
        with it are poorly coordinated and scheduled.
        Therefore, we need /mutual exclusion/.

    - We will discuss the relationship between /critical sections/ and _the causes
      of /race conditions/_ in the next subsection.

*** DONE How race conditions occur - 261
    CLOSED: [2021-05-11 Tue 01:28]
    - Example:
      Two /threads/ interact (read and write) with one shared resource.
      Change the read/write order, you can see different results.

    - Intuitively, we can see that a /race condition/ can result in the _mishandling
      and corruption of data._

** DONE Simulating race conditions in Python - 263
   CLOSED: [2021-05-11 Tue 01:52]
   - Before we discuss a solution that we can implement to solve the problem of
     /race conditions/, let's try to simulate the problem in Python.
     #+begin_src python
       import random
       import time


       def update():
           global counter

           current_counter = counter         # reading in shared resource
           time.sleep(random.randint(0, 1))  # reading in shared resource
           counter = current_counter + 1     # reading in shared resource
     #+end_src
     * This ~update~ function will be called in different /threads/ or /processes/.

     * We use the ~time.sleep~ to simulate a random heavy calculation, and this
       long enough time wait can improve the possibility of /race conditions/.
       + If we remove it (simulate a /thread/ doesn't spend anytime processing
         the data), the /race condition/ may not happen -- because the updates
         just *in time*.

   - Because of the randomness comes from ~random.randint(0, 1)~, we can see the
     variation of numbers of /race conditions/ happen from the output of this
     program.

** DONE Locks as a solution to race conditions - 265 - =NOTE=
   CLOSED: [2021-05-11 Tue 02:12]
*** TODO The effectiveness of locks - 265
*** TODO Implementation in Python - 267
*** TODO The downside of locks - 268
**** Turning a concurrent program sequential - 269
**** Locks do not lock anything - 271

** TODO Race conditions in real life - 272 - =START= =TODO=
*** TODO Security - 272
*** TODO Operating systems - 273
*** DONE Networking - 274
    CLOSED: [2021-05-11 Tue 02:19]
    Example:
    One system allows certain user (ONLY 1 at given anytime) to be a temporary
    admin. Two eligible to become the server admin users send requests amout
    simutaneously, it is possbile for both of them to gain that admin privilege.

    - Reason:
      When one user tries to get the admin privilege, and gets it.
      Because of the /race condition/, the system doesn't notice this when assign
      admin privilege to the second user, too.
    
    - This form of a /race condition/ is quite _COMMON_
      when a network is highly optimized for parallel processing (for example,
      non-blocking sockets), without a careful consideration of the resources
      shared across the network.
      =TODO= Learn more =TODO=

** TODO Summary - 275
** TODO Questions - 275
** TODO Further reading - 276

* DONE Chapter 15: The Global Interpreter Lock - 277
  CLOSED: [2021-05-10 Mon 03:11]
  - The following topics will be covered in this chapter:
    * A brief introduction to the GIL:
      + what gave rise to it, and
      + the problems it causes

    * Efforts in removing/fixing the GIL in Python

    * How to effectively work with the GIL in Python concurrent programs

** DONE Technical requirements - 277
   CLOSED: [2021-05-10 Mon 01:52]
** DONE An introduction to the Global Interpreter Lock - 278 - =NOTE=
   CLOSED: [2021-05-10 Mon 02:51]
   - Global Interpreter Lock (GIL) ::
     Designed as a /lock/ that will _only allow_ one /thread/ to access and
     control the Python interpreter at any given time, the GIL in Python is
     often known as the infamous GIL that prevents multithreaded programs from
     reaching their fully optimized speed.

   - In this section, we will discuss _the concept behind the GIL_, and _its goals_:
     * _WHY_ it was designed and implemented, and
     * _HOW_ it affected multithreaded programming in Python.

*** DONE An analysis of memory management in Python - 278
    CLOSED: [2021-05-10 Mon 02:06]
    - Compare the memory management about variables and values in C++ and Python.
      
    - =chapter15/example1.py= is used to show
      variables, the values these variables refer, and the assignments in Python.
      * The tool we use to check the count of references: ~sys.getrefcount(variable)~.
      
*** TODO The problem that the GIL addresses - 281 - =NOTE=
*** DONE Problems raised by the GIL - 282
    CLOSED: [2021-05-10 Mon 02:50]
    - =chapter15/example2.py= shows that, because of GIL,
      when running /CPU-bound/ tasks,
      _sequential program_ and _multithreading program_ need similar time.
      * Actually, because threads need to acquire GIL before proceed,
        the _multithreading program_ has more overhead, though it might be not
        significant in this example.

    - Instructions that are *not* /CPU-bound/ happen *outside* the /GIL/, and thus,
      they are *not affected by the GIL* (for example, _I/O-bound_ instructions).
      * =from Jian=
        What are _I/O-bound instructions_???

** DONE The potential removal of the GIL from Python - 284
   CLOSED: [2021-05-10 Mon 03:01]
   In fact, multiple attempts to remove the GIL have been made by prominent Python users. 
   However, no exciting result was achieved.

   - Since the most libraries and packages that are *not thread-safe* is
     _SO_ significantly dependent on the GIL,
     _THAT_ the removal of the GIL will actually engender bugs as well as backward
     incompatibility issues for your Python programs.

   - Now there are other viable solutions to address the problems that we have
     discussed; in other words, the GIL is in every way replaceable.
     _HOWEVER_, most of these solutions contain so many complex instructions
     that they actually *decrease the performance of sequential and I/O-bound
     programs,* _which are not affected by the GIL._
       So, these solutions will slow down single-threaded or multithreaded I/O
     programs, *which actually make up a large percentage of existing Python
     applications.*
     * =from Jian=
       The mainstream of Python code is still sequential code.
       It's performance is critical to the real world Python application,
       *not* the concurrent code performance.

     * Interestingly, the creator of Python, Guido van Rossum, also commented on
       this topic in his article, *It isn't Easy to Remove the GIL*:
       #+begin_quote
       I'd welcome a set of patches into Py3k only if the performance for a single-threaded
       program (and for a multi-threaded but I/O-bound program) does not decrease.
                                                                       -- Guido van Rossum
       #+end_quote
       _This request has *not* been achieved by any of the proposed alternatives to the GIL._

** DONE How to work with the GIL - 284
   CLOSED: [2021-05-10 Mon 03:11]
*** TODO Implementing multiprocessing, rather than multithreading - 285
    - /Processes/ executing over multiple cores of a system,
      each having _its own memory space,_
      are completely immune to the GIL.

    - Check the example =chapter15/example3.py=, you can see the multiprocessing code can
      cut the execution time.
      * However, since /processes/ are *fairly heavy weight,* multiprocessing
        instructions contain *significant overhead*, which is the reason why the
        speed of the multiprocessing program *was not exactly half* of the
        sequential program.
      
*** DONE Getting around the GIL with native extensions - 287
    CLOSED: [2021-05-10 Mon 03:03]
    - One popular and widely used example:
      _NumPy_.

    - _WITHIN these extensions,_
      *manual releases of the GIL* can be made,
      so that the execution can simply bypass the lock.
      * However, these releases need to be implemented carefully and accompanied
        by the reassertion of the GIL *before the execution goes back to* _the main
        Python execution._

*** DONE Utilizing a different Python interpreter - 287
    CLOSED: [2021-05-10 Mon 03:05]
    The GIL only exists in CPython.

    - Other Python implementations, like Jython and IronPython, though not very
      popular, don't have the GIL limitation.
      * _CAUTION:_
        some packages and libraries might not be compatible with one or both of them.

** TODO Summary - 287
** TODO Questions - 288
** TODO Further reading - 288

* TODO Chapter 16: Designing Lock-Based and Mutex-Free Concurrent Data Structures - 289
** TODO Technical requirements - 290
** TODO Lock-based concurrent data structures in Python - 290
*** TODO LocklessCounter and race conditions - 290
*** TODO Embedding locks in the data structure of the counter - 293
*** TODO The concept of scalability - 295
*** TODO Analysis of the scalability of the counter data structure - 297
*** TODO Approximate counters as a solution for scalability - 300
**** The idea behind approximate counters - 300
**** Implementing approximate counters in Python - 302
**** A few considerations for approximate counter designs - 308

** TODO Mutex-free concurrent data structures in Python - 308
*** TODO The impossibility of being lock-free in Python - 309
*** TODO Introduction to the network data structure - 310
*** TODO Implementing a simple network data structure in Python and race conditions - 311
*** TODO RCU as a solution - 315

** TODO Building on simple data structures - 318
** TODO Summary - 319
** TODO Questions - 319
** TODO Further reading - 320

* TODO Chapter 17: Memory Models and Operations on Atomic Types - 321
** TODO Technical requirements - 321
** TODO Python memory model - 322
*** TODO The components of Python memory manager - 322
*** TODO Memory model as a labeled directed graph - 323
*** TODO In the context of concurrency - 325

** TODO Atomic operations in Python - 326
*** TODO What does it mean to be atomic? - 326
*** TODO The GIL reconsidered - 327
*** TODO Innate atomicity in Python - 328
**** Atomic versus nonatomic - 328
**** Simulation in Python - 329

** TODO Summary - 332
** TODO Questions - 332
** TODO Further reading - 333

* TODO Chapter 18: Building a Server from Scratch - 334
** TODO Technical requirements - 334
** TODO Low-level network programming via the ~socket~ module - 335
*** The theory of server-side communication - 335
*** The API of the ~socket~ module - 337
*** Building a simple echo server - 338

** TODO Building a calculator server with the ~socket~ module - 342
*** The underlying calculation logic - 342
*** Implementing the calculator server - 343

** TODO Building a non-blocking server - 346
*** Analyzing the concurrency of the server - 346
*** Generators in Python - 350
*** Asynchronous generators and the send method - 352
*** Making the server non-blocking - 354

** TODO Summary - 360
** TODO Questions - 360
** TODO Further reading - 361

* TODO Chapter 19: Testing, Debugging, and Scheduling Concurrent Applications - 362
** TODO Technical requirements - 362
** TODO Scheduling with APScheduler - 363
*** TODO Installing APScheduler - 363
*** TODO Not a scheduling service - 364
*** TODO APScheduler functionalities - 364
*** TODO APScheduler API - 366
**** Scheduler classes - 366
**** Executor classes - 366
**** Trigger keywords - 367
**** Common scheduler methods - 367

*** TODO Examples in Python - 368
**** Blocking scheduler - 368
**** Background scheduler - 369
**** Executor pool - 370
**** Running on the cloud - 372

** TODO Testing and concurrency in Python - 373
*** TODO Testing concurrent programs - 374
**** Unit testing - 374
**** Static code analysis - 376

*** TODO Testing programs concurrently - 376

** TODO Debugging concurrent programs - 380
*** TODO Debugging tools and techniques - 380
*** TODO Debugging and concurrency - 381

** TODO Summary - 382
** TODO Questions - 383
** TODO Further reading - 384

* TODO Assessments - 385
** Chapter 1 - 385
** Chapter 2 - 386
** Chapter 3 - 387
** Chapter 4 - 389
** Chapter 5 - 390
** Chapter 6 - 391
** Chapter 7 - 393
** Chapter 8 - 394
** Chapter 9 - 395
** Chapter 10 - 395
** Chapter 11 - 397
** Chapter 12 - 398
** Chapter 13 - 400
** Chapter 14 - 402
** Chapter 15 - 403
** Chapter 16 - 403
** Chapter 17 - 405
** Chapter 18 - 406
** Chapter 19 - 408
