#+TITLE: Python Testing with pytest
#+SUBTITLE: Simple, Rapid, Effective, and Scalable
#+VERSION: 2nd
#+AUTHOR: Brian Okken
#+STARTUP: entitiespretty
#+STARTUP: indent
#+STARTUP: overview

* Acknowledgments - xi
* Preface - xiii
* Part I — Primary Power
** DONE 1. Getting Started with pytest - 3
CLOSED: [2025-01-06 Mon 14:49]
A test example:
#+file_name: ch1/test_one.py
#+begin_src python
  def test_passing():
      assert (1, 2, 3) == (1, 2, 3)
#+end_src
- _pytest_ can discover this ~test_passing~ because
  * it starts with ~test_~
  * it is in a file that starts with =test_=.

- Any /uncaught exception/ raised _within a test_ will cause the test to fail.
  * Although any type of /uncaught exception/ can cause a test to fail,
    _TRADITIONALLY we stick with ~AssertionError~ from assert to determine
    pass/fail for tests._

*** DONE Installing pytest - 3
CLOSED: [2025-01-06 Mon 14:40]
Use _pip_ to install _pytest_ in your virtual environment.

*** DONE Running pytest - 4
CLOSED: [2025-01-06 Mon 14:40]
- To run _pytest_,
  * You have the option to specify files and directories.
  * If you don't specify any files or directories, _pytest_ will look for tests
    in the _current working directory and subdirectories_. It looks for =.py=
    files starting with ~test_~ or ending with ~_test~.

- Run all tests in =ch1=. Turn off tracebacks, because we don't need the full
  output right now: ~pytest --tb=no~.

- Other ways to run with ~pytest~:
  * Inside the folder =ch1=: ~pytest --tb=no test_one.py test_two.py~
  * One level up from =ch1=: ~pytest --tb=no ch1~
  * Specify a test function within a test file to run by adding ~::test_name~:
    ~pytest -v ch1/test_one.py::test_passing~

**** Test Discovery - 7
Here's a brief overview of the /naming conventions/ to keep your test code
discoverable by _pytest_:
• /Test files/ should be named ~test_<something>.py~ or ~<something>_test.py~.
• /Test methods and functions/ should be named ~test_<something>~.
• /Test classes/ should be named ~Test<Something>~.

Because our _test files and functions_ start with ~test_~, we're good. There are
ways to *ALTER* these discovery rules if you have a bunch of tests named
differently.
=TODO=
=TODO=
=TODO=
I'll cover how to do that in Chapter 8, Configuration Files, on page 113

**** Test Outcomes - 8
_Pass_ and _fail_ are NOT the only outcomes possible.

Here are the possible outcomes of a test:
- *PASSED* (~.~) - The test ran successfully.

- *FAILED* (~F~) - The test did not run successfully.

- *SKIPPED* (~s~) - The test was skipped.
  You can tell pytest to skip a test by using either the ~@pytest.mark.skip()~
  or ~@pytest.mark.skipif()~ decorators, which are discussed in
  =TODO= Skipping Tests with ~pytest.mark.skip~, on page 74.

- *XFAIL* (~x~) - The test was not supposed to pass, and it ran and failed. You
  can tell pytest that a test is expected to fail by using the
  ~@pytest.mark.xfail()~ decorator, which is discussed in =TODO= Expecting Tests
  to Fail with ~pytest.mark.xfail~, on page 77.

- *XPASS* (~X~) - The test was marked with xfail, but it ran and passed.

- *ERROR* (~E~) - An exception happened
  either *during* the execution of a /fixture/ or /hook function/, and
  *NOT during* the execution of a test function.
  * /Fixtures/ are discussed in Chapter 3, pytest Fixtures, on page 31, and
  * /hook functions/ are discussed in Chapter 15, Building Plugins, on page 205.

*** DONE Review - 8
CLOSED: [2025-01-06 Mon 14:42]
*** DONE Exercises - 9
CLOSED: [2025-01-06 Mon 14:48]
*** DONE What's Next - 10
CLOSED: [2025-01-06 Mon 14:48]
- =NEXT=
  In the next chapter, we'll look at
  * writing test functions, and
  * grouping tests into /classes/, /modules/, and /directories/.

** DONE 2. Writing Test Functions - 11
CLOSED: [2025-01-07 Tue 00:14]
We're going to write tests for a simple task-tracking command-line application
called Cards.

- We'll look at

  how to use assert in tests,
  how tests handle unexpected exceptions, and
  how to test for expected exceptions.

- Eventually, we'll have a lot of tests. Therefore,
  we'll look at
  how to *ORGANIZE* tests into
  * /classes/,
  * /modules/, and
  * /directories/.

*** DONE Installing the Sample Application - 11
CLOSED: [2025-01-06 Mon 15:30]
1. Install the example project:
   ~pip install ./cards_proj/~

2. Try this application: =read this secion in book=

*** DONE Writing Knowledge-Building Tests - 13
CLOSED: [2025-01-06 Mon 16:27]
*FIRST* build some tests for fundamental understanding of application code.
Not exhaustive tests, not for corner cases or failure cases.

It is a good start for testing my own understanding, and really of using tests
as little playgrounds to play with the application code.

- Only use old ~assert~ statements in the examples of this subsection.

*** DONE Using assert Statements - 16
CLOSED: [2025-01-06 Mon 16:37]
- _pytest_ includes a feature called _“assert rewriting”_ that
  1. *intercepts* assert calls and
  2. *replaces* them *with* something that can tell you more about why your
     assertions failed.

- Try to run a test inside ~if __name__ == "__main__"~, without using _pytest_,
  you can see the original _assert failure_ can provide enough useful info.

*** DONE Failing with ~pytest.fail()~ and Exceptions - 19
CLOSED: [2025-01-06 Mon 16:42]
- A _test will fail_ IF there is _ANY uncaught exception_. This can happen if
  * an ~assert~ statement fails, which will _raise an ~Assertion;Error~
    exception_,
  * the test code calls ~pytest.fail()~, which will _raise an exception_, or
  * _any other exception is raised_

- In rare cases where ~assert~ is not suitable, use ~pytest.fail()~.
  * This kind of failure can't provide assert rewriting advices from _pytest_.

    However, there are reasonable times to use ~pytest.fail()~, such as in an
    assertion helper. =NEXT= see next subsection for examples.

*** DONE Writing Assertion Helper Functions - 20 - =TODO: See Also=
CLOSED: [2025-01-06 Mon 16:51]
#+file_name: ch2/test_helper.py
#+begin_src python
  from cards import card
  import pytest


  def assert_identical(c1: Card, c2: Card):
      __tracebackhide__ = True
      assert c1 == c2
      if c1.id != c2.id:
          pytest.fail(f"id's don't match. {c1.id} != {c2.id}")


  def test_identical():
      c1 = Card("foo", id=123)
      c2 = Card("foo", id=123)
      assert_identical(c1, c2)


  def test_identical_fail():
      c1 = Card("foo", id=123)
      c2 = Card("foo", id=456)
      assert_identical(c1, c2)
#+end_src
- The ~assert_identical~ function sets ~__tracebackhide__ = True~:
  1. This is optional.
  2. The effect will be that failing tests will NOT include this function in the
     traceback.

- =TODO: ??? conftest.py ???=
  Note that assert rewriting is only applied to conftest.py files and test
  files. =TODO= See the pytest documentation for more details:
  https://docs.pytest.org/en/stable/how-to/assert.html#assertion-introspection-details

*** DONE Testing for Expected Exceptions - 21
CLOSED: [2025-01-06 Mon 21:45]
#+file_name: ch2/test_exceptions.py
#+begin_src python
  import pytest
  import cards


  def test_no_path_raises():
      with pytest.raises(TypeError):
          cards.CardsDB()
#+end_src

- We just checked for the type of exception in ~test_no_path_raises()~. We can
  also check to make sure the message is correct, or any other aspect of the
  exception, like additional parameters:
  #+file_name: ch2/test_exceptions.py
  #+begin_src python
    import pytest
    import cards


    def test_raises_with_info():
        match_regex = "missing 1 .* positional argument"
        with pytest.raises(TypeError, match=match_regex):
            cards.CardsDB()

    def test_raises_with_info_alt():
        with pytest.raises(TypeError) as exc_info:
            cards.CardsDB()
        expected = "missing 1 required positional argument"
        assert expected in str(exc_info.value)
  #+end_src
  * =from Jian=
    Remember, ~with~ clause doesn't create a /scope/!

  * =TODO=
    See the _pytest documentation_ for full ~ExceptionInfo~ reference.

*** DONE Structuring Test Functions - 23
CLOSED: [2025-01-06 Mon 23:52]
- I recommend _making sure you keep assertions at the end of test functions._

  This is such a common recommendation that it has at least two names:
  _Arrange-Act-Assert_ and _Given-When-Then_.

- Bill Wake originally named the Arrange-Act-Assert pattern in 2001.
  * footnote 6 =TODO=

- Kent Beck later popularized the practice as part of test-driven development (TDD).
  * footnote 7 =TODO=

- /Behavior-driven development (BDD)/ uses the terms _Given-When-Then_, a
  pattern from Ivan Moore, popularized by Dan North.
  * footnote 8

- Regardless of the names of the steps, the goal is the same:
  *separate a test into stages*.

  * Benefits:
    Allows the test developer to focus attention on each part,
    + getting ready to do something
    + doing something
    + checking to see if it worked
    and be clear about what is really being tested.

- A common anti-pattern:
  interleaved stages
  * Example:
    "Arrange-Assert-Act-Assert-Act-Assert..."

- Stages:
  1. Given/Arrange
     A starting state. This is where you set up data or the environment to get
     ready for the action.

  2. When/Act
     Some action is performed. This is the focus of the test - the behavior we
     are trying to make sure is working right.

  3. Then/Assert
     Some expected result or end state should happen. At the end of the test, we
     make sure the action resulted in the expected behavior.

*** DONE Grouping Tests with Classes - 24
CLOSED: [2025-01-06 Mon 23:58]
So far so good.

However, _pytest_ also allows us to _group tests with /classes/._

- Example:
  #+file_name: ch2/test_classes.py
  #+begin_src python
    class TestEquality:
        def test_equality(self):
            c1 = Card("something", "brian", "todo", 123)
            c2 = Card("something", "brian", "todo", 123)
            assert c1 == c2

        def test_equality_with_diff_ids(self):
            c1 = Card("something", "brian", "todo", 123)
            c2 = Card("something", "brian", "todo", 4567)
            assert c1 == c2

        def test_inequality(self):
            c1 = Card("something", "brian", "todo", 123)
            c2 = Card("completely different", "okken", "done", 123)
            assert c1 != c2
  #+end_src

- =IMPORTANT=
  Though well-designed _test class inheritance_ can help to share helper
  functions/methods, getting fancy with _test class inheritance_ will certainly
  *confuse* someone, possibly yourself, in the future.

  Just use /test classes/ for grouping tests is a good, simple, and
  straightforward way. It is recommended.

*** DONE Running a Subset of Tests - 25
CLOSED: [2025-01-07 Tue 00:08]
- _pytest_ allows you to *run a subset of tests* in several ways:

  | Subset                        | Syntax                                               |
  |-------------------------------+------------------------------------------------------|
  | Single test method            | ~pytest path/test_module.py::TestClass::test_method~ |
  | All tests in a class          | ~pytest path/test_module.py::TestClass~              |
  | Single test function          | ~pytest path/test_module.py::test_function~          |
  | All tests in a module         | ~pytest path/test_module.py~                         |
  | All tests in a directory      | ~pytest path~                                        |
  | Tests matching a name pattern | ~pytest -k pattern~                                  |
  | Tests by marker               | Covered in Chapter 6, Markers, on page 73.           |

- The ~<pattern>~ in the ~pytest -k <pattern>~ is not a regex,
  but a pytes-specific pattern. Learn from examples:
  * ~pytest -v -k TestEquality~
  * ~pytest -v -k TestEq~
  * ~pytest -k equality~
  * ~pytest -k "equality and not equality_fail"~
    + keywords in pattern: ~and~, ~not~, ~or~
  * ~pytest -k "(dict or ids) and not TestEquality"~

*** DONE Review - 28
CLOSED: [2025-01-07 Tue 00:08]
*** DONE Exercises - 29
CLOSED: [2025-01-07 Tue 00:14]
*** DONE What's Next - 30
CLOSED: [2025-01-07 Tue 00:14]

** DONE 3. pytest Fixtures - 31 - =TODO: exercises=
CLOSED: [2025-01-07 Tue 18:17]
- fixtures :: _test helper functions_ that are essential to structuring test
  code for almost any non-trivial software system.

- fixtures :: functions that are run by _pytest_ *before* (and SOMETIMES
  *after*) the actual test functions.

  * The code in the fixture can do whatever you want it to.
    + You can use /fixtures/ to _get a data set_ for the tests to work on.
    + You can use /fixtures/ to _get a system into a known state_ before running
      a test.
    + /Fixtures/ are also used to get data ready for multiple tests.
    + etc.

- In this chapter, you'll learn _HOW TO_
  * create /fixtures/ and work with them.
  * structure /fixtures/ to hold both setup and teardown code.
  * use scope to allow /fixtures/ to run once over many tests, and
    tests can use MULTIPLE /fixtures/.
  * trace code execution through /fixtures/ and test code.

- First, let's look at
  * a small example /fixture/ and
  * how /fixtures/ and _test functions_ are *connected*.

*** DONE Getting Started with Fixtures - 31
CLOSED: [2025-01-07 Tue 10:29]
Here's a simple fixture that returns a number:
#+file_name: ch3/test_fixtures.py
#+begin_src python
  import pytest

  @pytest.fixture()
  def some_data():
      """Return answer to ultimate question."""
      return 42


  def test_some_data(some_data):
      """Use fixture return value in a test."""
      assert some_data == 42
#+end_src

- The term /fixture/ has many meanings in the programming and test community,
  and even in the Python community.

  In the context of _pytest_, /fixture/ is connected to functions / methods
  decorated by ~@pytest.fixture()~ and the mechanism _pytest_ provides to allow
  the separation of "getting ready for" and "cleaning up after" code from your
  test functions.

- _pytest_ *treats exceptions differently*
  during /fixtures/
  compared to
  during a test function:

  An exception (or /assert failure/ or call to ~pytest.fail()~) that happens
  * during the test code proper _results in a “Fail” result._
  * during a /fixture/, the test function is _reported as “Error.”_

  This distinction is helpful when debugging why a test didn't pass.

- pytest fixtures are one of the unique core features that make pytest stand out
  above other test frameworks, and are the reason why many people switch to and
  stay with pytest.
  =From Jian=, =TODO=
  I know /fixtures/ exist in many test frameworks in other languages. Not sure
  the frameworks in Python. Need a survey.

*** DONE Using Fixtures for Setup and Teardown - 33
CLOSED: [2025-01-07 Tue 12:29]
- Use a /fixture/ to improve a test:
  * No /fixture/ version:
    #+file_name: ch3/test_count_initial.py
    #+begin_src python
      from pathlib import Path
      from tempfile import TemporaryDirectory
      import cards

      def test_empty():
          with TemporaryDirectory() as db_dir:
              db_path = Path(db_dir)
              db = cards.CardsDB(db_path)

              count = db.count()
              db.close()

              assert count == 0
    #+end_src
    This test function really isn't too painful, but two problems exist:
    + It's better to move the set up out, and make it run before ~test_empty~.
    + It's better to place ~db.close()~ at the end of the function, but we have
      to call it before ~assert~, because if the ~assert~ statement fails, it
      won't be called.

  * Use /fixture/ (also thanks to the /context manager/):
    #+file_name: ch3/test_count.py
    #+begin_src python
      import pytest


      @pytest.fixture()
      def cards_db():
          with TemporaryDirectory() as db_dir:
              db_path = Path(db_dir)
              db = cards.CardsDB(db_path)
              yield db
              db.close()

      def test_empty(cards_db):
          assert cards_db.count() == 0
    #+end_src
    ~yield~ give the control to ~test_empy~.

    The /fixture/ resolves the first issue, and combined with the /context
    manager/, the second issue is also resolved: the teardown ~db.close()~ is
    moved into the /fixture/, but it is guaranteed to run after tests, no matter
    what happened during tests.

- =IMPORTANT=
  Remember:
  We never call /fixture/ functions directly - pytest looks at the specific name
  of the arguments to our test and then looks for a /fixture/ with the same name.

- One /fixture/ can be used in multiple tests.

- The /fixture/ and _test function_ are separate functions.

  Carefully naming your /fixtures/ to reflect
  _the work being done in the fixture_
  or
  _the object returned from the fixture_,
  or
  _both_,
  will help with readability.

*** DONE Tracing Fixture Execution with ~-–setup-show~ - 35
CLOSED: [2025-01-07 Tue 12:34]
Visualize when the _setup_ and _teardown_ portions of /fixtures/ run with
respect the tests using them. ~--setup-show~ can help to show this.

- ~pytest --setup-show test_count.py~
  #+begin_src text
    ======================== test session starts =========================
    collected 2 items

    test_count.py
      SETUP    F cards_db
      ch3/test_count.py::test_empty (fixtures used: cards_db).
      TEARDOWN F cards_db
      SETUP    F cards_db
      ch3/test_count.py::test_two (fixtures used: cards_db).
      TEARDOWN F cards_db

    ========================= 2 passed in 0.02s ==========================
  #+end_src
  The ~F~ in front of the _fixture name_ indicates that the /fixture/ is using
  /function scope/.

*** DONE Specifying Fixture Scope - 36
CLOSED: [2025-01-07 Tue 13:11]
- Each /fixture/ has a specific /scope/, which defines the order of when the
  _setup_ and _teardown_ run relative to running of all the test function using
  the /fixture/.

  * =IMPORTANT=
    The /scope/ dictates
    how often the setup and teardown get run when it's used by multiple test
    functions.

- The *DEFAULT scope* for /fixtures/ is /function scope/.
  That means
  1. the setup portion of the /fixture/ will run _BEFORE_ each test that needs
     it runs.
  2. the teardown portion runs _AFTER_ the test is done, for each test.

- However, there may be times when you don't want /function scope/:
  mostly you want to avoid /function scope/ for time-consuming operations.
  * Change the scope can be a solution if build the /fixture/ once is enough for
    multiple tests.

- It's a one-line change, adding scope="module" to the /fixture decorator/:
  #+file_name: ch3/test_mod_scope.py
  #+begin_src python
    @pytest.fixture(scope="module")
    def cards_db():
        with TemporaryDirectory() as db_dir:
            db_path = Path(db_dir)
            db = cards.CardsDB(db_path)
            yield db
            db.close()
  #+end_src
  You can check the /scope/ by running ~pytest --setup-show test_mod_scope.py~.

- Available /fixture scopes/:
  * ~scope='function'~
  * ~scope='class'~
  * ~scope='module'~
  * ~scope='package'~
  * ~scope='session'~ =TODO: ???=

- =CAUTION=:
  The /scope/ is set at the definition of a /fixture/, and _NOT_ at the place
  where it's called.

  The test functions that use a /fixture/ don't control how often a /fixture/ is
  set up and torn down.

- =TODO=
  With a /fixture/ defined within a test /module/, the /session scope/ and
  /package scope/ act just like /module scope/.

  * In order to make use of these OTHER /scopes/, we need to put them in a
    =conftest.py= file.

*** DONE Sharing Fixtures through =conftest.py= - 38
CLOSED: [2025-01-07 Tue 13:51]
- You can put fixtures into individual test files, but to share fixtures among
  multiple test files, you need to use a =conftest.py= file either in the same
  directory as the test file that's using it or in some parent directory.

- The =conftest.py= file is also optional. It is considered by _pytest_ as a
  “local plugin” and can contain /hook functions/ and /fixtures/.

- Let's start by moving the ~cards_db~ /fixture/ out of =test_count.py= and into
  a =conftest.py= file in the same directory:
  #+file_name: ch3/a/conftest.py
  #+begin_src python
    from pathlib import Path
    from tempfile import TemporaryDirectory
    import cards
    import pytest


    @pytest.fixture(scope="session")
    def cards_db():
        """CardsDB object connected to a temporary database"""
        with TemporaryDirectory() as db_dir:
            db_path = Path(db_dir)
            db = cards.CardsDB(db_path)
            yield db
            db.close()
  #+end_src

  #+file_name: ch3/a/test_count.py
  #+begin_src python
    import cards


    def test_empty(cards_db):
        assert cards_db.count() == 0


    def test_two(cards_db):
        cards_db.add_card(cards.Card("first"))
        cards_db.add_card(cards.Card("second"))
        assert cards_db.count() == 2
  #+end_src

  Run ~pytest --setup-show test_count.py~ to show how does the /fixture/ run.

- /Fixtures/ can _only depend on_ other /fixtures/ of their _SAME /scope/ or
  WIDER_.

- *Don’t Import conftest.py*
  Although =conftest.py= is a Python module, it should not be imported by test
  files. The =conftest.py= file *gets read by pytest automatically*, so you
  don't have import conftest anywhere.

*** DONE Finding Where Fixtures Are Defined - 39
CLOSED: [2025-01-07 Tue 14:17]
- _pytest_ shows us a list of all available /fixtures/ our test can use:
  ~pytest --fixtures -v~
  pytest 6.x need this ~-v~ to get the path and line numbers.
  pytest 7+ can show path and line numbers without ~-v~.

  * This list includes
    + a bunch of builtin fixtures that =TODO= we'll look at in the next chapter, as well as
    + those provided by plugins.
    + The /fixtures/ found in =conftest.py= files are at the bottom.

  * If you supply a directory,
    _pytest_ will list the /fixtures/ available to tests in that directory.

  * If you supply a _test file name_, pytest will include those defined in /test
    modules/ as well.

- _pytest_ also includes the first line of the docstring from the /fixture/,
  if you've defined one, and the file and line number where the /fixture/ is
  defined. It will also include the path if it's not in your current directory.
  =TODO= NEED EXAMPLES!

- You can also use ~--fixtures-per-test~ to see what /fixtures/ are used by each
  test and where the /fixtures/ are defined:
  ~pytest --fixtures-per-test test_count.py::test_empty~

*** DONE Using Multiple Fixture Levels - 40
CLOSED: [2025-01-07 Tue 14:51]

*** DONE Using Multiple Fixtures per Test or Fixture - 42
CLOSED: [2025-01-07 Tue 15:14]
Define multiple /fixtures/, and pass them to tests.

*** DONE Deciding Fixture Scope Dynamically - 43
CLOSED: [2025-01-07 Tue 16:13]
Just control the /fixture scope/ through some ways.

- An example about why do we need this:
  The ~cards_db~ /fixture/ is empty because it calls ~delete_all()~.
  If we don't completely trust that ~delete_all()~, we may want "run tests
  depend on it" can be turned off.

  For this kind of requirements, _deciding fixture scope dynamically_ is useful.

  * Code:
    #+file_name: ch3/d/conftest.py
    #+begin_src python
      @pytest.fixture(scope=db_scope)
      def db():
          """CardsDB object connected to a temporary database"""
          with TemporaryDirectory() as db_dir:
              db_path = Path(db_dir)
              db_ = cards.CardsDB(db_path)
              yield db_
              db_.close()
    #+end_src

    Here we use the /fixture scope/ ~db_scope~. Define the it:

    #+file_name: ch3/d/conftest.py
    #+begin_src python
      def db_scope(fixture_name, config):
          if config.getoption("--func-db", None):
              return "function"
          return "session"
    #+end_src

    * There are many ways tell _pytest_ which scope to use dynamically, but in
      this case, I chose to depend on a new command-line flag, ~--func-db~.
      + In order to tell _pytest_ to allow us to use this new flag, we need to
        write a /hook function/ (=TODO= which I'll cover in more depth in
        _Chapter 15, Building Plugins, on page 205_):
        #+file_name: ch3/d/conftest.py
        #+begin_src python
          def pytest_addoption(parser):
              parser.addoption(
                  "--func-db",
                  action="store_true",
                  default=False,
                  help="new db for each test",
              )
        #+end_src

        Test it with commands
        ~pytest --setup-show test_count.py~
        ~pytest --func-db --setup-show test_count.py~

*** DONE Using autouse for Fixtures That Always Get Used - 45
CLOSED: [2025-01-07 Tue 16:19]
- Example:
  Add test times after each /test/, and
  add the date and current time at the end of the /session/.
  #+file_name: ch3/test_autouse.py
  #+begin_src python
    import pytest
    import time


    @pytest.fixture(autouse=True, scope="session")
    def footer_session_scope():
            """Report the time at the end of a session."""
            yield
            now = time.time()
            print("--")
            print(
                "finished : {}".format(
                    time.strftime("%d %b %X", time.localtime(now))
                )
            )
            print("-----------------")


    @pytest.fixture(autouse=True)
    def footer_function_scope():
        """Report test durations after each function."""
        start = time.time()
        yield
        stop = time.time()
        delta = stop - start
        print("\ntest duration : {:0.3} seconds".format(delta))


    def test_1():
        """Simulate long-ish running test."""
        time.sleep(1)


    def test_2():
        """Simulate slightly longer test."""
        time.sleep(1.23)
  #+end_src
  Run this with the command ~pytest -v -s test_autouse.py~

  =IMPORTANT=
  Here ~-s~ is the shortcut flag for ~--capture=no~ that tells _pytest_ to turn
  off /output capture/ -- we need them to check how does fixture autouse work.

  =IMPORTANT=
  Without turning off /output capture/, _pytest_ only prints the output of tests
  that fail.

*** DONE Renaming Fixtures - 46
CLOSED: [2025-01-07 Tue 15:23]
#+file_name: ch3/test_rename_fixture.py
#+begin_src python
  import pytest


  @pytest.fixture(name="ultimate_answer")
  def ultimate_answer_fixture():
      return 42


  def test_everything(ultimate_answer):
      assert ultimate_answer == 42
#+end_src
People may want to add prefix or posfix like ~_fixture~ / ~fixture_~ to a
/fixture/, and rename them to make the shorter. Then the it is easy to identify
/fixtures/, the names at usage sites can keep shorter.

- One instance where renaming is useful is when the most obvious /fixture name/
  already exists as an existing variable or function name
  #+file_name: ch3/test_rename_2.py
  #+begin_src python
    import pytest
    from somewhere import app


    @pytest.fixture(scope="session", name="app")
    def _app():
        """The app object"""
        yield app()


    def test_that_uses_app(app):
        assert app.some_property == "something"
  #+end_src

- =from the author=
  * I usually only use /fixture renaming/ with a /fixture/ that lives in the
    same /module/ as the tests using it,

    as _renaming a /fixture/ can make it *harder to find* where it's defined._
    + However, remember that there is always ~--fixtures~, which can help you
      find where a /fixture/ lives.

*** DONE Review - 47
CLOSED: [2025-01-07 Tue 16:19]
*** TODO Exercises - 48
*** DONE What's Next - 48
CLOSED: [2025-01-07 Tue 18:17]

** TODO 4. Builtin Fixtures - 49
Reusing COMMON /fixtures/ is such a good idea that the _pytest_ developers
*included some commonly used /fixtures/ with _pytest_.*

- The /pytest builtin fixtures/ that come prepackaged with _pytest_ can help you
  do some pretty useful things in your tests easily and consistently.
  * For example, _pytest_ includes /builtin fixtures/ that can
    + handle temporary directories and files,
    + access command-line options,
    + communicate between test sessions,
    + validate output streams,
    + modify environment variables, and
    + interrogate warnings.

- The /builtin fixtures/ are extensions to the core functionality of _pytest_.

- We'll take a look at a few of the /builtin fixtures/ in this chapter:
  * ~tmp_path~ and ~tmp_path_factory~ - for _temporary directories_
  * ~capsys~ - for _capturing output_
  * ~monkeypatch~ - for changing the environment or application code, like a
    _LIGHTWEIGHT form of /mocking/._

- This is a good mix that shows you some of the extra capabilities you can get
  with creative /fixture/ use.

  I encourage you to read up on other builtin fixtures by reading the output of
  ~pytest --fixtures~.

*** Using ~tmp_path~ and ~tmp_path_factory~ - 49
*** Using ~capsys~ - 51
*** Using ~monkeypatch~ - 54
*** Remaining Builtin Fixtures - 58
*** Review - 59
*** Exercises - 59
*** What's Next - 60

** TODO 5. Parametrization - 61
- Parametrized testing ::
  adding _parameters_ to our _test functions_ and
  passing in multiple sets of arguments to the test
  TO create new test cases.

- We'll look at _THREE ways_ to implement /parametrized testing/ in _pytest_ in
  the order in which they should be selected:
  * Parametrizing functions
  * Parametrizing /fixtures/
  * Using a /hook function/ called ~pytest_generate_tests~

  We'll compare them side by side by solving the same parametrization problem
  using all THREE methods -- there are times when one solution is preferred over
  the others.

- Steps:
  Before we really jump in to how to use parametrization, though,
  1. we'll take a look at the redundant code we are avoiding with
     parametrization.

  2. Then we'll look at THREE methods of parametrization.

  3. When we're done, you’ll be able to write concise, easy-to-read test code
     that tests a huge number of test cases.

- *Parametrize or Parameterize?*
  * Legal English spellings:
    + parametrize
    + parameterize
    + parametrise
    + parameterise

  * _pytest_ pick one form and keeps using it: *parametrize*
    If you spell it wrong, you'll see:
    #+begin_quote
    "E Failed: Unknown 'parameterize' mark, did you mean 'parametrize'?"
    #+end_quote

*** Testing Without Parametrize - 62
*** Parametrizing Functions - 64
*** Parametrizing Fixtures - 66
*** Parametrizing with ~pytest_generate_tests~ - 67
*** Using Keywords to Select Test Cases - 69
*** Review - 71
*** Exercises - 71
*** What's Next - 72

** TODO 6. Markers - 73
In _pytest_,
/markers/ are a way to tell pytest there's something special about a particular
test.

- You can think of /markers/ like _tags_ or _labels_.

  * ~@pytest.mark.slow~:
    If some tests are slow, you can mark them with this and have _pytest_ *skip*
    those tests when you're in a hurry.

  * You can pick a handful of tests out of a test suite and mark them with
    ~@pytest.mark.smoke~ and run those as the first stage of a testing pipeline
    in a CI system.

  Really, for any reason you might have for *separating out some tests*, you can
  use /markers/.

- _pytest_ includes a handful of /builtin markers/ that _MODIFY the behavior of
  how tests are run._

  * In addition to the _custom tag-like markers_ we can create and add to our
    tests, =TODO=

    the /builtin markers/ tell _pytest_ to do something special with the _marked
    tests_.

- In this chapter,
  we're going to explore both /types/ of /markers/:
  the _builtins_ that change behavior, and
  the _custom markers_ we can create to select which tests to run.

- We can also use markers to pass information to a fixture used by a test. We’ll
  take a look at that, too.

*** Using Builtin Markers - 73
- /pytest's builtin markers/ are used to _modify the behavior of how tests run_.

- We explored ~@pytest.mark.parametrize()~ in the last chapter. Here's the full
  list of the /builtin markers/ included in pytest as of _pytest 6_:
  * ~@pytest.mark.filterwarnings(warning)~:
    This marker adds a warning filter to the given test.
    =TODO: ???=

  * ~@pytest.mark.skip(reason=None)~:
    This marker skips the test with an optional reason.

  * ~@pytest.mark.skipif(condition, ..., *, reason)~:
    This marker skips the test if any of the conditions are True.

  * ~@pytest.mark.xfail(condition, ..., *, reason, run=True, raises=None, strict=xfail_strict)~:
    This marker tells pytest that we expect the test to fail.

  * ~@pytest.mark.parametrize(argnames, argvalues, indirect, ids, scope)~:
    This marker calls a test function multiple times, passing in different
    arguments in turn.

  * ~@pytest.mark.usefixtures(fixturename1, fixturename2, ...)~:
    This marker marks tests as needing all the specified fixtures.
    =TODO: ???=

*** DONE Skipping Tests with ~pytest.mark.skip~ - 74
CLOSED: [2025-01-08 Wed 10:39]
#+file_name: ch6/builtins/test_less_than.py
#+begin_src python
  from cards import Card

  @pytest.mark.skip(reason="Card doesn't support < comparison yet")
  def test_less_than():
      c1 = Card("a task")
      c2 = Card("b task")
      assert c1 < c2


  def test_equality():
      c1 = Card("a task")
      c2 = Card("a task")
      assert c1 == c2
#+end_src
- ~pytest test_skip.py~

- ~pytest -v -ra test_skip.py~
  * ~-r~ flag tells _pytest_ to report reasons for different test results at the
    end of the session.

    =IMPORTANT=
    You give it a single character that represents the kind of result you want
    more information on.

    + ~a~ in ~-ra~ stands for "all except passed."
      The ~-ra~ flag is therefore the most useful, as we almost always want to
      know the reason why certain tests did not pass.

    + The default display use the flags ~-rfE~:
      - ~f~ for failed tests.
      - ~E~ for errors.

*** DONE Skipping Tests Conditionally with ~pytest.mark.skipif~ - 76
CLOSED: [2025-01-08 Wed 10:48]
- With both the ~skip~ and the ~skipif~ /markers/,
  *the test is NOT actually run*.

- Example:
  Won't support sorting in the 1.x.x versions of the Cards application, but will
  in version 2.x.x.

  We can tell _pytest_ to skip the test for all versions of Cards lower than than
  2.x.x like this:
  #+file_name: ch6/builtins/test_skipif.py
  #+begin_src python
    import cards
    from packaging.version import parse


    @pytest.mark.skipif(
        parse(cards.__version__).major < 2,
        reason="Card < comparison not supported in 1.x",
    )
    def test_less_than():
        c1 = Card("a task")
        c2 = Card("b task")
        assert c1 < c2
  #+end_src
  * Use a third-party package for handling version info: ~packaging~

- =IMPORTANT=
  Common use cases:
  * create tests for different OS with the help of ~skipif~.

*** DONE Expecting Tests to Fail with ~pytest.mark.xfail~ - 7
CLOSED: [2025-01-08 Wed 15:39]
If you want to run a test which will raise a expected failure, use the ~xfail~
/builtin marker/.

- The full signature for ~xfail~:
  #+begin_src python
    @pytest.mark.xfail(
        condition,
        ...,
        *,
        reason,
        run=True,
        raises=None,
        strict=xfail_strict
    )
  #+end_src

- Example:
  #+file_name: ch6/builtins/test_xfail.py
  #+begin_src python
    @pytest.mark.xfail(
        parse(cards.__version__).major < 2,
        reason="Card < comparison not supported in 1.x",
    )
    def test_less_than():
        c1 = Card("a task")
        c2 = Card("b task")
        assert c1 < c2


    @pytest.mark.xfail(reason="XPASS demo")
    def test_xpass():
        c1 = Card("a task")
        c2 = Card("a task")
        assert c1 == c2


    @pytest.mark.xfail(reason="strict demo", strict=True)
    def test_xfail_strict():
        c1 = Card("a task")
        c2 = Card("a task")
        assert c1 == c2
  #+end_src

  * ~pytest -v -ra test_xfail.py~
    #+begin_src text
      ========================= test session starts ==========================
      collected 3 items

      test_xfail.py::test_less_than XFAIL (Card < comparison not s...) [ 33%]
      test_xfail.py::test_xpass XPASS (XPASS demo)                     [ 66%]
      test_xfail.py::test_xfail_strict FAILED                          [100%]
      =============================== FAILURES ===============================
      __________________________ test_xfail_strict ___________________________
      [XPASS(strict)] strict demo
      ======================= short test summary info ========================
      XFAIL test_xfail.py::test_less_than
        Card < comparison not supported in 1.x
      XPASS test_xfail.py::test_xpass XPASS demo
      FAILED test_xfail.py::test_xfail_strict
      =============== 1 failed, 1 xfailed, 1 xpassed in 0.11s ================
    #+end_src

    For tests marked with ~xfail~:
    + Failing tests will result in ~XFAIL~.
    + Passing tests (with no ~strict~ setting) will result in ~XPASSED~.
    + Passing tests with ~strict=true~ will result in ~FAILED~.

- If you want to set ~strict~ for ~xfail~ globally, add ~xfail_strict=true~ to
  =pytest.ini=.

- =IMPORTANT=
  =IMPORTANT=
  =IMPORTANT=
  =THE RIGHT WAY TO USE xfail=
  There are a couple additional reasons why you might want to use ~xfail~:
  * =The author's favorite use of xfail=
    Writing tests first - =from Jian= also suggested by _How to Design Programs_

    Writing a bunch of test cases you know aren't implemented yet
    *BUT that you plan on implementing SHORTLY.*

    MARKS the new behaviors with ~xfail~ and REMOVE the ~xfail~ gradually as you
    implement the behavior.

    =from Jian= Summary: use this way, the ~xfail~'s disappear after a short
    period, when the planned behaviors have been implemented.

    OR

  * Something breaks, a test (or more) fails, and the person or team that needs
    to fix the break can't work on it right away.

    Marking the tests as xfail, ~strict=true~, with the reason _written to
    include the defect/issue report ID_ is a decent way to keep the test
    running, not forget about it, and alert you when the bug is fixed.

- =IMPORTANT=
  =IMPORTANT=
  =IMPORTANT=
  *BAD reasons to use use ~xfail~ or ~skip~.* Here's one:

  Remember YAGNI ("Ya Aren’t Gonna Need It") that comes from Extreme
  Programming and states: "Always implement things when you actually need
  them, never when you just foresee that you need them."

  Don't use ~xfail~ and ~~skip~ for features if you can't finish the
  development of these features in a short term.

*** DONE Selecting Tests with Custom Markers - 79
CLOSED: [2025-01-08 Wed 15:51]
/Custom markers/ can be used to *select* tests to _run_ or _skip_.

- Example:
  Two custom markers and register them for using without warnings:
  #+file_name: ch6/reg/pytest.ini
  #+begin_src ini
    [pytest]
    markers =
        smoke: subset of tests
        exception: check for expected exceptions
  #+end_src

  #+file_name: ch6/reg/test_start.py
  #+begin_src python
    @pytest.mark.smoke
    def test_start(cards_db):
        """start changes state from "todo" to "in prog"
        """
        i = cards_db.add_card(Card("foo", state="todo"))
        cards_db.start(i)
        c = cards_db.get_card(i)
        assert c.state == "in prog"


    @pytest.mark.exception
    def test_start_non_existent(cards_db):
        """Shouldn't be able to start a non-existent card."""
        any_number = 123 # any number will be invalid, db is empty
        with pytest.raises(InvalidCardId):
            cards_db.start(any_number)
  #+end_src

  * ~pytest -v -m smoke test_start.py~
  * ~pytest -v -m exception test_start.py~

*** TODO Marking Files, Classes, and Parameters - 82
*** TODO Using “and,” “or,” “not,” and Parentheses with Markers - 85
*** DONE Being Strict with Markers - 86
CLOSED: [2025-01-08 Wed 16:01]
#+file_name: ch6/strict/pytest.ini
#+begin_src toml
  [pytest]
  markers =
      smoke: subset of tests
      exception: check for expected exceptions
      finish: all of the "cards finish" related tests
  addopts =
      --strict-markers
#+end_src

This is good for /markers/ that can't be found -- feedback about this error is
issued at collection time, not at test case run time.

*** DONE Combining Markers with Fixtures - 88
CLOSED: [2025-01-08 Wed 16:37]
=from Jian=
I think _property test frameworks_ often support this directly.
_Property test frameworks_ is more straightforward in my experience, I mean in
Scala.

- /Markers/ can be used in conjunction with /fixtures/.
  * They also can be used in conjunction with /plugins/ and /hook functions/
    (=TODO= but that's a topic for Chapter 15, Building Plugins, on page 205).

    Here, we'll combine /markers/ and /fixtures/ to help test _the Cards
    application_.

- The /builtin markers/ took parameters,
  WHILE the custom ones we've used *SO FAR do not*.

  Let's create a new /marker/ called ~num_cards~ that we can pass to the
  ~cards_db~ /fixture/.

- The ~cards_db~ /fixture/
  CURRENTLY
  cleans out the database for each test that wants to use it:
  #+file_name: ch6/combined/test_three_cards.py
  #+begin_src python
    @pytest.fixture(scope="function")
    def cards_db(session_cards_db):
        db = session_cards_db
        db.delete_all()
        return db
  #+end_src

- Example:
  #+file_name: ch6/combined/pytest.ini
  #+begin_src ini
    [pytest]
    markers =
        smoke: subset of tests
        exception: check for expected exceptions
        finish: all of the "cards finish" related tests
        num_cards: number of cards to prefill for cards_db fixture
  #+end_src

  #+file_name: ch6/combined/conftest.py
  #+begin_src python
    @pytest.fixture(scope="function")
    def cards_db(session_cards_db, request, faker):
        db = session_cards_db
        db.delete_all()

        # support for `@pytest.mark.num_cards(<some number>)`
        # random seed
        faker.seed_instance(101)
        m = request.node.get_closest_marker("num_cards")
        if m and len(m.args) > 0:
            num_cards = m.args[0]
            for _ in range(num_cards):
                db.add_card(
                    Card(summary=faker.sentence(), owner=faker.first_name())
                )
        return db
  #+end_src
  This assume ~@pytest.mark.num_cards~ is the same as
  ~@pytest.mark.num_cards(0)~.

  #+file_name: ch6/combined/test_num_cards.py
  #+begin_src python
    import pytest


    def test_no_marker(cards_db):
        assert cards_db.count() == 0


    @pytest.mark.num_cards
    def test_marker_with_no_param(cards_db):
        assert cards_db.count() == 0


    @pytest.mark.num_cards(3)
    def test_three_cards(cards_db):
        assert cards_db.count() == 3
        # just for fun, let's look at the cards Faker made for us
        print()
        for c in cards_db.list_cards():
            print(c)

    @pytest.mark.num_cards(10)
    def test_ten_cards(cards_db):
        assert cards_db.count() == 10
  #+end_src

  ~pytest -v -s test_num_cards.py~

*** DONE Listing Markers - 92
CLOSED: [2025-01-08 Wed 16:03]
~pytest --markers~

*** TODO Review - 92
*** TODO Exercises - 94
*** TODO What's Next - 95

* Part II — Working with Projects
** TODO 7. Strategy - 99
- _SO FAR in this book_ we've been talking about the mechanics of _pytest_ - the
  "how to write tests" part of software testing - including
  * writing test functions
  * using fixtures
  * implementing parametrized testing

  =IMPORTANT=
  In this chapter, we're going to use all that you've learned about _pytest_ so
  far to *create a test strategy for the Cards project* - the *"what tests to
  write"* part of software testing.

- Steps:
  1. We'll start by defining goals for our test suite.

  2. We'll then look at HOW the software architecture of Cards has INFLUENCE on
     our /test strategy/ and IS INFLUENCED BY the need for tests.

  3. Then we can start selecting and prioritizing which features to test.

  4. Once we know what features need tests, we can generate a list of test cases
     needed.

  5. All of this methodical planning really doesn't take long, and will help to
     generate a pretty decent initial test suite.

*** Determining Test Scope - 99
- Different projects have different test goals and requirements.

- We will almost always want to test the behavior of the user visible
  functionality.

  However, there are quite a few other questions we need to consider when
  determining how much testing we need to do:
  * _Is security a concern?_
    This is especially important if you save any confidential information.

  * _Performance?_
    Do interactions need to be fast? How fast?

  * _Loading?_
    Can you handle lots of people with lots of requests? Are you expecting to need to?
    If so, you should test for that.

  * _Input validation?_
    For really any system that accepts input from users, we should validate the
    data before acting on it.

- _The Cards project_ is intended for use by an individual or a small team.

  Even so, in reality, all of the concerns above apply to this project,
  especially as it grows.

  *So for an initial test suite*, how much testing should we do?
  Here's a reasonable start:
  * Test the behavior of user visible functionality.

  * Postpone security, performance, and load testing for the current design.
    + The current design is to have the database stored in the users home
      directory. When/if that moves to a shared location with multiple users,
      these concerns will definitely be more important.

  * _Input validation is also less important while Cards is a single user
    application._
    However, I also don't want stack traces to occur while using the app, so we
    should test wacky input, at least at the CLI level.

- All projects will need to have /functionality or feature testing/.
  * However, even with /functionality testing/ alone,
    + we need to decide which features need testing and at what priority.
    + Then for each feature, we need to decide on test cases.

- Using a methodical approach makes all of this fairly straightforward.
  * We'll go through all of this for the Cards project as an example.
  * We'll begin by prioritizing features and then generating test cases.
  * *But first*, let's take a look at how your project's software architecture
    can influence the testing strategy you choose.

*** Considering Software Architecture - 101
How your application is set up - its software architecture - is an important
consideration when determining a testing strategy.

- Software architecture ::
  * how your project's software is organized,
  * what APIs are available,
  * what the interfaces are,
  * where code complexity lives, modularity, and so much more.

- In relation to testing, we need to know
  * how much of the system we need to test
  * what the entry points are

- As a simple example,
  let's say we're testing _code that exists in one module_,
  1. is intended to be used on the command line,
  2. has no interactive components other than print output,
  3. has no API.

- If the code is not written in Python, we have no choices by test it as a black box.
  This is not the case of _the Cards project_, which we want to test.

- If the code is written in Python and is importable, and we can test the
  different parts of it by calling functions within the module, we then have
  choices.
  1. We can still test it as before, as a black box.
  2. But we can also test the functions inside separately if we want to.

*** Evaluating the Features to Test - 103
*** Creating Test Cases - 105
*** Writing a Test Strategy - 108
*** Review - 109
*** Exercises - 110
*** What's Next - 111

** DONE 8. Configuration Files - 113 - =NOTE=
CLOSED: [2025-01-09 Thu 12:28]
Configuration files for _pytest_ -
those non-test files that _affect how pytest runs_ - save time and duplicated
work.

- For example,
  if you find yourself always using certain flags in your tests, like
  ~--verbose~ or ~--strict-markers~, you can tuck those away in a config file
  and not have to type them all the time.

- IN ADDITION TO /configuration files/, a handful of other files are useful when
  using pytest to make work of writing and running tests easier.
  We'll cover all of them in this chapter.

*** DONE Understanding pytest Configuration Files - 113
CLOSED: [2025-01-08 Wed 18:50]
- Let's run down the non-test files relevant to pytest:
  * =pytest.ini=:
    This is the primary pytest configuration file that allows you to change
    pytest's default behavior. Its location also defines _the pytest root
    directory_, or ~rootdir~.

  * =conftest.py=:
    This file contains /fixtures/ and /hook functions/. It can exist at the
    ~rootdir~ or in _ANY subdirectory_.

  * =__init__.py=:
    When put into test subdirectories, this file allows you to have identical
    test file names in multiple test directories.
    =TODO: ??? re-export ???=

  * =tox.ini=, =pyproject.toml=, and =setup.cfg=:
    These files can take the place of =pytest.ini=.
    If you already have one of these files in a project, you can use it to save
    pytest settings.

    + =tox.ini=
      is used by _tox_, the command-line automated testing tool we take a look
      at in =TODO= Chapter 11, _tox_ and _Continuous Integration, on page 151._

    + =pyproject.toml=
      is used for packaging Python projects and can be used to save settings for
      various tools, INCLUDING _pytest_.

    + =setup.cfg=
      is also used for packaging, and can be used to save _pytest_ settings.

- Example project directory, with =tests=
  #+begin_src text
    cards_proj
    ├── ... top level project files, src dir, docs, etc ...
    ├── pytest.ini
    └── tests
        ├── conftest.py
        ├── api
        │   ├── __init__.py
        │   ├── conftest.py
        │   └── ... test files for api ...
        └── cli
            ├── __init__.py
            ├── conftest.py
            └── ... test files for cli ...
  #+end_src

*** TODO Saving Settings and Flags in pytest.ini - 114 - =NOTE=
*** TODO Using tox.ini, pyproject.toml, or setup.cfg in place of pytest.ini - 116 - =NOTE=
*** TODO Determining a Root Directory and Config File - 118
*** TODO Sharing Local Fixtures and Hook Functions with conftest.py - 119
*** TODO Avoiding Test File Name Collision - 119
*** TODO Review - 121
*** TODO Exercises - 121
*** TODO What's Next - 122

** DONE 9. Coverage - 123 - =NOTE=
CLOSED: [2025-01-09 Thu 15:54]
- /line coverage/
- /branch coverage/

- Use _Coverage.py_ and optinoally _pytest-cov_ (make the command line a little
  shorter).

*** Using coverage.py with pytest-cov - 123
- They are 3rd-party packages that need installation.

- ~pytest --cov=cards ch7~ (use coverage.py through pytest-cov)
  1. run ~coverage~ with ~--source~ set to ~cards~ while running _pytest_ with
     the tests in ch7, and
  2. run ~coverage report~ for the terminal line-coverage report.

- Use coverage.py directly
  1. ~coverage run --source=cards -m pytest ch7~
  2. ~coverage report~

*** Generating HTML Reports - 127
*** Excluding Code from Coverage - 129
*** Running Coverage on Tests - 130
*** Running Coverage on a Directory - 131
*** Running Coverage on a Single File - 132
*** Review - 134
*** Exercises - 134
*** What's Next - 135

** TODO 10. Mocking - 137
- In the last chapter, we _tested the Cards project through the API._

- In this chapter, we're going to _test the CLI._

  When we wrote the /test strategy/ for the Cards project in Writing a Test
  Strategy, on page 108, we included the following statement:
  * Test the CLI enough to verify the API is getting properly called for all features.

- We're going to use the ~mock~ package to help us with that.

  Shipped as part of the Python standard library as ~unittest.mock~ as of
  _Python 3.3_, the ~mock~ package *is used to swap out pieces of the system to
  isolate bits of our application code from the rest of the system.*

  * /Mock objects/ are sometimes called test *doubles*, *spies*, *fakes*, or
    *stubs*. Between pytest's own ~monkeypatch~ fixture (covered in Using
    monkeypatch, on page 54) and ~mock~, you should have all the test double
    functionality you need.

- In this chapter,
  * we'll take a look at using ~mock~ to help us test the Cards CLI.
  * we'll also look at using the ~CliRunner~ provided by _Typer_ to *assist* in
    testing.

*** Isolating the Command-Line Interface - 137
*** Testing with Typer - 139
*** Mocking an Attribute - 140
*** Mocking a Class and Methods - 141
*** Keeping Mock and Implementation in Sync with Autospec - 143
*** Making Sure Functions Are Called Correctly - 145
*** Creating Error Conditions - 146
*** Testing at Multiple Layers to Avoid Mocking - 147
*** Using Plugins to Assist Mocking - 148
*** Review - 149
*** Exercises - 149
*** What's Next - 150

** 11. tox and Continuous Integration - 151
*** What Is Continuous Integration? - 151
*** Introducing tox - 152
*** Setting Up tox - 153
*** Running tox - 154
*** Testing Multiple Python Versions - 155
*** Running tox Environments in Parallel - 156
*** Adding a Coverage Report to tox - 156
*** Specifying a Minimum Coverage Level - 157
*** Passing pytest Parameters Through tox - 158
*** Running tox with GitHub Actions - 159
*** Review - 162
*** Exercises - 162
*** What's Next - 163

** TODO 12. Testing Scripts and Applications - 165
*** Testing a Simple Python Script - 166
*** Testing an Importable Python Script - 168
*** Separating Code into src and tests Directories - 170
*** Defining the Python Search Path - 171
*** Testing requirements.txt-Based Applications - 172
*** Review - 175
*** Exercises - 176
*** What's Next - 177

** TODO 13. Debugging Test Failures - 179
*** Adding a New Feature to the Cards Project - 179
*** Installing Cards in Editable Mode - 182
*** Debugging with pytest Flags - 183
*** Re-Running Failed Tests - 184
*** Debugging with pdb - 186
*** Combining pdb and tox - 189
*** Review - 191
*** Exercises - 192
*** What's Next - 193

* Part III — Booster Rockets
** 14. Third-Party Plugins - 197
*** Finding Plugins - 197
*** Installing Plugins - 198
*** Exploring the Diversity of pytest Plugins - 198
*** Running Tests in Parallel - 201
*** Randomizing Test Order - 203
*** Review - 204
*** Exercises - 204
*** What's Next - 204

** 15. Building Plugins - 205
*** Starting with a Cool Idea - 205
*** Building a Local conftest Plugin - 207
*** Creating an Installable Plugin - 209
*** Testing Plugins with pytester - 214
*** Testing Multiple Python and pytest Versions with tox - 217
*** Publishing Plugins - 218
*** Review - 218
*** Exercises - 219
*** What's Next - 220

** 16. Advanced Parametrization - 221
*** Using Complex Values - 221
*** Creating Custom Identifiers - 223
*** Parametrizing with Dynamic Values - 227
*** Using Multiple Parameters - 227
*** Using Indirect Parametrization - 229
*** Review - 232
*** Exercises - 233
*** What's Next - 233

* A1. Virtual Environments - 235
* A2. pip - 237
* Index - 241
