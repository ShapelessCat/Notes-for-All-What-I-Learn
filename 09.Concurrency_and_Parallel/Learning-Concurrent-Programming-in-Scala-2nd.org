#+TITLE: Learning Concurrent Programming in Scala
#+SUBTITLE: Learn the art of building intricate, modern, scalable, and concurrent applications using Scala
#+VERSION: 2nd - 2017
#+FOREWORD BY: Martin Odersky, Professor at EPFL, the creator of Scala
#+AUTHOR: Aleksandar Prokopec
#+STARTUP: entitiespretty
#+STARTUP: indent
#+STARTUP: overview

* DONE Preface - 1
  CLOSED: [2021-08-31 Tue 00:57]
  - As *the level of abstraction GROWS* in _modern languages_ and _concurrency frameworks_,
    it is becoming crucial to know *how* and *when* to use them:

    1. Having a good grasp of the _CLASSICAL /concurrency and synchronization
       primitives/,_ such as /threads/, /locks/, and /monitors/,
       *is NO LONGER SUFFICIENT*.

    2. /High-level concurrency frameworks/, which
       + solve many issues of traditional concurrency and
       + are tailored towards specific tasks,
       are gradually overtaking the world of /concurrent programming/.

  - This book describes _high-level concurrent programming_ in Scala.
    * It
      + presents detailed explanations of _various concurrency topics_ and
      + covers the _basic theory of concurrent programming_.

    * Simultaneously, it
      + describes _MODERN concurrency frameworks_,
      + shows their DETAILED /semantics/, and
      + teaches you _how to use them_.

      Its goal is to *introduce* important _concurrency abstractions_ and, at the
      same time, *show* _how they work in real code_.

  - We are convinced that, by reading this book,
    1. you will both 
       + GAIN a SOLID _theoretical understanding_ of /concurrent programming/
       + DEVELOP a set of _useful practical skills_ that are required to write
         _CORRECT and EFFICIENT concurrent programs._

    2. These skills are the *first steps* toward becoming a _modern concurrency
       expert_.

** DONE What this book covers - 1
   CLOSED: [2021-08-31 Tue 00:51]
   This book is organized into a sequence of chapters with various topics on
   /concurrent programming/.

   - The book
     * covers the _fundamental concurrent APIs_ that are a part of /the Scala runtime/,
     * introduces _MORE COMPLEX_ /concurrency primitives/, and
     * gives an _extensive overview_ of /high-level concurrency abstractions/.

   - _Chapter 1, Introduction,_
     * explains the *need* for /concurrent programming/ and
     * gives some philosophical background.
       
     * At the same time,
       it covers the basics of the _Scala programming language_ that are
       required for understanding the rest of this book.
     
   - _Chapter 2, Concurrency on the JVM and the Java Memory Model,_
     teaches you the basics of concurrent programming.
       This chapter will teach you
     * how to use /threads/ and
     * how to *protect access to* /shared memory/ and
     * introduce the /Java Memory Model/.
     
   - _Chapter 3, Traditional Building Blocks of Concurrency,_
     presents *classic concurrency utilities*, such as
     * /thread pools/,
     * /atomic variables/, and
     * /concurrent collections/,
     * with a particular focus on the interaction with the features of the Scala
       language.

     The emphasis in this book is on the /modern, high-level concurrent
     programming frameworks/. Consequently, this chapter presents an _overview
     of *TRADITIONAL* /concurrent programming/ techniques,_ but it does not aim
     to be extensive.
     
   - _Chapter 4, Asynchronous Programming with Futures and Promises,_ is the
     first chapter that deals with _a *Scala-specific* concurrency framework._

     * This chapter presents the /futures/ and /promises/ API and shows how to
       correctly use them when implementing /asynchronous programs/.
     
   - _Chapter 5, Data-Parallel Collections,_
     describes the /Scala parallel collections/ framework.
     * In this chapter,
       you will learn
       + *how* to _PARALLELIZE collection operations_,
       + *when* it is _ALLOWED to parallelize them_, and
       + *how* to assess the _performance benefits_ of doing so.
     
   - _Chapter 6, Concurrent Programming with Reactive Extensions,_
     teaches you
     how to use the /Reactive Extensions framework/ for event-based and
     /asynchronous programming/.

     * You will see
       + how the *operations* on /event streams/ correspond to _collection operations_,
       + how to *pass* events *from* one /thread/ to another, and
       + how to *design* a /reactive user interface/ using /event streams/.
     
   - _Chapter 7, Software Transactional Memory,_
     introduces *the ScalaSTM library* for /transactional programming/,
     which aims to provide a *safer*, *more intuitive*, /shared-memory
     programming model/.

     * In this chapter,
       you will learn
       + how to *protect access to* /shared data/ using /scalable memory
         transactions/ and,

       + at the same time, *reduce* the risk of /deadlocks/ and /race conditions/.
     
   - _Chapter 8, Actors,_
     presents the /actor programming model/ and the /Akka framework/.
     * In this chapter,
       you will learn
       + how to /transparently *build* message-passing distributed programs/
         that run on multiple machines.

   - _Chapter 9, Concurrency in Practice,_
     summarizes the _DIFFERENT_ /concurrency libraries/ introduced in the earlier
     chapters.
     * In this chapter, you will learn
       + how to *choose* the /correct concurrency abstraction/ to solve a given
         problem, and
       + how to *combine* _different_ /concurrency abstractions/ together
         when *designing* LARGER /concurrent applications/.
     
   - _Chapter 10, Reactors,_
     presents the /reactor programming model/, whose focus is *improved
     composition* in /concurrent and distributed programs/.
     * This emerging model enables
       *separation* of /concurrent and distributed programming patterns/
       *into* /modular components/ called /protocols/.

   While we recommend that you read the chapters in the order in which they
   appear, this is not strictly necessary.

** DONE What you need for this book - 3
   CLOSED: [2021-08-31 Tue 00:51]
*** Installing the JDK
*** Installing and using SBT
*** Using Eclipse, IntelliJ IDEA, or another IDE
    
** DONE Who this book is for - 8
   CLOSED: [2021-08-31 Tue 00:56]
   This book is primarily intended for developers
   who *have learned* how to write /sequential Scala programs/, and
   *wish to learn* how to write correct /concurrent programs/.

   - Basic understanding of OO or FP should be a sufficient prerequisite.

** DONE Conventions - 8
   CLOSED: [2021-08-31 Tue 00:56]
** DONE Reader feedback - 10
   CLOSED: [2021-08-31 Tue 00:56]
** DONE Customer support - 10
   CLOSED: [2021-08-31 Tue 00:57]
** DONE Downloading the example code - 10
   CLOSED: [2021-08-30 Mon 21:06]
** TODO Errata - 11
   https://www.packtpub.com/support/code-downloads

** DONE Piracy - 11
   CLOSED: [2021-08-31 Tue 00:57]
** DONE Questions - 12
   CLOSED: [2021-08-31 Tue 00:57]
   
* TODO Chapter 1: Introduction - 13
  - This chapter _explains the basics of concurrent computing and presents some
    Scala preliminaries required for this book_. Specifically, it does the following:
    * Shows a brief _overview_ of /concurrent programming/
    * Studies the _advantages_ of using Scala when it comes to concurrency
    * Covers the Scala _preliminaries required_ for reading this book

  - We will start by examining
    * *What* /concurrent programming/ is
    * *Why* it is important
    
** TODO Concurrent programming - 14
   - Concurrent programming :: we express a program as a set of concurrent
        computations that execute _during OVERLAPPING time intervals and
        coordinate_ in some way.

   - /Concurrent programming/ has MULTIPLE _advantages_:
     1. Increased concurrency can _improve program performance_.
        Instead of executing the entire program on a single processor, different
        subcomputations can be performed on separate processors, making the
        program run faster.

        

   - =TODO=

*** A brief overview of traditional concurrency - 15
    - There are
      + Operating system level concurrency
      + Programming language level concurrency

    - We'll focus mainly on *programming-language-level concurrency*.

    - synchronization :: the coordination of multiple executions in a concurrent
         system.

    - /synchronization/ is a key part in successfully *implementing* concurrency.

    - /Synchronization/
      + includes *mechanisms* used to order concurrent executions in time.

      + specifies *how concurrent executions communicate*, that is, how they
        exchange information.

    - Java uses shared memory.

      Its /synchronization/ is called /shared memory communication/.

      Establishing _an /order/ between_ the /threads/ *ensures* that the memory
      modifications done by one /thread/ are *visible* to a /thread/ that
      executes later.

    - The *crucial difference* lies in the fact that a /high-level concurrency/
      framework _expresses which goal to achieve_, RATHER THAN _how to achieve
      that goal_.

*** Modern concurrency paradigms - 15

** TODO The advantages of Scala - 17
** TODO Preliminaries - 18
*** Execution of a Scala program - 18
*** A Scala primer - 20

** TODO Overview of new features in Scala 2.12 - 25
** TODO Summary - 26
** TODO Exercises - 26
   =TODO=
   6, 7, 8, 9

* TODO Chapter 2: Concurrency on the JVM and the Java Memory Model - 29
  - Since Scala has run _primarily on top of JVM_, and this fact has *driven* the
    design of many of its _concurrency libraries_.
    * When we talk about /concurrency/ in Scala, we should know Scala inherits
      things from the JVM
      + /memory model/
      + /multithreading capabilities/
      + /inter-thread synchronization/

  - Most, if not all, /higher-level Scala concurrency constructs/
    *are implemented in terms of* the /low-level primitives/
    presented in this chapter.
    * In a way, the /APIs/ and /synchronization primitives/ in this chapter
      constitute the assembly of /concurrent programming on the JVM/.

  - In most cases,
    you should
    *avoid* /low-level concurrency constructs/
    *in place of* /higher-level constructs/
    =TODO= introduced later.
    * However, it is _IMPORTANT_ for you to understand
      + what a /thread/ is, that
      + a /guarded block/ is better than /busy-waiting/, =TODO=
      + _why a /memory model/ is useful_. =TODO=
      This is essential for a better understanding of /high-level concurrency
      abstractions/.

    * *In practice, all abstractions are to some extent leaky.*
      This is why you need to understand what are *behind* the /abstraction/.

  - In what follows, we =TODO=
    * not only *explain* _the CORNERSTONES of /concurrency on JVM/,_
    * but also *discuss* _HOW they *interact* with some /Scala-specific features/._

  - In particular, _we will cover the following topics in this chapter_: =TODO=
    * *Creating* and *starting* /threads/ and *waiting* for their completion

    * *Communication between* /threads/
      USING /object monitors/ and the /synchronized statement/ =???=

    * How to *avoid* /busy-waiting/ using /guarded blocks/ =???=

    * The /semantics/ of /volatile variables/ =???=

    * The specifics of the /Java Memory Model (JMM)/, and *why* the /JMM/ is
      _important_

  - In the following section, we will study how to use /threads/ -- the BASIC WAY
    to *express* /concurrent computations/.

** DONE Processes and threads - 30 - =TODO: NOTE=
   CLOSED: [2021-09-07 Tue 04:44]
   - In OS's of _MODERN_, /pre-emptive/, /multitasking/,
     the programmer has _LITTLE or NO control_ over the choice of processor on
     which the program will be executed --
     it is usually the task of the OS to
     *assign* executable parts of the program
     *to* SPECIFIC /processors/.

   - multitasking :: the /concurrent execution/ of MULTIPLE /tasks/ (also known as /processes/)
                     over a certain period of time. 
                     =from Jian= from Wikipedia

   - /Multitasking/ happens _transparently_ for the computer users.
     * =from Jian=
       Computer users can use computers without noticing the details of /multitasking/:
       The same program might
       + run on _MANY *different* processors_ during its execution
         AND
       + sometimes even *simultaneously* on several _processors_.

     * Historically,
       /Multitasking/ was introduced to OS's to _improve the user experience_ by
       allowing multiple users or programs to share resources of the same computer
       simutaneously.

   - In cooperative /multitasking/,
     * OLD solution (easy to be out of control):
       + *programs were able to decide*
         1. when to stop using the processor
            AND
         2. yield control to other programs.

       + _HOWEVER_,
         - this required a lot of discipline on the programmer's part
         - programs could easily give the impression of being *unresponsive*.

         *Blocking* the execution _UNTIL_ a non-short-term job complete often
         *ruin* the /user experience/.

     * Modern solution:
       _MOST_ OS's today _rely on_ /pre-emptive multitasking/, in which each
       program is *repetitively assigned* _slices of execution time (/time
       slices/)_ at a specific processor.

     Thus, /multitasking/ happens *transparently* for the application programmer
     as well as the user -- OS's do the control, not programmers.

   - The same computer program
     can be started _more than once_, or _even simultaneously_ within the same OS.

   - process :: an instance of a computer program that *is being executed*.
     1. When a /process/ starts,
        the OS *reserves*
        * a part of the /memory/ and
        * OTHER _computational resources_ and
         *associates* them *with* a _SPECIFIC computer program_.
     2. The OS then associates a /processor/ with this /process/, and this /process/
        executes during _ONE /time slice/._

     3. Eventually, the OS gives _OTHER_ /processes/ control over the /processor/.

   - Importantly, the /memory/ and _other computational resources_ of one /process/ are
     *ISOLATED* from the _other_ /processes/:
     * they CAN'T read each other's /memory/ _directly_
       or
     * they CAN'T _simultaneously_ use MOST of the resources.

   - For /multiple processes programs/,
     DIFFERENT /tasks/ within the program are expressed as _SEPARATE_ /processes/.
     Since SEPARATE /processes/ *cannot* access the SAME /memory/ areas
     directly, *it _can be CUMBERSOME_ to express /multitasking/ using MULTIPLE
     /processes/.*

   - /Multitasking/ was important long *BEFORE* recent years when /multicore
     computers/ became mainstream. Large programs such as _web browsers/ are the
     examples.

   - Large programs are divided into many logical modules.
     For example, web browsers:
     * A browser's _download manager_ downloads files *independent* of
       _rendering_ the web page or updating the HTML /Document Object Model
       (DOM)/.

     * *BUT*
       both INDEPENDENT computations (/threads/) occur as part of the *SAME*
       /process/.

   - threads :: independent computations occurring in the *SAME* /process/.

   - In a typical OS,
     there are *MANY MORE* /threads/ *than* /processors/.

   - Every /thread/ describes
     * the _current state_ of the /program stack/
       + program stack :: a sequence of /method invocations/ that are currently
                          being executed, along with the /local variables/ and
                          /method parameters of each method/.

     * the /program counter/ _DURING_ program execution
       + program counter :: describes the *position* of the CURRENT /instruction/
                             in the current /method/.

   - A /processor/ can _advance_ the computation in some /thread/
     by MANIPULATING
     * the /STATE/ of its /stack/
       or
     * the /STATE/ of the program objects

     and
     *executing* the /instruction/ at the /current program counter/.

   - When we say that
     _a /thread/ performs an action such as writing to a memory location_,
     we mean that
     _the /processor/ executing that /thread/ performs that action._

   - In /pre-emptive multitasking/, /thread/ execution is *scheduled by* the OS.
     * A programmer *must assume* that the /processor/ _time assigned_ to their
       /thread/ is *UNBIASED* towards other /threads/ in the system.

   - /OS threads/ are a programming facility _provided by_ the OS,
     usually exposed THROUGH an /OS-specific programming interface/.
     * *UNLIKE* separate /processes/,
       SEPARATE /OS threads/ _within_ the same /process/ *share*
       + a region of /memory/, and
       + *communicate* by _writing_ to and _reading_ parts of that /memory/.

   - /process/ :: (alternative definition)
                  a set of OS /threads/ along with the /memory/ and /resources/
                  *shared* by these /threads/.

   - We _turn our attention to_ see
     * HOW these concepts relate to the JVM,
       the runtime on top of which Scala programs execute.

   - _Starting_ a _NEW_ /JVM instance/
     ALWAYS CREATES *only one* /process/.
     * _WITHIN_ the JVM /process/, MULTIPLE /threads/ can run simultaneously.
       The JVM represents its /threads/ with the ~java.lang.Thread~ /class/.

   - *UNLIKE* /runtimes/ for languages such as Python,
     the JVM _does *NOT* implement its custom /threads/._
     INSTEAD, each /Java thread/ is _directly mapped to_ an /OS thread/.

     * This means that Java /threads/
       + behave in a _very similar_ way to the /OS threads/
       + the JVM _depends on_ the OS and its RESTRICTIONS.

   - Scala is a programming language that is by default compiled to the JVM
     bytecode, and the Scala compiler output is largely equivalent to that of
     Java from the JVM's perspective. This allows Scala programs to
     transparently call Java libraries, and in some cases, even vice versa.

   - Scala *reuses* the /threading API/ from Java for _several REASONS_:
     * Scala can _transparently_ *interact with* the existing /Java thread model/,
       which is already sufficiently comprehensive.

     * it is useful to *retain* the same /threading API/ *for compatibility
       reasons*, and _there is *NOTHING* fundamentally new_ that Scala can
       introduce with respect to the /Java thread API/.

   - =TODO=
     The rest of this chapter shows
     * HOW to *create* /JVM threads/ using Scala
     * HOW they can be *executed*
     * HOW they can *communicate*.

   - We will show and discuss several concrete examples.
     Java aficionados, already well-versed in this subject, might choose to *SKIP*
     the rest of this chapter.

*** DONE Creating and starting threads - 33
    CLOSED: [2021-09-06 Mon 00:43]
    - Every time a new /JVM process/ starts,
      *it creates several /threads/ _by default_.*

    - The most important /thread/ among them is *the /main thread/,*
      which executes the ~main~ /method/ of the Scala program.

    - We will show this in the following program, which _gets the NAME of the
      CURRENT /thread/_ and prints it to the standard output:
      #+BEGIN_SRC scala
        object ThreadsMain extends App {
          val t: Thread = Thread.currentThread
          val name = t.getName
          println(s"I am the thread $name")
        }
      #+END_SRC
      * If you run this program directly, you can see
        =[info] I am the thread main=

      * If you run this program *in SBT*, BY DEFAULT, you can see
        =[info] I am the thread run-main-0=
        + By default (~fork := false~),
          SBT started this program *INSIDE its /process/, on a SEPARATE /thread/.*

        + To ensure that the program runs *INSIDE a SEPARATE JVM /process/,*
          type ~set fork := true~ in SBT console or add ~fork := true~ to the
          project =build.sbt=, and then you can see:
          =[info] I am the thread main=

    - EVERY /thread/ *goes through* several /thread states/ during its existence.
      1. When a ~Thread~ object is *created*, it is initially in _the *NEW* state_.
      2. After the newly created /thread/ object *starts executing*, it goes into
         _the *runnable* state_.
      3. After the /thread/ is *done* executing, the /thread/ object goes into
         _the *terminated* state_, and _CANNOT execute anymore_.

    - Starting an independent /thread/ of computation consists of
      *TWO steps*:
      1. *Create* a ~Thread~ /object/
         to *allocate* the /memory/ for the /stack/ and /thread state/.

      2. Call the ~start~ /method/ to _start the computation_.

    - ~ThreadsCreation~:
      #+BEGIN_SRC scala
        object ThreadsCreation extends App {
          class MyThread extends Thread {
            override def run(): Unit = {
              println("New thread running.")
            }
          }

          val t = new MyThread
          t.start()
          t.join()
          println("New thread joined.")
        }
      #+END_SRC
      A JVM application starts and creates the /main thread/ to execute the
      /method/ call ~main~ from a specified /class/, in this case, the
      ~ThreadsCreation~ /object/.
      
      In this example, the /main thread/
      1. Creates another /thread/ of the ~MyThread~ type
         AND
         assigns it to ~t~.

      2. Starts ~t~ by calling the ~start~ /method/.
         1) Calling the ~start~ /method/ eventually
            *results in* executing the ~run~ /method/ from _the NEW /thread/._

         2) The OS is notified that ~t~ *MUST start executing*.
            * *NOT* start executing IMMEDIATELY!

         3) When the OS decides to assign _the NEW /thread/_ to some /processor/
            is largely *out of the programmer's control*,
            BUT the OS must *ensure* that this _eventually happens._

         4) After the /main thread/ *starts* the _NEW /thread/ ~t~,_
            it calls the ~join~ /method/ of ~t~. This /method/
            *halts* the execution of the /main thread/
            *until* ~t~ _completes_ its execution.

            * The ~join~ operation
              *puts* the /main thread/ into the waiting state
              *until* ~t~ terminates.

              + Importantly,
                _the WAITING /thread/_ *relinquishes* its control over the /processor/, and
                the OS can *assign* that /processor/ *to* _some OTHER /thread/._
                - /Waiting threads/
                  * *notify* the OS that they
                    are *waiting* for some _CONDITION_
                    AND
                    *cease spending* /CPU cycles/,

                  * INSTEAD of *repetitively checking* that _CONDITION_.

    - In the meantime, the OS
      1. *finds* an AVAILABLE /processor/ and
      2. *instructs* it to run the /CHILD thread/.
         * The /instructions/ that a /thread/ must execute are
           specified by _overriding_ its ~run~ /method/.

    - The ~t~ /instance/ of the ~MyThread~ class
      1. starts by printing the =“New thread running."= text to the /standard output/ and
      2. then terminates.

    - At this point,
      1. the OS is notified that ~t~ *is terminated* and
         eventually lets the /main thread/ *continue the execution*.

      2. The OS then puts the /main thread/ back into the /running state/, and
         the /main thread/ prints ="New thread joined."=.
         
    - The two outputs ="New thread running."= and ="New thread joined."=
      are always printed in this order.
      * The ~join~ call ENSURES that
        the *termination* of the ~t~ /thread/
        _occurs BEFORE_
        the /instructions/ following the ~join~ call.
        
    - When running the program, it is executed so fast that the two ~println~
      statements occur almost simultaneously.

      * Q :: Could it be that the ordering of the ~println~ statements is just
             an artifact in how the OS chooses to execute these threads?

      * A :: To verify the hypothesis that
             the /main thread/ really waits for ~t~ and that the output is not
             just because the OS is biased to execute ~t~ first in this particular
             example, we can experiment by tweaking the execution schedule.
             =TODO=

    - Before we do that,
      we will introduce a _shorthand_ to *create* and *start* a _NEW_ /thread/;
      the current syntax is too verbose!
      * The new /thread/ method simply
        runs a block of code in a newly started /thread/.
          This time, we will create the new /thread/ using an /anonymous thread
        class/ declared inline at the instantiation site:
        #+begin_src scala
          def thread(body: => Unit): Thread = {
            val t = new Thread {
              override def run() = body
            }
            t.start()
            t
          }
        #+end_src
        Think twice before using this ~thread~ statement in production projects.
        + It is prudent to correlate the syntactic burden with the computational cost;
        + lightweight syntax can be mistaken for a cheap operation and
          creating a new thread is *relatively expensive*.
      
    - /deterministic/
    - /nondeterministic/
      
*** DONE Atomic execution - 38
    CLOSED: [2021-09-07 Tue 03:52]
    - /Race condition/ example:
      #+begin_src scala
        object ThreadsUnprotectedUid extends App {
          var uidCount = 0L
        
          def getUniqueId(): Long = {
            val freshUid = uidCount + 1
            uidCount = freshUid
            freshUid
          }
        
          def printUniqueIds(n: Int): Unit = {
            val uids = for (i <- 0 until n) yield getUniqueId()
            log(s"Generated uids: $uids")
          }
        
          val t = thread { printUniqueIds(5) }
          printUniqueIds(5)
          t.join()
        }
      #+end_src

    - A race condition is not necessarily an incorrect program behavior.
      * However,
        if SOME /execution schedule/ CAUSES an *UNDESIRED program output*,
        the /race condition/ is considered to be a _program error_.

    - The ~synchronized~ can be called on ANY /object/.
      * To make ~getUniqueId()~ atomic, we can
        #+begin_src scala
          def getUniqueId(): Long = this.synchronized {
            val freshUid = uidCount + 1
            uidCount = freshUid
            freshUid
          }
        #+end_src

      * The ~synchronized~ call _ENSURES_ that
        the subsequent block of code can ONLY execute
        _IF_ there is no
          other /thread/ simultaneously executing this /synchronized block/ of code,
          _OR_
          any other /synchronized block/ of code called on the same this /object/.

      * *DISCOURAGED*:
        Omit the ~this~, from which /object/ the ~synchronized~ comes, in this
        kind of example.
        + It's better to always write down where a specific ~synchronized~ come from,
          BECAUSE _synchronizing on incorrect objects_ results in concurrency
          errors that are *NOT easily identified*.
          
    - The JVM _ENSURES_ that
        the /thread/ executing
        a /synchronized block/ invoked on some ~x~ /object/
        is the *only* /thread/ executing any /synchronized block/ on that
        PARTICULAR ~x~ /object/.

      * Example:
        A /thread/ _T_ will be put into the *blocked* state
        IF, when it calls ~x.synchronized {...}~, another /thread/ _S_ has
            already been running this /synchronized block/, 
        THEN
           the _T_ /thread/ is put into *the /blocked/ state*.
           
        + Once the _S_ /thread/ completes its /synchronized block/, the JVM can
          choose the _T_ /thread/ to execute its own /synchronized block/.
      
    - *EVERY* /object/ created inside the /JVM/ has a special entity called an
      /intrinsic lock/ or a /monitor/, which is used to ensure that *ONLY one*
      /thread/ is executing some /synchronized block/ on that /object/.
      1. When a /thread/ *starts executing* a /synchronized block/,
         it /ACQUIRE/ the /intrinsic lock/ (or called /monitor/) of this
         /synchronized block/.

      2. When a /thread/ *completes* the /synchronized block/,
         it /RELEASES/ the /intrinsic lock/.
        
    - The ~synchronized~ statement is one of the *fundamental mechanisms* for
      /inter-thread communication/ in Scala and on the JVM.

*** TODO Reordering - 42
    =TODO=
    - JMM
    
    - example

** DONE Monitors and synchronization - 45
   CLOSED: [2021-09-11 Sat 20:54]
   In this section, we will study /inter-thread communication/ using the /synchronized
   statement/ in more detail.

   - As we saw in the previous sections,
     the /synchronized statement/ serves both
     * to *ensure the visibility* of writes performed _by DIFFERENT /threads/,_ and
     * to *limit concurrent access* to a _shared region of memory_.

   - Lock ::
     * Generally speaking, it is a /synchronization mechanism/ that *enforces
       access limits on a shared resource*.

   - /Locks/ are also used to ensure that *NO* two /threads/ execute the same code
     simultaneously; that is, they implement /mutual exclusion/.
     
   - As mentioned previously,
     1. *EACH* /object/ on the JVM has a special /built-in monitor lock/,
        also called the /intrinsic lock/.
        1) WHEN a /thread/ calls the /synchronized statement/ on an ~x~ /object/,
           it *gains* _ownership of the monitor lock_ of the ~x~ object,
           GIVEN that no other /thread/ owns the monitor.

        2) OTHERWISE, the /thread/ is *blocked* until the monitor is released.

     2. UPON gaining _ownership of the monitor_,
        the /thread/ can _WITNESS_
        the *memory writes* of ALL the /threads/ that *PREVIOUSLY released* that
        monitor.
     
     3. A natural consequence is that /synchronized statements/ _can be *NESTED*._
        A /thread/ can own monitors belonging to several different objects
        SIMULTANEOUSLY.
          This is useful when composing larger systems from simpler components.
        We do not know which sets of monitors independent software components
        use in advance.
        * Example:
          Design *an online banking system* in which we want to *log* money
          transfers. Additional requirement: if a money transfer is bigger than
          10 currency units, we need to log it.
          #+begin_src scala
            object SynchronizedNesting extends App {
              import scala.collection.mutable
            
              private val transfers = mutable.ArrayBuffer.empty[String]
            
              def logTransfer(name: String, n: Int) = transfers.synchronized {
                transfers += s"transfer to account '$name' = $n"
              }
            
              class Account(val name: String, var money: Int)
            
              def add(account: Account, n: Int) = account.synchronized {
                account.money += n
                if (n > 10) logTransfer(account.name, n)
              }
            
              // Continuation of the bank account example.
              val jane = new Account("Jane", 100)
              val john = new Account("John", 200)
              val t1 = thread { add(jane, 5) }
              val t2 = thread { add(john, 50) }
              val t3 = thread { add(jane, 70) }
              t1.join(); t2.join(); j3.join()
              log(s"--- transfers ---\n$transfers")
            }
          #+end_src
          + We can maintain the transfers list of all the money transfers in a
            ~mutable.ArrayBuffer~, the ~val transfers~.
            - The banking application does *NOT manipulate transfers DIRECTLY,* 
              BUT instead appends new messages with a ~logTransfer~ method that
              calls ~synchronized~ on ~transfers~.
              * The ~ArrayBuffer~ implementation is a collection designed *for
                single-threaded use* (=from Jian= not thread-safe), so we need to
                protect it from concurrent writes.

*** DONE Deadlocks - 47
    CLOSED: [2021-09-12 Sun 00:34]
    - A factor that worked to our advantage in the banking system example is that
      the ~logTransfer~ method never attempts to acquire any /monitors/ other
      than /the ~transfers~ monitor/.
        Once the /monitor/ is obtained, a /thread/ will eventually modify the
      ~transfers~ buffer and release the /monitor/; in a stack of _NESTED_ /monitor/
      acquisitions, transfers always comes last.
        Given that ~logTransfer~ is the only method synchronizing on
      ~transfers~, it cannot indefinitely delay other /threads/ that synchronize
      on ~transfers~.
      
    - /Deadlock :: two or more executions wait for each other to complete an action
                   BEFORE proceeding with their own action.
      
    - _The ~logTransfer~ method *can never cause* a /deadlock/,_
      because it only attempts to acquire a single /monitor/ that is released
      eventually.

    - Let's now extend our banking example to
      allow money transfers _BETWEEN specific accounts,_ as follows:
      #+begin_src scala
        object SynchronizedDeadlock extends App {
          import SynchronizedNesting.Account
        
          def send(a: Account, b: Account, n: Int) = a.synchronized {
            b.synchronized {
              a.money -= n
              b.money += n
            }
          }
        
          val a = new Account("Jack", 1000)
          val b = new Account("Jill", 2000)
          val t1 = thread { for (_ <- 0 until 100) send(a, b, 1) }
          val t2 = thread { for (_ <- 0 until 100) send(b, a, 1) }
          t1.join(); t2.join()
          log(s"a = ${a.money}, b = ${b.money}")
        }
      #+end_src
      * A /deadlock/ occurs when a set of two or more /threads/ acquire resources
        and then *CYCLICALLY try* to acquire other /thread/'s resources *WITHOUT
        releasing* their own.
      
      * If you are running this example, you'll want to close the terminal session
        at this point and restart SBT.
        
    - Whenever resources are acquired in the same order, there is no danger of a
      /deadlock/.
      * *The _ORDERING_ breaks the cycle,* which is one of the necessary
        preconditions for a /deadlock/.

      * _TIP_:
        *Establish a /total order/ between resources when acquiring them;*
        this ensures that no set of /threads/ _cyclically wait_ on the resources
        they previously acquired.
        
    - In our example, we need to establish an order between different accounts.
      One way of doing so is to use the ~getUniqueId~ method introduced in an
      earlier section:
      #+begin_src scala
        import SynchronizedProtectedUid.getUniqueId
        
        class Account(val name: String, var money: Int) {
          val uid = getUniqueId()
        }
      #+end_src
      This new ~Account~ class *ensures* that
      no two accounts share the SAME ~uid~ value, regardless of the /thread/
      they were created on.

    - The /deadlock-free/ ~send~ method then needs to
      ACQUIRE the accounts *in the order of their ~uid~ values*, as follows:
      #+begin_src scala
        def send(a1: Account, a2: Account, n: Int) {
          def adjust(): Unit = {
            a1.money -= n
            a2.money += n
          }
        
          if (a1.uid < a2.uid)
            a1.synchronized {
              a2.synchronized {
                adjust()
              }
            }
          else
            a1.synchronized {
              a2.synchronized {
                adjust()
              }
            }
        
        }
      #+end_src
      
    - /Deadlocks/ are *inherent* to any concurrent system in which the /threads/
      wait indefinitely for a resource without releasing the resources they
      previously acquired.
      * However, while they should be avoided, /deadlocks/ are often not as deadly
        as they sound:
        =IMPORTANT=
        A nice thing about /deadlocks/ is that by their definition, a /deadlocked/
        system *does not progress*.

        + The developer that resolved Jack and Jill's issue was able to act
          quickly by doing a /heap dump/ of the _running JVM instance_ and
          analyzing the /thread stacks/; /deadlocks/ can at least be easily
          identified, even when they occur in a production system.
          =TODO= =TODO= =TODO=
          Learn how to do /heap dump/ and /thread stacks/ analysis!!!

        + This is unlike the errors due to /race conditions/, which only become
          apparent long after the system transitions into an invalid state.
      
*** TODO Guarded blocks - 50
    - _Creating a new /thread/_ is *much more expensive than* _creating a new
      lightweight object such as ~Account~._

    - A high-performance banking system should be _QUICK_ and _RESPONSIVE_, and
      creating a new /thread/ on each request can be *TOO SLOW* when there are
      thousands of requests per second.
      * *CONCLUSION*:
        The same /thread/ should be *reused* for many requests;
        
      * thread pool :: a set of such /reusable threads/.

    - xx
      #+begin_src scala
        object SynchronizedBadPool extends App {
          import scala.collection.mutable
        
          private val tasks = mutable.Queue.empty[() => Unit]
        
          val worker: Thread = new Thread {
            def poll(): Option[() => Unit] = tasks.synchronized {
              Option.when(tasks.nonEmpty)(tasks.dequeue())
            }
        
            override def run(): Unit =
              while (true)
                poll() match {
                  case Some(task) => task()
                  case None =>
                }
          }
        
          worker.setDaemon(true)
          worker.start()
        
          def asynchronous(body: =>Unit): Unit = tasks.synchronized {
            tasks.enqueue(() => body)
            ()
          }
        
          asynchronous { log("Hello") }
          asynchronous { log(" world!")}
          Thread.sleep(5000)
        }
      #+end_src

*** TODO Interrupting threads and the graceful shutdown - 55

** TODO Volatile variables - 56
** TODO The Java Memory Model - 58
*** TODO Immutable objects and final fields - 60

** TODO Summary - 62
** TODO Exercises - 63

* TODO Chapter 3: Traditional Building Blocks of Concurrency - 67
** The Executor and ExecutionContext objects - 68
** Atomic primitives - 72
*** Atomic variables - 73
*** Lock-free programming - 76
*** Implementing locks explicitly - 78
*** The ABA problem - 80

** Lazy values - 83
** Concurrent collections - 88
*** Concurrent queues - 89
*** Concurrent sets and maps - 93
*** Concurrent traversals - 98

** Custom concurrent data structures - 101
*** Implementing a lock-free concurrent pool - 102
*** Creating and handling processes - 106

** Summary - 108
** Exercises - 109

* TODO Chapter 4: Asynchronous Programming with Futures and Promises - 112
** Futures - 113
*** Starting future computations - 115
*** Future callbacks - 117
*** Futures and exceptions - 120
*** Using the Try type - 121
*** Fatal exceptions - 123
*** Functional composition on futures - 124

** Promises - 132
*** Converting callback-based APIs - 134
*** Extending the future API - 137
*** Cancellation of asynchronous computations - 138

** Futures and blocking - 141
*** Awaiting futures - 141
*** Blocking in asynchronous computations - 142

** The Scala Async library - 143
** Alternative future frameworks - 146
** Summary - 148
** Exercises - 148

* TODO Chapter 5: Data-Parallel Collections - 152
** Scala collections in a nutshell - 153
** Using parallel collections - 154
*** Parallel collection class hierarchy - 158
*** Configuring the parallelism level - 160
*** Measuring the performance on the JVM - 161

** Caveats with parallel collections - 164
*** Non-parallelizable collections - 164
*** Non-parallelizable operations - 165
*** Side effects in parallel operations - 168
*** Nondeterministic parallel operations - 169
*** Commutative and associative operators - 170

** Using parallel and concurrent collections together - 173
*** Weakly consistent iterators - 174

** Implementing custom parallel collections - 175
*** Splitters - 176
*** Combiners - 179

** Summary - 182
** Exercises - 184

* TODO Chapter 6: Concurrent Programming with Reactive Extensions - 186
** Creating Observable objects - 188
*** Observables and exceptions - 190
*** The Observable contract - 192
*** Implementing custom Observable objects - 194
*** Creating Observables from futures - 195
*** Subscriptions - 196

** Composing Observable objects - 199
*** Nested Observables - 201
*** Failure handling in Observables - 206

** Rx schedulers - 209
*** Using custom schedulers for UI applications - 211

** Subjects and top-down reactive programming - 218
** Summary - 223
** Exercises - 223

* TODO Chapter 7: Software Transactional Memory - 227
** The trouble with atomic variables - 228
** Using Software Transactional Memory - 232
*** Transactional references - 235
*** Using the atomic statement - 236

** Composing transactions - 238
*** The interaction between transactions and side effects - 238
*** Single-operation transactions - 243
*** Nesting transactions - 244
*** Transactions and exceptions - 247

** Retrying transactions - 252
*** Retrying with timeouts - 256

** Transactional collections - 258
*** Transaction-local variables - 258
*** Transactional arrays - 259
*** Transactional maps - 261

** Summary - 263
** Exercises - 264

* TODO Chapter 8: Actors - 267
** Working with actors - 268
*** Creating actor systems and actors - 271
*** Managing unhandled messages - 274
*** Actor behavior and state - 276
*** Akka actor hierarchy - 282
*** Identifying actors - 285
*** The actor lifecycle - 288

** Communication between actors - 292
*** The ask pattern - 294
*** The forward pattern - 297
*** Stopping actors - 298

** Actor supervision - 300
** Remote actors - 306
** Summary - 310
** Exercises - 310

* TODO Chapter 9: Concurrency in Practice - 313
** Choosing the right tools for the job - 314
** Putting it all together – a remote file browser - 319
*** Modeling the filesystem - 320
*** The server interface - 324
*** Client navigation API - 326
*** The client user interface - 330
*** Implementing the client logic - 334
*** Improving the remote file browser - 339

** Debugging concurrent programs - 340
*** Deadlocks and lack of progress - 341
*** Debugging incorrect program outputs - 346
*** Performance debugging - 351

** Summary - 358
** Exercises - 359

* TODO Chapter 10: Reactors - 361
** The need for reactors - 362
** Getting started with Reactors - 364
** The “Hello World” program - 364
** Event streams - 366
*** Lifecycle of an event stream - 367
*** Functional composition of event streams - 369

** Reactors - 371
*** Defining and configuring reactors - 373
*** Using channels - 374

** Schedulers - 377
** Reactor lifecycle - 378
** Reactor system services - 381
*** The logging service - 381
*** The clock service - 382
*** The channels service - 383
*** Custom services - 384

** Protocols - 387
*** Custom server-client protocol - 387
*** Standard server-client protocol - 390
**** Using an existing connector - 391
**** Creating a new connector - 391
**** Creating a protocol-specific reactor prototype - 392
**** Spawning a protocol-specific reactor directly - 393

*** Router protocol - 393
*** Two-way protocol - 395
** Summary - 399
** Exercises - 399

* Index - 402
