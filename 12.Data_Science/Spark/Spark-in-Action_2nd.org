#+TITLE: Spark in Action
#+SUBTITLE: With examples in Java, Python, and Scala
#+VERSION: 2020, 2nd, Covers Apache Spark 3
#+AUTHOR: Jean-Georges Perrin
#+FOREWORD-BY: Rob Thomas
#+STARTUP: overview
#+STARTUP: entitiespretty

* foreword xiii
* preface xv
* acknowledgments xvii
* about this book xix
* about the author xxv
* about the cover illustration xxvi
* PART 1 THE THEORY CRIPPLED BY AWESOME EXAMPLES - 1
** 1 So, what is Spark, anyway? - 3
*** 1.1 The big picture: What Spark is and what it does - 4
**** What is Spark? - 4
**** The four pillars of mana - 6
     
*** 1.2 How can you use Spark? - 8
**** Spark in a data processing/engineering scenario - 8
**** Spark in a data science scenario - 9
    
*** 1.3 What can you do with Spark? - 10
**** Spark predicts restaurant quality at NC eateries - 11
**** Spark allows fast data transfer for Lumeris - 11
**** Spark analyzes equipment logs for CERN - 12
**** Other use cases - 12
     
*** 1.4 Why you will love the dataframe - 12
**** The dataframe from a Java perspective - 13
**** The dataframe from an RDBMS perspective - 13
**** A graphical representation of the dataframe - 14

*** 1.5 Your first example - 14
**** Recommended software - 15
**** Downloading the code - 15
**** Running your first application - 15
**** Your first code - 17

** 2 Architecture and flow - 19
*** 2.1 Building your mental model - 20
*** 2.2 Using Java code to build your mental model - 21
*** 2.3 Walking through your application - 23
**** Connecting to a master - 24
**** Loading, or ingesting, the CSV file - 25
**** Transforming your data - 28
**** Saving the work done in your dataframe to a database - 29

** 3 The majestic role of the dataframe - 33
*** 3.1 The essential role of the dataframe in Spark - 34
**** Organization of a dataframe - 35
**** Immutability is not a swear word - 36
     
*** 3.2 Using dataframes through examples - 37
**** A dataframe after a simple CSV ingestion - 39
**** Data is stored in partitions - 44
**** Digging in the schema - 45
**** A dataframe after a JSON ingestion - 46
**** Combining two dataframes - 52
     
*** 3.3 The dataframe is a ~Dataset<Row>~ - 57
**** Reusing your POJOs - 58
**** Creating a dataset of strings - 59
**** Converting back and forth - 60
     
*** 3.4 ~Dataframe~'s ancestor: the ~RDD~ - 66
    
** 4 Fundamentally lazy - 68
*** 4.1 A real-life example of efficient laziness 69
*** 4.2 A Spark example of efficient laziness - 70
**** Looking at the results of transformations and actions - 70
**** The transformation process, step by step - 72
**** The code behind the transformation/action process - 74
**** The mystery behind the creation of 7 million datapoints in 182 ms - 77
**** The mystery behind the timing of actions - 79
     
*** 4.3 Comparing to RDBMS and traditional applications - 83
**** Working with the teen birth rates dataset - 83
**** Analyzing differences between a traditional app and a Spark app - 84
     
*** 4.4 Spark is amazing for data-focused applications - 86
*** 4.5 Catalyst is your app catalyzer - 86

** 5 Building a simple app for deployment - 90
*** 5.1 An ingestionless example - 91
**** Calculating p - 91
**** The code to approximate p - 93
**** What are lambda functions in Java? - 99
**** Approximating p by using lambda functions - 101
     
*** 5.2 Interacting with Spark - 102
**** Local mode - 103
**** Cluster mode - 104
**** Interactive mode in Scala and Python - 107
     
** 6 Deploying your simple app - 114
*** 6.1 Beyond the example: The role of the components - 116
**** Quick overview of the components and their interactions - 116
**** Troubleshooting tips for the Spark architecture - 120
**** Going further - 121
    
*** 6.2 Building a cluster - 121
**** Building a cluster that works for you - 122
**** Setting up the environment - 123
    
*** 6.3 Building your application to run on the cluster - 126
**** Building your application’s uber JAR - 127
**** Building your application by using Git and Maven - 129
     
*** 6.4 Running your application on the cluster - 132
**** Submitting the uber JAR - 132
**** Running the application - 133
**** Analyzing the Spark user interface - 133

* TODO PART 2 INGESTION - 137
* TODO PART 3 TRANSFORMING YOUR DATA - 245
* PART 4 GOING FURTHER - 345
** 16 Cache and checkpoint: Enhancing Spark’s performances - 347
*** 16.1 Caching and checkpointing can increase performance - 348
**** The usefulness of Spark caching - 350
**** The subtle effectiveness of Spark checkpointing - 351
**** Using caching and checkpointing - 352
     
*** 16.2 Caching in action - 361
*** 16.3 Going further in performance optimization - 371
    
** 17 Exporting data and building full data pipelines - 373
*** 17.1 Exporting data - 374
**** Building a pipeline with NASA datasets - 374
**** Transforming columns to datetime - 378
**** Transforming the confidence percentage to confidence level - 379
**** Exporting the data - 379
**** Exporting the data: What really happened? - 382
     
*** 17.2 Delta Lake: Enjoying a database close to your system - 383
**** Understanding why a database is needed - 384
**** Using Delta Lake in your data pipeline - 385
**** Consuming data from Delta Lake - 389
     
*** 17.3 Accessing cloud storage services from Spark - 392
    
** 18 Exploring deployment constraints: Understanding the ecosystem - 395
*** 18.1 Managing resources with YARN, Mesos, and Kubernetes - 396
**** The built-in standalone mode manages resources - 397
**** YARN manages resources in a Hadoop environment - 398
**** Mesos is a standalone resource manager - 399
**** Kubernetes orchestrates containers - 401
**** Choosing the right resource manager - 402
     
*** 18.2 Sharing files with Spark - 403
**** Accessing the data contained in files - 404
**** Sharing files through distributed filesystems - 404
**** Accessing files on shared drives or file server - 405
**** Using file-sharing services to distribute files - 406
**** Other options for accessing files in Spark - 407
**** Hybrid solution for sharing files with Spark - 408
     
*** 18.3 Making sure your Spark application is secure - 408
**** Securing the network components of your infrastructure - 408
**** Securing Spark’s disk usage - 409
  
* appendix A Installing Eclipse 411
* appendix B Installing Maven 418
* appendix C Installing Git 422
* appendix D Downloading the code and getting started with Eclipse 424
* appendix E A history of enterprise data 430
* appendix F Getting help with relational databases 434
* appendix G Static functions ease your transformations 438
* appendix H Maven quick cheat sheet 446
* appendix I Reference for transformations and actions 450
* appendix J Enough Scala 460
* appendix K Installing Spark in production and a few tips 462
* appendix L Reference for ingestion 476
* appendix M Reference for joins 488
* appendix N Installing Elasticsearch and sample data 499
* appendix O Generating streaming data 505
* appendix P Reference for streaming 510
* appendix Q Reference for exporting data 520
* appendix R Finding help when you’re stuck 528
* index 533
