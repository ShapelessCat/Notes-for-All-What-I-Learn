#+TITLE: Learning Spark
#+SUBTITLE: Lightning-Fast Data Analysis
#+VERSION: 2nd, Covers Apache Spark 3.0
#+YEAR: 2020
#+AUTHOR: Jules S. Damji, Brooke Wenig, Tathagata Das & Denny Lee
#+Foreword by: Matei Zaharia
#+STARTUP: overview
#+STARTUP: entitiespretty

* Foreword - xiii
* TODO Preface - xv
  - This new edition has been updated to reflect Apache Spark's *evolution through
    Spark 2.x and Spark 3.0*, including its expanded ecosystem of built-in and
    external data sources, machine learning, and streaming technologies with which
    Spark is tightly integrated.

** Who This Book Is For
** How the Book Is Organized
** How to Use the Code Examples
** Software and Configuration Used
** Conventions Used in This Book
** Using Code Examples
** O'Reilly Online Learning
** How to Contact Us
** Acknowledgments

* TODO 1. Introduction to Apache Spark: A Unified Analytics Engine - 1
  This chapter
  - lays out the _origins_ of Apache Spark and its _underlying philosophy_.
  - Surveys the _main components_ of the project and its _distributed architecture_.

** The Genesis of Spark - 1
   In this section, we'll chart the course of Apache Spark's short evolution:
   its genesis, inspiration, and adoption in the community as a de facto big
   data unified processing engine.

*** Big Data and Distributed Computing at Google - 1
*** Hadoop at Yahoo! - 2
*** Spark's Early Years at AMPLab - 3

** What Is Apache Spark? - 4
   - Spark provides in-memory storage for intermediate computations, making it
     much faster than Hadoop MapReduce.

   - It incorporates libraries with composable APIs for
     + machine learning (MLlib)
     + SQL for interactive queries (Spark SQL)
     + stream processing (Structured Streaming) for interacting with real-time data,
     + and graph processing (GraphX).

   - Spark's design philosophy centers around four key characteristics:
     + Speed
     + Ease of use
     + Modulatirty =???=
     + Extensibility =???=

*** TODO Speed - 4
*** DONE Ease of Use - 5
    CLOSED: [2020-09-23 Wed 22:20]
    Spark achieves simplicity by providing a fundamental abstraction of a simple
    logical data structure called a /Resilient Distributed Dataset (RDD)/ upon
    which all other higher-level structured data abstractions, such as ~DataFrame~'s
    and ~Dataset~'s, are constructed.
    - By providing a set of _transformations_ and _actions_ as operations, Spark
      offers a simple programming model that you can use to build big data
      applications in familiar languages.

*** TODO Modularity - 5
*** TODO Extensibility - 5

** Unified Analytics - 6
*** Apache Spark Components as a Unified Stack - 6
**** Spark SQL - 7
**** Spark MLlib - 7
**** Spark Structured Streaming - 8
**** GraphX

*** Apache Spark's Distributed Execution - 10
**** Spark driver - 10
**** SparkSession - 11
**** Cluster manager - 12
**** Spark executor - 12
**** Deployment modes - 12
**** Distributed data and partitions - 12

** The Developer's Experience - 14
*** Who Uses Spark, and for What? - 14
**** Data science tasks - 14
**** Data engineering tasks - 15
**** Popular Spark use cases - 16

*** Community Adoption and Expansion - 16

* TODO 2. Downloading Apache Spark and Getting Started - 19
** Step 1: Downloading Apache Spark - 19
*** Spark's Directories and Files - 21

** Step 2: Using the Scala or PySpark Shell - 22
*** Using the Local Machine - 23

** Step 3: Understanding Spark Application Concepts - 25
*** Spark Application and SparkSession - 26
*** Spark Jobs - 27
*** Spark Stages - 28
*** Spark Tasks - 28

** Transformations, Actions, and Lazy Evaluation - 28
*** Narrow and Wide Transformations - 30

** The Spark UI - 31
   - *Databricks Community Edition*

** Your First Standalone Application - 34
*** Counting M&Ms for the Cookie Monster - 35
*** Building Standalone Applications in Scala - 40

** Summary - 42

* TODO 3. Apache Spark's Structured APIs - 43
** Spark: Whatâ€™s Underneath an RDD? - 43
** Structuring Spark - 44
*** Key Merits and Benefits - 45

** The DataFrame API - 47
*** Spark's Basic Data Types - 48
*** Spark's Structured and Complex Data Types - 49
*** Schemas and Creating DataFrames - 50
**** Two ways to define a schema - 51

*** Columns and Expressions - 54
*** Rows - 57
*** Common DataFrame Operations - 58
**** Using DataFrameReader and DataFrameWriter - 58
**** Transformations and actions - 61

*** End-to-End DataFrame Example - 68

** The Dataset API - 69
*** Typed Objects, Untyped Objects, and Generic Rows - 69
*** Creating Datasets - 71
**** Scala: Case classes - 71

*** Dataset Operations - 72
*** End-to-End Dataset Example - 74

** DataFrames Versus Datasets - 74
*** When to Use RDDs - 75

** Spark SQL and the Underlying Engine - 76
*** The Catalyst Optimizer - 77
**** Phase 1: Analysis - 81
**** Phase 2: Logial optimization - 81
**** Phase 3: Physical planning - 81
**** Phase 4: Code generation - 81

** Summary - 82

* TODO 4. Spark SQL and DataFrames: Introduction to Built-in Data Sources - 83
** Using Spark SQL in Spark Applications - 84
*** Basic Query Examples - 85

** SQL Tables and Views - 89
*** Managed Versus UnmanagedTables - 89
*** Creating SQL Databases and Tables - 90
**** Creating a managed table - 90
**** Creating an unmanaged table - 91

*** Creating Views - 91
**** Temporary views versus global temporary views - 92

*** Viewing the Metadata - 93
*** Caching SQL Tables - 93
*** Reading Tables into DataFrames - 93

** Data Sources for DataFrames and SQL Tables - 94
*** DataFrameReader - 94
*** DataFrameWriter - 96
*** Parquet - 97
**** Reading Parquet files into a DataFrame - 97
**** Reading Parquet files into a Spark SQL table - 98
**** Writing DataFrames to Parquet files - 99
**** Writing DataFrames to Spark SQL tables - 99

*** JSON - 100
**** Reading a JSON file into a DataFrame - 100
**** Reading a JSON file into a Spark SQL table - 100
**** Writing DataFrames to JSON files - 101
**** JSON data source options - 101

*** CSV - 102
**** Reading a CSV file into a DataFrame - 102
**** Reading a CSV file into a Spark SQL table - 102
**** Writing DataFrames to CSV files - 103
**** CSV data source options - 103

*** Avro - 104
**** Reading an Avro file into a DataFrame - 104
**** Reading an Avro file into a Spark SQL table - 105
**** Writing DataFrames to Avro files - 105
**** Avro data source options - 106

*** ORC - 106
**** Reading an ORC file into a DataFrame - 107
**** Reading an ORC file into a Spark SQL table - 107
**** Writing DataFrames to ORC files - 108

*** Images - 108
**** Reading an image file into a DataFrame - 109

*** Binary Files - 110
**** Reading a binary file into a DataFrame - 110

** Summary - 111

* TODO 5. Spark SQL and DataFrames: Interacting with External Data Sources - 113
** Spark SQL and Apache Hive - 113
*** User-Defined Functions - 114
**** Spark SQL UDFs - 114
**** Evaluation order and null checking in Spark SQL - 115
**** Speeding up and distributing PySpark UDFs with Pandas UDFs - 115

** Querying with the Spark SQL Shell, Beeline, and Tableau - 119
*** Using the Spark SQL Shell - 119
**** Create a table - 119
**** Insert data into the table - 120
**** Running a Spark SQL query - 120

*** Working with Beeline - 120
**** Start the Thrift server - 121
**** Connect to the Thrift server via Beeline - 121
**** Execute a Spark SQL query with Beeline - 121
**** Stop the Thrift server - 122

*** Working with Tableau - 122
**** Start the Thrift server - 122
**** Start Tableau - 123
**** Stop the Thrift server - 129

** External Data Sources - 129
*** JDBC and SQL Databases - 129
**** The importance of partitioning - 130

*** PostgreSQL - 132
*** MySQL - 133
*** Azure Cosmos DB - 134
*** MS SQL Server - 136
*** Other External Sources - 137

** Higher-Order Functions in DataFrames and Spark SQL - 138
*** Option 1: Explode and Collect - 138
*** Option 2: User-Defined Function - 138
*** Built-in Functions for Complex Data Types - 139
*** Higher-Order Functions - 141
**** ~transform()~ - 142
**** ~filter()~ - 143
**** ~exists()~ - 143
**** ~reduce()~ - 144

** Common DataFrames and Spark SQL Operations - 144
*** Unions - 147
*** Joins - 148
*** Windowing - 149
*** Modifications - 151
**** Adding new columns - 152
**** Dropping columns - 152
**** Renaming columns - 153
**** Pivoting - 153
**** Summary - 155

** Summary - 155

* TODO 6. Spark SQL and Datasets - 157
** Single API for Java and Scala - 157
*** Scala Case Classes and JavaBeans for Datasets - 158

** Working with Datasets - 160
*** Creating Sample Data - 160
*** Transforming Sample Data - 162
**** Higher-order functions and functional programming - 162
**** Converting DataFrames to Datasets - 166

** Memory Management for Datasets and DataFrames - 167
** Dataset Encoders - 168
*** Spark's Internal Format Versus Java Object Format - 168
*** Serialization and Deserialization (SerDe) - 169

** Costs of Using Datasets - 170
*** Strategies to Mitigate Costs - 170

** Summary - 172

* TODO 7. Optimizing and Tuning Spark Applications - 173
** Optimizing and Tuning Spark for Efficiency - 173
*** Viewing and Setting Apache Spark Configurations - 173
*** Scaling Spark for Large Workloads - 177
**** Static versus dynamic resource allocation - 177
**** Configuring Spark executors' memory and the shuffle service - 178
**** Maximizing Spark parallelism - 180

** Caching and Persistence of Data - 183
*** ~DataFrame.cache()~ - 183
*** ~DataFrame.persist()~ - 184
*** When to Cache and Persist - 187
*** When Not to Cache and Persist - 187

** A Family of Spark Joins - 187
*** Broadcast Hash Join - 188
**** When to use a broadcast hash join - 189

*** Shuffle Sort Merge Join - 189
**** Optimizing the shuffle sort merge join - 193
**** When to use a shuffle sort merge join - 197

** Inspecting the Spark UI - 197
*** Journey Through the Spark UI Tabs - 197
**** Jobs and Stages - 198
**** Executors - 200
**** Storage - 200
**** SQL - 202
**** Environment - 203
**** Debugging Spark applications - 204

** Summary - 205

* TODO 8. Structured Streaming - 207
** Evolution of the Apache Spark Stream Processing Engine - 207
*** The Advent of Micro-Batch Stream Processing - 208
*** Lessons Learned from Spark Streaming (DStreams) - 209
*** The Philosophy of Structured Streaming - 210

** The Programming Model of Structured Streaming - 211
** The Fundamentals of a Structured Streaming Query - 213
*** Five Steps to Define a Streaming Query - 213
**** Step 1: Define input sources - 213
**** Step 2: Transform data - 214
**** Step 3: Define output sink and output mode - 215
**** Step 4: Specify processing details - 216
**** Step 5: Start the query - 218
**** Putting it all together - 218

*** Under the Hood of an Active Streaming Query - 219
*** Recovering from Failures with Exactly-Once Guarantees - 221
*** Monitoring an Active Query - 223
**** Querying current status using StreamingQuery - 223
**** Publishing metrics using Dropwizard Metrics - 224
**** Publishing metrics using custom StreamingQueryListeners - 225

** Streaming Data Sources and Sinks - 226
*** Files - 226
**** Reading from Files - 226
**** Writing from Files - 227

*** Apache Kafka - 228
**** Reading from Kafka - 228
**** Writing to Kafka - 229

*** Custom Streaming Sources and Sinks - 230
**** Writing to any storage system - 230
**** Reading to any storage system - 234

** Data Transformations - 234
*** Incremental Execution and Streaming State - 234
*** Stateless Transformations - 235
*** Stateful Transformations - 235
**** Distributed and fault-tolerant state management - 236
**** Types of stageful operations - 237

** Stateful Streaming Aggregations - 238
*** Aggregations Not Based on Time - 238
*** Aggregations with Event-Time Windows - 239
**** Handling late data with watermarks - 243
**** Supported output modes - 245

** Streaming Joins - 246
*** Streamâ€“Static Joins - 246
*** Streamâ€“Stream Joins - 248
**** Inner joins with optional watermarking - 248
**** Outer joins with watermarking - 252

** Arbitrary Stateful Computations - 253
*** Modeling Arbitrary Stateful Operations with ~mapGroupsWithState()~ - 254
*** Using Timeouts to Manage Inactive Groups - 257
**** Processing-time timeouts - 257
**** Event-time timeouts - 259

*** Generalization with ~flatMapGroupsWithState()~ - 261

** Performance Tuning - 262
** Summary - 264

* 9. Building Reliable Data Lakes with Apache Spark - 265
** The Importance of an Optimal Storage Solution - 265
** Databases - 266
*** A Brief Introduction to Databases - 266
*** Reading from and Writing to Databases Using Apache Spark - 267
*** Limitations of Databases - 267

** Data Lakes - 268
*** A Brief Introduction to Data Lakes - 268
*** Reading from and Writing to Data Lakes using Apache Spark - 269
*** Limitations of Data Lakes - 270

** Lakehouses: The Next Step in the Evolution of Storage Solutions - 271
*** Apache Hudi - 272
*** Apache Iceberg - 272
*** Delta Lake - 273

** Building Lakehouses with Apache Spark and Delta Lake - 274
*** Configuring Apache Spark with Delta Lake - 274
*** Loading Data into a Delta Lake Table - 275
*** Loading Data Streams into a Delta Lake Table - 277
*** Enforcing Schema on Write to Prevent Data Corruption - 278
*** Evolving Schemas to Accommodate Changing Data - 279
*** Transforming Existing Data - 279
*** Auditing Data Changes with Operation History - 282
*** Querying Previous Snapshots of a Table with Time Travel - 283

** Summary - 284

* 10. Machine Learning with MLlib - 285
** What Is Machine Learning? - 286
*** Supervised Learning - 286
*** Unsupervised Learning - 288
*** Why Spark for Machine Learning? - 289

** Designing Machine Learning Pipelines - 289
*** Data Ingestion and Exploration - 290
*** Creating Training and Test Data Sets - 291
*** Preparing Features with Transformers - 293
*** Understanding Linear Regression - 294
*** Using Estimators to Build Models - 295
*** Creating a Pipeline - 296
*** Evaluating Models - 302
*** Saving and Loading Models - 306

** Hyperparameter Tuning - 307
*** Tree-Based Models - 307
*** k-Fold Cross-Validation - 316
*** Optimizing Pipelines - 320

** Summary - 321

* 11. Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark - 323
** Model Management - 323
*** MLflow - 324

** Model Deployment Options with MLlib - 330
*** Batch - 332
*** Streaming - 333
*** Model Export Patterns for Real-Time Inference - 334

** Leveraging Spark for Non-MLlib Models - 336
*** Pandas UDFs - 336
*** Spark for Distributed Hyperparameter Tuning - 337

** Summary - 341

* TODO 12. Epilogue: Apache Spark 3.0 - 343
** Spark Core and Spark SQL - 343
*** Dynamic Partition Pruning - 343
*** Adaptive Query Execution - 345
**** The AQE framework - 346

*** SQL Join Hints - 348
**** Shuffle sort merge join (SMJ) - 348
**** Broadcast hash join (BHJ) - 348
**** Shuffle hash join (SHJ) - 348
**** Shuffle-and-replicate nested loop join (SNLJ) - 348

*** Catalog Plugin API and DataSourceV2 - 349
*** Accelerator-Aware Scheduler - 351

** Structured Streaming - 352
** PySpark, Pandas UDFs, and Pandas Function APIs - 354
*** Redesigned Pandas UDFs with Python Type Hints - 354
*** Iterator Support in Pandas UDFs - 355
*** New Pandas Function APIs - 356

** Changed Functionality - 357
*** Languages Supported and Deprecated - 357
*** Changes to the DataFrame and Dataset APIs - 357
*** DataFrame and SQL Explain Commands - 358

** Summary - 360

* Index - 361
