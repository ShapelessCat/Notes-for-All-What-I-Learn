#+TITLE: Learning Spark
#+SUBTITLE: Lightning-Fast Data Analysis
#+VERSION: 2nd, Covers Apache Spark 3.0
#+YEAR: 2020
#+AUTHOR: Jules S. Damji, Brooke Wenig, Tathagata Das & Denny Lee
#+Foreword by: Matei Zaharia
#+STARTUP: overview
#+STARTUP: entitiespretty

* Foreword - xiii
* TODO Preface - xv
  - This new edition has been updated to reflect Apache Spark's *evolution through
    Spark 2.x and Spark 3.0*, including its
    * expanded ecosystem of built-in
    * external data sources
    * machine learning
    * streaming technologies with which Spark is tightly integrated.

** Who This Book Is For
** How the Book Is Organized
** How to Use the Code Examples
** Software and Configuration Used
   - Recommended systems:
     * Apache Spark 3.0 (prebuilt for Apache Hadoop 2.7)
     * Java Development Kit (JDK) 1.8.0

** Conventions Used in This Book
** Using Code Examples
** O'Reilly Online Learning
** How to Contact Us
** Acknowledgments

* TODO 1. Introduction to Apache Spark: A Unified Analytics Engine - 1
  This chapter
  - lays out the _origins_ of Apache Spark and its _underlying philosophy_.
  - Surveys the _main components_ of the project and its _distributed architecture_.

** The Genesis of Spark - 1
   In this section, we'll chart the course of Apache Spark's short evolution:
   its genesis, inspiration, and adoption in the community as a de facto big
   data unified processing engine.

*** Big Data and Distributed Computing at Google - 1
*** Hadoop at Yahoo! - 2
*** Spark's Early Years at AMPLab - 3

** What Is Apache Spark? - 4
   - Spark provides in-memory storage for intermediate computations, making it
     much faster than Hadoop MapReduce.

   - It incorporates libraries with composable APIs for
     + machine learning (MLlib)
     + SQL for interactive queries (Spark SQL)
     + stream processing (Structured Streaming) for interacting with real-time data,
     + and graph processing (GraphX).

   - Spark's design philosophy centers around four key characteristics:
     + Speed
     + Ease of use
     + Modulatirty =???=
     + Extensibility =???=

*** TODO Speed - 4
*** DONE Ease of Use - 5
    CLOSED: [2020-09-23 Wed 22:20]
    Spark achieves simplicity by providing a fundamental abstraction of a simple
    logical data structure called a /Resilient Distributed Dataset (RDD)/ upon
    which all other higher-level structured data abstractions, such as ~DataFrame~'s
    and ~Dataset~'s, are constructed.
    - By providing a set of _transformations_ and _actions_ as operations, Spark
      offers a simple programming model that you can use to build big data
      applications in familiar languages.

*** TODO Modularity - 5
*** TODO Extensibility - 5

** Unified Analytics - 6
*** Apache Spark Components as a Unified Stack - 6
**** Spark SQL - 7
**** Spark MLlib - 7
**** Spark Structured Streaming - 8
**** GraphX

*** Apache Spark's Distributed Execution - 10
**** Spark driver - 10
**** SparkSession - 11
**** Cluster manager - 12
**** Spark executor - 12
**** Deployment modes - 12
**** Distributed data and partitions - 12

** The Developer's Experience - 14
*** Who Uses Spark, and for What? - 14
**** Data science tasks - 14
**** Data engineering tasks - 15
**** Popular Spark use cases - 16

*** Community Adoption and Expansion - 16

* TODO 2. Downloading Apache Spark and Getting Started - 19
** Step 1: Downloading Apache Spark - 19
*** Spark's Directories and Files - 21

** Step 2: Using the Scala or PySpark Shell - 22
*** Using the Local Machine - 23

** Step 3: Understanding Spark Application Concepts - 25
*** Spark Application and SparkSession - 26
*** Spark Jobs - 27
*** Spark Stages - 28
*** Spark Tasks - 28

** Transformations, Actions, and Lazy Evaluation - 28
*** Narrow and Wide Transformations - 30

** The Spark UI - 31
   - *Databricks Community Edition*

** Your First Standalone Application - 34
*** Counting M&Ms for the Cookie Monster - 35
*** Building Standalone Applications in Scala - 40

** Summary - 42

* TODO 3. Apache Spark's Structured APIs - 43
** Spark: What’s Underneath an RDD? - 43
** Structuring Spark - 44
*** Key Merits and Benefits - 45

** The DataFrame API - 47
*** Spark's Basic Data Types - 48
*** Spark's Structured and Complex Data Types - 49
*** Schemas and Creating DataFrames - 50
**** Two ways to define a schema - 51

*** Columns and Expressions - 54
*** Rows - 57
*** Common DataFrame Operations - 58
**** Using DataFrameReader and DataFrameWriter - 58
**** Transformations and actions - 61

*** End-to-End DataFrame Example - 68

** The Dataset API - 69
*** Typed Objects, Untyped Objects, and Generic Rows - 69
*** Creating Datasets - 71
**** Scala: Case classes - 71

*** Dataset Operations - 72
*** End-to-End Dataset Example - 74

** DataFrames Versus Datasets - 74
*** When to Use RDDs - 75

** Spark SQL and the Underlying Engine - 76
*** The Catalyst Optimizer - 77
**** Phase 1: Analysis - 81
**** Phase 2: Logial optimization - 81
**** Phase 3: Physical planning - 81
**** Phase 4: Code generation - 81

** Summary - 82

* TODO 4. Spark SQL and DataFrames: Introduction to Built-in Data Sources - 83
** Using Spark SQL in Spark Applications - 84
*** Basic Query Examples - 85

** SQL Tables and Views - 89
*** Managed Versus UnmanagedTables - 89
*** Creating SQL Databases and Tables - 90
**** Creating a managed table - 90
**** Creating an unmanaged table - 91

*** Creating Views - 91
**** Temporary views versus global temporary views - 92

*** Viewing the Metadata - 93
*** Caching SQL Tables - 93
*** Reading Tables into DataFrames - 93

** Data Sources for DataFrames and SQL Tables - 94
*** DataFrameReader - 94
*** DataFrameWriter - 96
*** Parquet - 97
**** Reading Parquet files into a DataFrame - 97
**** Reading Parquet files into a Spark SQL table - 98
**** Writing DataFrames to Parquet files - 99
**** Writing DataFrames to Spark SQL tables - 99

*** JSON - 100
**** Reading a JSON file into a DataFrame - 100
**** Reading a JSON file into a Spark SQL table - 100
**** Writing DataFrames to JSON files - 101
**** JSON data source options - 101

*** CSV - 102
**** Reading a CSV file into a DataFrame - 102
**** Reading a CSV file into a Spark SQL table - 102
**** Writing DataFrames to CSV files - 103
**** CSV data source options - 103

*** Avro - 104
**** Reading an Avro file into a DataFrame - 104
**** Reading an Avro file into a Spark SQL table - 105
**** Writing DataFrames to Avro files - 105
**** Avro data source options - 106

*** ORC - 106
**** Reading an ORC file into a DataFrame - 107
**** Reading an ORC file into a Spark SQL table - 107
**** Writing DataFrames to ORC files - 108

*** Images - 108
**** Reading an image file into a DataFrame - 109

*** Binary Files - 110
**** Reading a binary file into a DataFrame - 110

** Summary - 111

* TODO 5. Spark SQL and DataFrames: Interacting with External Data Sources - 113
** Spark SQL and Apache Hive - 113
*** User-Defined Functions - 114
**** Spark SQL UDFs - 114
**** Evaluation order and null checking in Spark SQL - 115
**** Speeding up and distributing PySpark UDFs with Pandas UDFs - 115

** Querying with the Spark SQL Shell, Beeline, and Tableau - 119
*** Using the Spark SQL Shell - 119
**** Create a table - 119
**** Insert data into the table - 120
**** Running a Spark SQL query - 120

*** Working with Beeline - 120
**** Start the Thrift server - 121
**** Connect to the Thrift server via Beeline - 121
**** Execute a Spark SQL query with Beeline - 121
**** Stop the Thrift server - 122

*** Working with Tableau - 122
**** Start the Thrift server - 122
**** Start Tableau - 123
**** Stop the Thrift server - 129

** External Data Sources - 129
*** JDBC and SQL Databases - 129
**** The importance of partitioning - 130

*** PostgreSQL - 132
*** MySQL - 133
*** Azure Cosmos DB - 134
*** MS SQL Server - 136
*** Other External Sources - 137

** Higher-Order Functions in DataFrames and Spark SQL - 138
*** Option 1: Explode and Collect - 138
*** Option 2: User-Defined Function - 138
*** Built-in Functions for Complex Data Types - 139
*** Higher-Order Functions - 141
**** ~transform()~ - 142
**** ~filter()~ - 143
**** ~exists()~ - 143
**** ~reduce()~ - 144

** Common DataFrames and Spark SQL Operations - 144
*** Unions - 147
*** Joins - 148
*** Windowing - 149
*** Modifications - 151
**** Adding new columns - 152
**** Dropping columns - 152
**** Renaming columns - 153
**** Pivoting - 153
**** Summary - 155

** Summary - 155

* TODO 6. Spark SQL and Datasets - 157
** Single API for Java and Scala - 157
*** Scala Case Classes and JavaBeans for Datasets - 158

** Working with Datasets - 160
*** Creating Sample Data - 160
*** Transforming Sample Data - 162
**** Higher-order functions and functional programming - 162
**** Converting DataFrames to Datasets - 166

** Memory Management for Datasets and DataFrames - 167
** Dataset Encoders - 168
*** Spark's Internal Format Versus Java Object Format - 168
*** Serialization and Deserialization (SerDe) - 169

** Costs of Using Datasets - 170
*** Strategies to Mitigate Costs - 170

** Summary - 172

* TODO 7. Optimizing and Tuning Spark Applications - 173
** TODO Optimizing and Tuning Spark for Efficiency - 173
*** Viewing and Setting Apache Spark Configurations - 173
    - There are *THREE* ways you can get and set _Spark properties_:
      1. Through a set of configuration files. In your deployment’s
         ~$SPARK_HOME~ directory (where you installed Spark), there are a number
         of config files:
         =conf/spark-defaults.conf.template=,
         =conf/log4j.properties.template=, and
         =conf/spark-env.sh.template=.
         Changing the default values in these files and saving them without the
         =.template= suffix instructs Spark to use these new values.
         * *Note*:
           Configuration changes in the =conf/spark-defaults.conf= file apply to
           + _the Spark cluster_ and
           + _all Spark applications submitted to the cluster_.

      2. Specify _Spark configurations_ directly
         *in your Spark application* or
         #+begin_src scala
           // In Scalaimport org.apache.spark.sql.SparkSession
           
           object BuildSparkSession {
             def printConfigs(session: SparkSession) = {
               // Get conf
               val mconf = session.conf.getAll
           
               // Print them
               for (k <- mconf.keySet) {
                 println(s"${k} -> ${mconf(k)}\n")
               }
             }
           
             def main(args: Array[String]) {
               // Create a session
               val spark = SparkSession.builder
                 .config("spark.sql.shuffle.partitions", 5)
                 .config("spark.executor.memory", "2g")
                 .master("local[*]")
                 .appName("SparkConfig")
                 .getOrCreate()
           
               printConfigs(spark)
               spark.conf.set("spark.sql.shuffle.partitions",
                              spark.sparkContext.defaultParallelism)
               println(" ****** Setting Shuffle Partitions to Default Parallelism")
               printConfigs(spark)
             }
           
             /// spark.driver.host -> 10.8.154.34
             /// spark.driver.port -> 55243
             /// spark.app.name -> SparkConfig
             /// spark.executor.id -> driver
             /// spark.master -> local[*]
             /// spark.executor.memory -> 2g
             /// spark.app.id -> local-1580162894307
             /// spark.sql.shuffle.partitions -> 5
           }
         #+end_src
         *on the command line* when submitting the application with ~spark-submit~,
         using the ~--conf~ flag:
         #+begin_src shell
           spark-submit --conf spark.sql.shuffle.partitions=5 \
                        --conf"spark.executor.memory=2g" \
                        --class main.scala.chapter7.SparkConfig_7_1 jars/main-scala-chapter7_2.12-1.0.jar
         #+end_src

      3. Through a _programmatic interface_ via the /Spark shell/.
         As with everything else in Spark, APIs are the primary method of
         interaction.
           Through the ~SparkSession~ object,
         you can access MOST _Spark config settings_.

         - In a Spark REPL, for example, this Scala code shows the Spark configs on a
           _local host_ where Spark is launched in /local mode/ (for details on
           the different modes available, =TODO= _see “Deployment modes” in Chapter 1_):
           #+begin_src scala
             // scala>
             val mconf = spark.conf.getAll
             
             // scala>
             mconf.keySet.foreach { k => println(s"${k} -> ${mconf(k)}\n") }
             // spark.driver.host -> 10.13.200.101
             // spark.driver.port -> 65204
             // spark.repl.class.uri -> spark://10.13.200.101:65204/classes
             // spark.jars ->
             // spark.repl.class.outputDir -> /private/var/folders/jz/qg062ynx5v39wwmfxmph5nn...
             // spark.app.name -> Spark shell
             // spark.submit.pyFiles ->
             // spark.ui.showConsoleProgress -> true
             // spark.executor.id -> driver
             // spark.submit.deployMode -> client
             // spark.master -> local[*]
             // spark.home -> /Users/julesdamji/spark/spark-3.0.0-preview2-bin-hadoop2.7
             // spark.sql.catalogImplementation -> hive
             // spark.app.id -> local-1580144503745
           #+end_src
      
*** Scaling Spark for Large Workloads - 177
**** Static versus dynamic resource allocation - 177
**** Configuring Spark executors' memory and the shuffle service - 178
**** Maximizing Spark parallelism - 180

** TODO Caching and Persistence of Data - 183
*** ~DataFrame.cache()~ - 183
*** ~DataFrame.persist()~ - 184
*** When to Cache and Persist - 187
*** When Not to Cache and Persist - 187

** TODO A Family of Spark Joins - 187
*** Broadcast Hash Join - 188
**** When to use a broadcast hash join - 189

*** Shuffle Sort Merge Join - 189
**** Optimizing the shuffle sort merge join - 193
**** When to use a shuffle sort merge join - 197

** TODO Inspecting the Spark UI - 197
*** Journey Through the Spark UI Tabs - 197
**** Jobs and Stages - 198
**** Executors - 200
**** Storage - 200
**** SQL - 202
**** Environment - 203
**** Debugging Spark applications - 204

** TODO Summary - 205

* TODO 8. Structured Streaming - 207
** Evolution of the Apache Spark Stream Processing Engine - 207
*** The Advent of Micro-Batch Stream Processing - 208
*** Lessons Learned from Spark Streaming (DStreams) - 209
*** The Philosophy of Structured Streaming - 210

** The Programming Model of Structured Streaming - 211
** The Fundamentals of a Structured Streaming Query - 213
*** Five Steps to Define a Streaming Query - 213
**** Step 1: Define input sources - 213
**** Step 2: Transform data - 214
**** Step 3: Define output sink and output mode - 215
**** Step 4: Specify processing details - 216
**** Step 5: Start the query - 218
**** Putting it all together - 218

*** Under the Hood of an Active Streaming Query - 219
*** Recovering from Failures with Exactly-Once Guarantees - 221
*** Monitoring an Active Query - 223
**** Querying current status using StreamingQuery - 223
**** Publishing metrics using Dropwizard Metrics - 224
**** Publishing metrics using custom StreamingQueryListeners - 225

** Streaming Data Sources and Sinks - 226
*** Files - 226
**** Reading from Files - 226
**** Writing from Files - 227

*** Apache Kafka - 228
**** Reading from Kafka - 228
**** Writing to Kafka - 229

*** Custom Streaming Sources and Sinks - 230
**** Writing to any storage system - 230
**** Reading to any storage system - 234

** Data Transformations - 234
*** Incremental Execution and Streaming State - 234
*** Stateless Transformations - 235
*** Stateful Transformations - 235
**** Distributed and fault-tolerant state management - 236
**** Types of stageful operations - 237

** Stateful Streaming Aggregations - 238
*** Aggregations Not Based on Time - 238
*** Aggregations with Event-Time Windows - 239
**** Handling late data with watermarks - 243
**** Supported output modes - 245

** Streaming Joins - 246
*** Stream–Static Joins - 246
*** Stream–Stream Joins - 248
**** Inner joins with optional watermarking - 248
**** Outer joins with watermarking - 252

** Arbitrary Stateful Computations - 253
*** Modeling Arbitrary Stateful Operations with ~mapGroupsWithState()~ - 254
*** Using Timeouts to Manage Inactive Groups - 257
**** Processing-time timeouts - 257
**** Event-time timeouts - 259

*** Generalization with ~flatMapGroupsWithState()~ - 261

** Performance Tuning - 262
** Summary - 264

* 9. Building Reliable Data Lakes with Apache Spark - 265
** The Importance of an Optimal Storage Solution - 265
** Databases - 266
*** A Brief Introduction to Databases - 266
*** Reading from and Writing to Databases Using Apache Spark - 267
*** Limitations of Databases - 267

** Data Lakes - 268
*** A Brief Introduction to Data Lakes - 268
*** Reading from and Writing to Data Lakes using Apache Spark - 269
*** Limitations of Data Lakes - 270

** Lakehouses: The Next Step in the Evolution of Storage Solutions - 271
*** Apache Hudi - 272
*** Apache Iceberg - 272
*** Delta Lake - 273

** Building Lakehouses with Apache Spark and Delta Lake - 274
*** Configuring Apache Spark with Delta Lake - 274
*** Loading Data into a Delta Lake Table - 275
*** Loading Data Streams into a Delta Lake Table - 277
*** Enforcing Schema on Write to Prevent Data Corruption - 278
*** Evolving Schemas to Accommodate Changing Data - 279
*** Transforming Existing Data - 279
*** Auditing Data Changes with Operation History - 282
*** Querying Previous Snapshots of a Table with Time Travel - 283

** Summary - 284

* 10. Machine Learning with MLlib - 285
** What Is Machine Learning? - 286
*** Supervised Learning - 286
*** Unsupervised Learning - 288
*** Why Spark for Machine Learning? - 289

** Designing Machine Learning Pipelines - 289
*** Data Ingestion and Exploration - 290
*** Creating Training and Test Data Sets - 291
*** Preparing Features with Transformers - 293
*** Understanding Linear Regression - 294
*** Using Estimators to Build Models - 295
*** Creating a Pipeline - 296
*** Evaluating Models - 302
*** Saving and Loading Models - 306

** Hyperparameter Tuning - 307
*** Tree-Based Models - 307
*** k-Fold Cross-Validation - 316
*** Optimizing Pipelines - 320

** Summary - 321

* 11. Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark - 323
** Model Management - 323
*** MLflow - 324

** Model Deployment Options with MLlib - 330
*** Batch - 332
*** Streaming - 333
*** Model Export Patterns for Real-Time Inference - 334

** Leveraging Spark for Non-MLlib Models - 336
*** Pandas UDFs - 336
*** Spark for Distributed Hyperparameter Tuning - 337

** Summary - 341

* TODO 12. Epilogue: Apache Spark 3.0 - 343
** Spark Core and Spark SQL - 343
*** Dynamic Partition Pruning - 343
*** Adaptive Query Execution - 345
**** The AQE framework - 346

*** SQL Join Hints - 348
**** Shuffle sort merge join (SMJ) - 348
**** Broadcast hash join (BHJ) - 348
**** Shuffle hash join (SHJ) - 348
**** Shuffle-and-replicate nested loop join (SNLJ) - 348

*** Catalog Plugin API and DataSourceV2 - 349
*** Accelerator-Aware Scheduler - 351

** Structured Streaming - 352
** PySpark, Pandas UDFs, and Pandas Function APIs - 354
*** Redesigned Pandas UDFs with Python Type Hints - 354
*** Iterator Support in Pandas UDFs - 355
*** New Pandas Function APIs - 356

** Changed Functionality - 357
*** Languages Supported and Deprecated - 357
*** Changes to the DataFrame and Dataset APIs - 357
*** DataFrame and SQL Explain Commands - 358

** Summary - 360

* Index - 361
