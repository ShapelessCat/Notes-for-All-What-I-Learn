#+TITLE: Spark in Action
#+VERSION: 2016
#+AUTHOR: Petar Zečević, Marko Bonači
#+STARTUP: overview
#+STARTUP: entitiespretty

* PART 1 First Steps - 1
* TODO 1 Introduction to Apache Spark - 3
** 1.1 What is Spark? - 4
*** The Spark revolution - 5
*** MapReduce's shortcomings - 5
*** What Spark brings to  the table - 6

** 1.2 Spark components - 8
*** Spark Core - 8
*** Spark SQL - 9
*** Spark Streaming - 10
*** Spark MLlib - 10
*** Spark GraphX - 10

** 1.3 Spark program flow - 10
** 1.4 Spark ecosystem - 13
** 1.5 Setting up the spark-in-action VM - 14
*** Downloading and starting the virtual machine - 15
*** virtual machine - 16

** 1.6 Summary - 16

* TODO 2 Spark fundamentals - 18
** 2.1 Using the spark-in-action VM - 19
*** Cloning the Spark in Action GitHub repository - 20
*** Finding Java - 20
*** Using the VM's Hadoop installation - 21
*** Examining the VM's Spark installation - 22

** 2.2 Using Spark shell and writing your first Spark program - 23
*** Starting the Spark shell - 23
*** The first Spark code example - 25
*** The notion of a resilient distributed dataset - 27

** 2.3 Basic RDD actions and transformations - 28
*** Using the map transformation - 28
*** Using the distinct and flatMap transformations - 30
*** Obtaining RDD's elements with the sample, take, and takeSample operations - 34

** 2.4 Double RDD functions - 36
*** Basic statistics with double RDD functions - 37
*** Visualizing data distribution with histograms - 38
*** Approximate sum and mean - 38

** 2.5 Summary - 39

* TODO 3 Writing Spark applications - 40
** 3.1 Generating a new Spark project in Eclipse - 41
** 3.2 Developing the application - 46
*** Preparing the GitHub archive dataset - 46
*** Loading JSON - 48
*** Running the application from Eclipse - 50
*** Aggregating the data - 52
*** Excluding non-employees - 53
*** Broadcast variables - 55
*** Using the entire dataset - 57

** 3.3 Submitting the application - 58
*** Building the uberjar - 59
*** Adapting the application - 60
*** Using spark-submit - 62

** 3.4 Summary - 64

* TODO 4 The Spark API in depth - 66
** 4.1 Working with pair RDDs - 67
*** Creating pair RDDs - 67
*** The Spark API in depth - 68

** 4.2 Understanding data partitioning and reducing data shuffling - 74
*** Using Spark's data partitioners - 74
*** Understanding and avoiding unnecessary shuffling - 76
*** Repartitioning RDDs - 80
*** Mapping data in partitions - 81

** 4.3 Joining, sorting, and grouping data - 82
*** Joining data - 82
*** Sorting data - 88
*** Grouping data - 91

** 4.4 Understanding RDD dependencies - 94
*** RDD dependencies and Spark execution - 94
*** Spark stages and tasks - 96
*** Saving the RDD lineage with checkpointing - 97

** 4.5 Using accumulators and broadcast variables to communicate with Spark executors - 97
*** Obtaining data from executors with accumulators - 97
*** Sending data to executors using broadcast variables - 99

** 4.6 Summary - 101

* PART 2 Meet the Spark Family - 103
* TODO 5 Sparkling queries with Spark SQL - 105
** 5.1 Working with DataFrames - 106
*** Creating DataFrames from RDDs - 108
*** DataFrame API basics - 115
*** Using SQL functions to perform calculations on data - 118
*** Working with missing values - 123
*** Converting DataFrames to RDDs - 124
*** Grouping and joining data - 125
*** Performing joins - 128

** 5.2 Beyond DataFrames: introducing DataSets - 129
** 5.3 Using SQL commands - 130
*** Table catalog and Hive metastore - 131
*** Executing SQL queries - 133
*** Connecting to Spark SQL through the Thrift server - 134

** 5.4 Saving and loading DataFrame data - 137
*** Built-in data sources - 138
*** Saving data - 139
*** Loading data 141

** 5.5 Catalyst optimizer - 142
** 5.6 Performance improvements with Tungsten - 144
** 5.7 Beyond DataFrames: introducing DataSets - 145
** 5.8 Summary - 145

* TODO 6 Ingesting data with Spark Streaming - 147
** 6.1 Writing Spark Streaming applications - 148
*** Introducing the example application - 148
*** Creating a streaming context - 150
*** Creating a discretized stream - 150
*** Using discretized streams - 152
*** Saving the results to a file - 153
*** Starting and stopping the streaming computation - 154
*** Saving the computation state over time - 155
*** Using window operations for time-limited calculations - 162
*** Examining the other built-in input streams - 164

** 6.2 Using external data sources - 165
*** Setting up Kafka - 166
*** Changing the streaming application to use Kafka - 167

** 6.3 Performance of Spark Streaming jobs - 172
*** Obtaining good performance - 173
*** Achieving fault-tolerance - 175

** 6.4 Structured Streaming - 176
*** Creating a streaming DataFrame - 176
*** Outputting streaming data - 177
*** Examining streaming executions - 178
*** Future direction of structured streaming - 178

** 6.5 Summary - 178

* TODO 7 Getting smart with MLlib - 180
** 7.1 Introduction to machine learning - 181
*** Definition of machine learning - 184
*** Classification of machine-learning algorithm - 184
*** Machine learning with Spark - 187

** 7.2 Linear algebra in Spark - 187
*** Local vector and matrix implementations - 188
*** Distributed matrices - 192

** 7.3 Linear regression - 193
*** About linear regression - 193
*** Simple linear regression - 194
*** Expanding the model to multiple linear regression - 196

** 7.4 Analyzing and preparing the data - 198
*** Analyzing data distribution - 199
*** Analyzing column cosine similarities - 200
*** Computing the covariance matrix - 200
*** Transforming to labeled points - 201
*** Splitting the data - 201
*** Feature scaling and mean normalization - 202

** 7.5 Fitting and using a linear regression model - 202
*** Predicting the target values - 203
*** Evaluating the model's performance - 204
*** Interpreting the model parameters - 204
*** Loading and saving the model - 205

** 7.6 Tweaking the algorithm - 205
*** Finding the right step size and number of iterations - 206
*** Adding higher-order polynomials - 207
*** Bias-variance tradeoff and model complexity - 209
*** Plotting residual plots - 211
*** Avoiding overfitting by using regularization - 212
*** K-fold cross-validation - 213

** 7.7 Optimizing linear regression - 214
*** Mini-batch stochastic gradient descent - 214
*** LBFGS optimizer - 216

** 7.8 Summary - 217

* TODO 8 ML: classification and clustering - 218
** TODO 8.1 Spark ML library - 219
*** Estimators, transformers, and evaluators - 220
*** ML parameters - 220
*** ML pipelines - 221

** TODO 8.2 Logistic regression - 221
*** Binary logistic regression model - 222
*** Preparing data to use logistic regression in Spark - 224
*** Training the model - 229
*** Evaluating classification models - 231
*** Performing k-fold cross-validation - 234
*** Multiclass logistic regression - 236

** TODO 8.3 Decision trees and random forests - 238
*** Decision trees - 239
*** Random forests - 244

** TODO 8.4 Using k-means clustering - 246
*** K-means clustering - 247

** TODO 8.5 Summary

* TODO 9 Connecting the dots with GraphX - 254
** 9.1 Graph processing with Spark - 255
*** Constructing graphs using GraphX API - 256
*** Transforming graphs - 257

** 9.2 Graph algorithms- 256
*** Presentation of the dataset - 263
*** Shortest-paths algorithm - 264
*** Page rank - 265
*** Connected components - 266
*** Strongly connected components - 268

** 9.3 Implementing the A* search algorithm - 269
*** Understanding the A* algorithm - 270
*** Implementing the A* algorithm - 272
*** Testing the implementation - 279

** 9.4 Summary - 280

* PART 3 Spark OPS - 283
* TODO 10 Running Spark - 285
** 10.1 An overview of Spark's runtime architecture - 286
*** Spark runtime components - 286
*** Spark cluster types - 288

** 10.2 Job and resource scheduling - 289
*** Cluster resource scheduling - 290
*** Spark job scheduling Data-locality considerations - 293
*** Spark memory scheduling - 294

** 10.3 Configuring Spark - 295
*** Spark configuration file - 295
*** Command-line parameters - 295
*** System environment variables - 296
*** Setting configuration programmatically - 296
*** The master parameter - 297
*** Viewing all configured parameters - 297

** 10.4 Spark web UI - 297
*** Jobs page - 298
*** Stages page - 300
*** Storage page - 302
*** Environment page 302
*** Executors page - 303

** 10.5 Running Spark on the local machine - 303
*** Local mode - 304
*** Local cluster mode - 305

** 10.6 Summary - 305

* TODO 11 Running on a Spark standalone cluster - 306
** 11.1 Spark standalone cluster components - 307
** 11.2 Starting the standalone cluster - 308
*** Starting the cluster with shell scripts - 309
*** Starting the cluster manually - 312
*** Viewing Spark processes - 313
*** Standalone master high availability and recovery - 313

** 11.3 Standalone cluster web UI - 315
** 11.4 Running applications in a standalone cluster - 317
*** Location of the driver - 317
*** Specifying the number of executors - 318
*** Specifying extra classpath entries and files - 319
*** Killing applications - 320
*** Application automatic restart - 320

** 11.5 Spark History Server and event logging - 321
** 11.6 Running on Amazon EC2 - 322
*** Prerequisites - 323
*** Creating an EC2 standalone cluster - 324
*** Using the EC2 cluster - 327
*** Destroying the cluster - 329

* TODO 12 Running on YARN and Mesos - 331
** 12.1 Running Spark on YARN - 332
*** YARN architecture - 332
*** Installing, configuring, and starting YARN - 334
*** Resource scheduling in YARN - 335
*** Submitting Spark applications to YARN - 336
*** Configuring Spark on YARN - 338
*** Configuring resources for Spark jobs - 339
*** YARN UI - 341
*** Finding logs on YARN - 343
*** Security considerations - 344
*** Dynamic resource allocation - 344

** 12.2 Running Spark on Mesos - 345
*** Mesos architecture - 346
*** Installing and configuring Mesos - 349
*** Mesos web UI - 351
*** Mesos resource scheduling - 353
*** Submitting Spark applications to Mesos - 354
*** Running Spark with Docker - 356

** 12.3 Summary - 359

* PART 4 Bringing It Together - 361
* TODO 13 Case study: real-time dashboard - 363
* TODO 14 Deep learning on Spark with H2O - 383
** 14.1 What is deep learning? - 384
** 14.2 Using H2O with Spark - 385
*** What is H2O? - 386
*** Starting Sparkling Water on Spark - 386
*** Starting the H2O cluster - 389
*** Accessing the Flow UI - 390

** 14.3 Performing regression with H2O’s deep learning - 391
*** Loading data into an H2O frame - 391
*** Building and evaluating a deep-learning model using the Flow UI - 394
*** Building and evaluating a deep-learning model using the Sparkling Water API - 398

** 14.4 Performing classification with H2O’s deep learning - 402
*** Loading and splitting the data - 402
*** Building the model through the Flow UI - 403
*** Building the model with the Sparkling Water API - 406
*** Stopping the H2O cluster - 406

** 14.5 Summary - 407

* appendix A Installing Apache Spark - 409
* appendix B Understanding MapReduce - 415
* appendix C A primer on linear algebra - 418
* index - 423
