#+TITLE: Apache Kafka Series: Learn Apache Kafka for Beginners v3
#+VERSION: 2023-06
#+STARTUP: overview
#+STARTUP: entitiespretty

* DONE Section 1: Kafka Introduction
  CLOSED: [2023-06-18 Sun 17:36]
** 1. Course Introduction
** 2. Apache Kafka in 5 minutes
   - Types of problems organisations are facing with the previous architecture
     (source system -> [processor] -> target system)
     * Need to implement size(sources) * size(targets) integrations

     * Each integration comes with difficulties around
       + Protocol - how the data is transpoted (TCP, HTTP, REST, FTP, JDBC ...)
       + Data format - how the data is parsed (Binary, CSV, JSON, Avro, Protobuf ...)
       + Data schema & evolution - how the data is shaped and may change

     * Each source system will have an _increased load_ from the connections

   - History:
     Created by LinkedIn, now Open-Source Project mainly maintained by
     Confluent, IBM, Cloudera

   - Why Apache Kafka:
     * Decoupling of data stream & systems

     * Distributed, resilient architecture, fault tolerant

     * Horizontal scalability:
       + Can scale to hundreds of brokers
       + Can scale to millions of messages per second

     * High performance (latency of less than 10ms) -- real time

     * Used by the 2000+ firms, 80% of the Fortune 100

   - Created by LinkedIn, now Open-Source Project mainly maintained by Confluent,

   - Use cases:
     * Messaing System
     * Activity Tracking
     * Gather metrics from many different locations
     * Application Logs gathering
     * Stream processing (with the Kafka Streams API for example)
     * De-coupling of system dependencies
     * Integration with Spark, Flink, Storm, Hadoop, and many other Big Data technologies
     * Micro-services pub/sub

   - Examples:
     * _Netflix_ uses Kafka to apply recommendations in real-time while you're
       watchiing TV show.

     * _Uber_ uses Kafka to gather user, taxi and trip data in real-time to compute
       and forecast demand, and compute surge pricing in real-time

     * _LinkedIn_ uses Kafka to prevent spam, collect usser interactions to make
       better connection recommendations in real time

   - Remember that Kafka is only used as transportation mechanism!

** 3. Course Objectives
   Kafka for Beginners
   What we'll learn in this course

   - The basic architecutre of projects using Kafka:

                                                      |* topics
                                                      |* partitions
                                                      |* rerplications
                                                      |* partition leader & in-sync-replicas (ISR)
                                                      |* offsets topic

     Source System --> Producers ---> Kafka Clusters ---> Consumers --> Target Systems
                         /||\ (Broker 101, Broker 102, ...)  /||\
                          ||               /||\               ||
                     |* round robin         ||         |* consumer offsets
                     |* key based ordering  ||         |* consumer groups
                     |* acks strategy      \||/        |* at least/most once
                                        Zookeeper
                                        |* leader follower
                                        |* broker management

   - Plus:
     * Intro to Conduktor
     * Intro to Kafka Connect
     * Intro to Kafka Streams
     * Intro to Confluent Schema Registry
     * Intro to Kafka Architecture in the enterprise
     * Real world use cases
     * Advanced API + Configurations
     * Topic Configurations

   - And so much more!

   - Course Structure:
     * Part 1 Fundamentals - 4 hours
       + Kafka Theory
       + Starting Kafka
       + Kafka CLI
       + Kafka & Java 101

     * Part 2 Real World - 3 hours
       + Wikimedia Producer
       + OpenSearch Consumer
       + Extended API Intro
         Case Studies
         Kafka in the Enterprise

     * Part 3 Advanced & Annexes
       + Advanced Topic Configuration

   - Apache Kafka Series - Welcome!
     1. *Kafka for Beginners*
        get a strong base for Kafka, basic operations, write your first
        producers and consumers

     2. *Kafka Connect API*
        Understand how to import / export data to / from Kafka

     3. *Kafka Streams API*
        Learn how to process and transform data within Kafka

     4. *ksqlDB*
        Write Kafka Streams applications using SQL

     5. *Confluent Components*
        REST Proxy and Schema Registry

     6. *Kafka Security*
        Setup Kafka security in a Cluster and Integrate your applications with
        Kafka Security

     7. *Kafka Monitoring and Operations*
        use _Prometheus_ and _Grafana_ to monitor Kafka, learn operations.

     8. *Kafka Cluster Setup & Administration*
        Get a deep understanding of how Kafka & Zookeeper works, how to setup
        Kafka and various administration tasks

     9. *Confluent Cerrtifications for Developers Practice Exams*

     10. *Confluent Cerrtifications for Operators Practice Exams*

** 4. Welcome! - About your instructor

* TODO Section 2: Code Download
** 5. Code Download

* DONE Section 3: Kafka Fundamentals
  CLOSED: [2023-06-18 Sun 18:08]
** 6. Kafka Fundamentals
   - In this first learning block, we are going through Kafka 101.
     1. We'll have a very necessary *all-theory* section, during which we'll learn
        all the fundamentals of Kafka

     2. We'll then go ahead and *set up* Kafka
        * on our computers
        * in the cloud

     3. We'll spend a lot of time learning how to use the *Kafka CLI*

     4. We'll write some Java Code to create our first *Producers* and *Consumers*

   By the end of this learning block, you'll be "proficient" with Kafka.

   Hope you're ready, let's get started!

* DONE Section 4: Kafka Theory - =TODO= =NOTE=
  CLOSED: [2023-06-18 Sun 22:51]
** TODO 7. Topics, Partitions and Offsets - =NOTE=
   - Topic :: a particular stream of data
     * Like a table in a database (without all the constraints)
     * You can have as many /topics/ as you want
     * A /topic/ is identified by its _name_
     * Any kind of message format
     * The sequence of messages is called a _data stream_
     * You can't query topics, instead,
       1. use Kafka /Producers/ to send data
       2. use Kafka /Consumers/ to read data

   - /Partitions/ and /offsets/:
     * Topics are split in _partitions_ (example: 100 partitions)
       * Messages within each partition are ordered
       * Each message within a partition gets an incremental id, called _offset_

     * /Kafka topics/ are *immutable*:
       once data is written to a partition, it cannot be changed

   - IMPORTANT NOTEs for
     /Topics/, /partitions/ and /offsets/
     06:05

** DONE 8. Producers and Message Keys
   CLOSED: [2023-06-18 Sun 19:58]
   - Kafka messages (created by producers) anatomy:
     ----------------------------------------
     || key-binary      || values-binary   ||
     || (Can be ~null~) || (Can be ~null~) ||
     ----------------------------------------
     ||           Compression Type         ||
     ||   (none, gzip, snappy, lz4, zstd)  ||
     ----------------------------------------
     ||         Headers (optional)         ||
     ||         || Key || Value ||         ||
     ||         || Key || Value ||         ||
     ----------------------------------------
     ||         Partition + Offset         ||
     ||____________________________________||
     ||   Timestamp (system or user set)   ||
     ----------------------------------------

   - Kafka message serializer
     * Kafka *ONLY*
       + accepts bytes as an input from /producers/
       + sends bytes out as an output to /consumers/

     * Message Serialization
       means _tranforming_ objects / data _into_ bytes

     * They are used on BOTH the value and the key

     * Common Serializers
       + String (includes JSON)
       + Int, Float
       + Avro
       + Protobuf

   - For the curious: Kafka Message Key Hashing
     * A *Kafka partitioner* is a code logic that
       takes as record and determines to which /partition/ to send it into.

     * *Key Hashing* is the process of determining the mapping of a key to a partition
       + In the default Kafka partitioner, the keys are hashed using the
         /murmur2 algorithm/, with the formula below for the curious:
         ~targetPartition = Math.abs(Utils.murmur2(keyBytes)) % (numPartitions - 1)~

** DONE 9. Consumers & Deserialization
   CLOSED: [2023-06-18 Sun 20:47]
   - /Consumers/ read data from a topic (identified by name) -- pull model

   - /Consumers/ automatically know which /broker/ to read from =???=

   - In case of /broker/ failures, /consumers/ know how to recover

   - Data is read in order from low to high offset *within each partitions*

   - Consumer Deserializer
     * Deserialize indicates how to transform bytes into objects / data

     * They are used on the value and the key of the message

     * Common Deserializers
       + String (includes JSON)
       + Int, Float
       + Avro
       + Protobuf

     * The serialization / deserialization type *MUST NOT* change _during a topic
       lifecycle (create a new topic instead)_

** DONE 10. Consumer Groups & Consumer Offsets
   CLOSED: [2023-06-18 Sun 21:39]
   - All the consumers in an application read data as a consumer groups

   - Each consumer within a group reads from exclusive partitions

   - Q :: Consumer groups - What if too many consumers?
   - A :: If you have more consumers than partitions, some consumers will be _inactive_.

   - Multiple Consumers on one topic
     In Apache Kafka it is acceptable to have _MULTIPLE_ /consumer groups/ on the
     same /topic/.

   - To create distinct consumer groups, use the consumer property ~group.id~

   - Consumer Offsets
     * Kafka stores the offsets at which a /consumer group/ has been reading
       + The offsets committed are in Kafka /topic/ named ~__consumer_offsets~

     * When a consumer in a group has processed data received from Kafka,
       it should be *periodically* committing the offsets (the Kafka broker will
       write to ~__consumer_offsets~, not the group itself)

     * If a consumer dies, it will be able to read back from where it left off thanks
       to the committed consumer offsets!

   - Delivery semantics for consumers =TODO=
     * By default, Java Consumers will automatically commit offsets (*at least once*)
     * There are 3 delivery semantics if you choose to commit manually
     * *At least once* _(usually preferred)_
       + Offsets are committed after the message is processed
       + If the processing goes wrong, the message will be read again
       + This can result in duplicate processing of messages. Make sure your
         processing is _idempotent_ (i.e. processing again the messages won't
         impact your systems)

     * *At most once*
       + Offsets are committed as soon as messages are received
       + If the processing goes wrong, some messages will be lost (they won't be read again)

     * *Exactly once*
       + For Kafka to Kafka workflows: use the ~Transactional~ API (easy with Kafka Streams API)
       + For Kafka to external system workflows: use an /_idempotent_ consumer/

** DONE 11. Brokers and Topics
   CLOSED: [2023-06-18 Sun 22:00]
   - A Kafka cluster is composed of multiple brokers (servers)

   - Each broker is identified with its ID (integer)

   - Each broker contains certain topic partitions

   - After connecting to any broker (called a boostrap broker), you will be
     connected to the entire cluster (Kafka clients have smart mechanics for that)

   - A good number to get started is 3 brokers, but some big clusters have over
     100 brokers

   - In these examples we choose to number brokers starting at 100 (arbitrary)
     * Example: Broker 101, Broker 102, Broker 103

   - Brokers and topics
     * Example of Topic-A with 3 /partitions/ and Topic-B with 2 /partitionns/

   - Kafka Broker Discovery
     * Every Kafka broker is also called a /bootstrap server/

     * You only need to connect to one /broker/, and the Kafka clients will know
       how to be connected to the entire cluster (smart clients)

     * Each broker knows about ALL (metadata)
       + brokers
       + topics
       + partitions

** DONE 12. Topic Replication
   CLOSED: [2023-06-18 Sun 22:10]
   - Topic replication factor
     * Topics should have a replication factor > 1 (usually between 2 and 3)
     * This way if a broker is down, another broker can serve the data

   - Concept of Leander for a Partition
     * At any time *only ONE* /broker/ can be _leader for a given /partition/._
     * /Producers/ can *ONLY* send data to the broker that is _leader of a /partition/._
     * The OTHER brokers will replicate the data.
     * Therefore, each /partition/ has ONE leader and MULTIPLE /ISR (in-sync replica)/.

   - *Default* producer & consumer behavior with leaders
     * Kafka /producers/ can ONLY _write to_ the /leader broker/ for a partition.
     * Kafka /consumers/ will _read from_ the /leader broker/ for a partition.

   - Kafka consumers replica fetching (*Kafka v2.4+*)
     * Since Kafka 2.4, it is possible to configure consumers to read from the *closest* replica
     * This may help,  if using the cloud,
       + improve latency
       + decrease network costs

** DONE 13. Producer Acknowledgements & Topic Durability
   CLOSED: [2023-06-18 Sun 22:15]
   - /Producers/ can choose to receive acknowledgement of data writes:
     * ~acks=0~: Producer won't wait for acknowledgement (POSSIBLE data loss)
     * ~acks=1~: Producer will wait for leader acknowledgement (LIMITED data loss)
     * ~acks=all~: Leader + replicas acknowledgement (NO data loss)

   - Kafka Topic Durability:
     * For a topic replication factor of 3, topic data durability can withstand
       2 brokers loss.

     * As a rule, for a replication factor of N, you can permanently lose up to
       N - 1 brokders and still recover your data.

** TODO 14. Zookeeper - =TODO=
   - Zookeeper manages brokens (keeps a list of them)

   - Zookeeper helps in performing leader election for partitions

   - Zookeeper sends notifications to Kafka in case of changes (e.g. new topic,
     broker dies, broker comes up, delete topics, etc...)

   - Kafka 2.x can't work without Zookeeper

   - Kafka 3.x can work without Zookeeper (KIP-500) - using Kafka Raft instead

   - Kafka 4.x will not have Zookeeper

   - Zookeeper by design operates with an odd number of servers (1, 3, 5, 7,
     never more than 7)

   - Zookeeper has a leader (writes) the rest of the servers are followers (reads)

   - (Zookeeper does NOT store consumer offsets with Kafka > v0.10)

   - Q :: Should you use /Zookeeper/ with *Kafka Brokders*?
   - A :: Yes, until Kafka 4.0 is out while waiting for Kafka without Zookeeper
          to be production-ready

   - Q :: Should you use Zookeeper with /Kafka Clients/?
   - A :: NO
     * 04:57 =TODO= =NOTE=

** TODO 15. Kafka KRaft - Removing Zookeeper - =TODO= =NOTE=
   - xxx

** TODO 16. Theory Roundup - =TODO= =NOTE=
   - xxx

** TODO Quiz 1: Quiz on Theory - =TODO= =NOTE=
   - xxx

* TODO Section 5: Starting Kafka
** 17. Important: Starting Kafka & Lectures Order
** 18. FAQ for Setup Problems
** 19. Starting Kafka with Conduktor - Multi Platform
** 20. Mac OS X - Download and Setup Kafka in PATH
** 21. Mac OS X - Start Zookeeper and Kafka
** 22. Mac OS X - Using brew
** 23. Linux - Download and Setup Kafka in PATH
** 24. Linux - Start Zookeeper and Kafka
** 25. Windows WSL2 - Download Kafka and PATH Setup
** 26. Windows WSL2 - Start Zookeeper & Kafka
** 27. Windows WSL2 - How to Fix Problems
** 28. Windows WSL2 - Extra Instructions
** 29. Windows non-WSL2 - Start Zookeeper and Kafka

* TODO Section 6: Starting Kafka without Zookeeper
** 30. Note: try out Kafka KRaft
** 31. Mac OS X - Start Kafka in KRaft mode
** 32. Linux - Start Kafka in KRaft mode
** 33. Windows WSL2 - Start Kafka in KRaft mode

* TODO Section 7: CLI (Command Line Interface) 101
** 34. CLI Introduction
** 35. WINDOWS WARNING: PLEASE READ
** 36. Kafka Topics CLI
** 37. Kafka Console Producer CLI
** 38. Kafka Console Consumer CLI
** 39. Kafka Consumers in Group
** 40. Kafka Consumer Groups CLI
** 41. Resetting Offsets
** Quiz 2: Quiz on CLI

* TODO Section 8: Kafka UI - Conduktor Demo
** 42. Conduktor - Demo

* TODO Section 9: Kafka Java Programming 101
** 43. Kafka SDK List
** 44. Creating Kafka Project
** 45. Java Producer
** 46. Java Producer Callbacks
** 47. Java Producer with Keys
** 48. Java Consumer
** 49. Java Consumer - Graceful Shutdown
** 50. Java Consumer inside Consumer Group
** 51. Java Consumer Incremental Cooperative Rebalance & Static Group Membership
** 52. Java Consumer Incremental Cooperative Rebalance - Practice
** 53. Java Consumer Auto Offset Commit Behavior
** 54. Programming - Advanced Tutorials
** Quiz 3: Quiz on Java Programming 101

* TODO Section 10: Kafka Real World Project
** 55. Real World Project Overview
** 56. Real World Exercise - Solution

* TODO Section 11: Kafka Wikimedia Producer & Advanced Producer Configurations
** 57. IMPORTANT: Start Local Kafka with Conduktor using Docker
** 58. Wikimedia Producer Project Setup
** 59. Wikimedia Producer Implementation
** 60. Wikimedia Producer Run
** 61. Wikimedia Producer - Producer Config Intros
** 62. Producer Acknowledgements Deep Dive
** 63. Producer Retries
** 64. Idempotent Producer
** 65. Safe Kafka Producer Settings
** 66. Wikimedia Producer Safe Producer Implementation
** 67. Kafka Message Compression
** 68. linger.ms and batch.size Producer settings
** 69. Wikimedia Producer High Throughput Implementation
** 70. Producer Default Partitioner & Sticky Partitioner
** 71. [Advanced] max.block.ms and buffer.memory
** Quiz 4: Quiz on Producer Configurations

* TODO Section 12: OpenSearch Consumer & Advanced Producer Configurations
** 72. OpenSearch Consumer - Project Overview
** 73. OpenSearch Consumer - Project Setup
** 74. Setting up OpenSearch on Docker
** 75. Setting up OpenSearch on the Cloud
** 76. OpenSearch 101
** 77. OpenSearch Consumer Implementation - Part 1
** 78. OpenSearch Consumer Implementation Part 2
** 79. Consumer Delivery Semantics
** 80. OpenSearch Consumer Implementation Part 3 - Idempotence
** 81. Consumer Offsets Commit Strategies
** 82. OpenSearch Consumer Implementation Part 4 - Delivery Semantics
** 83. OpenSearch Consumer Implementation Part 5 - Batching Data
** 84. Consumer Offset Reset Behavior
** 85. OpenSearch Consumer Implementation Part 6 - Replaying Data
** 86. Consumer Internal Threads
** 87. Consumer Replica Fetching - Rack Awareness
** Quiz 5: Quiz on Consumer Configurations

* TODO Section 13: Kafka Extended APIs for Developers
** 88. Kafka Extended APIs - Overview
** 89. Kafka Connect Introduction
** 90. Kafka Connect Hands On: Warning
** 91. Kafka Connect Wikimedia & ElasticSearch Hands On
** 92. Kafka Streams Introduction
** 93. Kafka Streams Hands-On
** 94. Kafka Schema Registry Introduction
** 95. Kafka Schema Registry Hands On
** 96. Which Kafka API should I use?
** Quiz 6: Quiz on Kafka Extended APIs

* TODO Section 14: Real World Insights and Case Studies (Big Data / Fast Data)
** 97. Choosing Partition Count & Replication Factor
** 98. Kafka Topics Naming Convention
** 99. Case Study - MovieFlix
** 100. Case Study - GetTaxi
** 101. Case Study - MySocialMedia
** 102. Case Study - MyBank
** 103. Case Study - Big Data Ingestion
** 104. Case Study - Logging and Metrics Aggregation

* TODO Section 15: Kafka in the Enterprise for Admins
** 105. Kafka Cluster Setup High Level Architecture Overview
** 106. Kafka Monitoring & Operations
** 107. Kafka Security
** 108. Kafka Multi Cluster & MirrorMaker
** 109. Advertised Listeners: Kafka Client & Server Communication Protocol

* TODO Section 16: Advanced Kafka
** 110. Advanced Kafka

* TODO Section 17: Advanced Topics Configurations
** 111. Changing a Topic Configuration
** 112. Segment and Indexes
** 113. Log Cleanup Policies
** 114. Log Cleanup Delete
** 115. Log Compaction Theory
** 116. Log Compaction Practice
** 117. Unclean Leader Election
** 118. Large Messages in Kafka

* TODO Section 18: Next Steps
** 119. What's Next?
** 120. THANK YOU!
** 121. Bonus Lecture
